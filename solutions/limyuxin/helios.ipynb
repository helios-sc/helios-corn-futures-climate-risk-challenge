{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0ab10f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T09:39:57.677795Z",
     "iopub.status.busy": "2026-01-29T09:39:57.677398Z",
     "iopub.status.idle": "2026-01-29T09:40:20.491567Z",
     "shell.execute_reply": "2026-01-29T09:40:20.490426Z"
    },
    "papermill": {
     "duration": 22.824754,
     "end_time": "2026-01-29T09:40:20.493830",
     "exception": false,
     "start_time": "2026-01-29T09:39:57.669076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CONFIG_KEY = ROB_01_CHN_m03\n",
      "Wrote: submission.csv | rows: 41556 | cols: 54\n"
     ]
    }
   ],
   "source": [
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _available_configs():\n",
    "    return {\n",
    "        \"ROB_01_CHN_m03\": dict(\n",
    "            country=\"China\",\n",
    "            month=3,\n",
    "            preproc=\"raw\",\n",
    "            signal=\"share_plus_locations__wet_wapr_wmean\",\n",
    "            agg=\"streakthr0.5\",\n",
    "            w=527,\n",
    "            shift=0,\n",
    "            transform=\"square\",\n",
    "            feature_col=\"climate_risk_chi_m03_share_plus_locations_wet_wapr_wmean_streakthr0_5_w527_shift0_square\",\n",
    "        ),\n",
    "        \"PUB_01_BRA_m12\": dict(\n",
    "            country=\"Brazil\",\n",
    "            month=12,\n",
    "            preproc=\"raw\",\n",
    "            signal=\"share_only_norm__temp_stress_max\",\n",
    "            agg=\"std\",\n",
    "            w=749,\n",
    "            shift=86,\n",
    "            transform=\"square\",\n",
    "            feature_col=\"climate_risk_bra_m12_share_only_norm_temp_stress_max_std_w749_lag86_square\",\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def _resolve_config():\n",
    "    key = os.environ.get(\"CONFIG_KEY\", \"ROB_01_CHN_m03\")\n",
    "    cfgs = _available_configs()\n",
    "    if key not in cfgs:\n",
    "        raise ValueError(f\"Unknown CONFIG_KEY={key}. Options: {list(cfgs.keys())}\")\n",
    "    cfg = cfgs[key]\n",
    "    return key, cfg, cfg[\"feature_col\"]\n",
    "\n",
    "\n",
    "CONFIG_KEY, cfg, FEATURE_COL = _resolve_config()\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "COMP_DIR = \".\"\n",
    "MAIN_CSV = f\"{COMP_DIR}/data/corn_climate_risk_futures_daily_master.csv\"\n",
    "SHARE_CSV = f\"{COMP_DIR}/data/corn_regional_market_share.csv\"\n",
    "OUT_PATH = \"submission.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Rolling / transforms\n",
    "# -------------------------\n",
    "class _Ops:\n",
    "    \"\"\"Minimal numeric ops mirroring the original script (logic must remain identical).\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_array(x: np.ndarray, shift: int) -> np.ndarray:\n",
    "        x = np.asarray(x, dtype=np.float64)\n",
    "        n = x.shape[0]\n",
    "        out = np.full((n,), np.nan, dtype=np.float64)\n",
    "        if shift == 0:\n",
    "            out[:] = x\n",
    "            return out\n",
    "        if shift > 0:\n",
    "            out[shift:] = x[: n - shift]\n",
    "        else:\n",
    "            s = -shift\n",
    "            out[: n - s] = x[s:]\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def rolling_std_min1(x: np.ndarray, w: int) -> np.ndarray:\n",
    "        x = np.asarray(x, dtype=np.float64)\n",
    "        n = x.shape[0]\n",
    "        if w <= 1:\n",
    "            return np.zeros_like(x, dtype=np.float64)\n",
    "        out = np.full((n,), np.nan, dtype=np.float64)\n",
    "        xx = np.where(np.isnan(x), 0.0, x)\n",
    "        cs = np.zeros(n + 1, dtype=np.float64)\n",
    "        cs2 = np.zeros(n + 1, dtype=np.float64)\n",
    "        cs[1:] = np.cumsum(xx)\n",
    "        cs2[1:] = np.cumsum(xx * xx)\n",
    "        cnt = np.zeros(n + 1, dtype=np.int32)\n",
    "        cnt[1:] = np.cumsum(~np.isnan(x))\n",
    "        for i in range(n):\n",
    "            j0 = max(0, i - w + 1)\n",
    "            s = cs[i + 1] - cs[j0]\n",
    "            s2 = cs2[i + 1] - cs2[j0]\n",
    "            c = cnt[i + 1] - cnt[j0]\n",
    "            if c > 1:\n",
    "                mean = s / c\n",
    "                var = max(0.0, (s2 / c) - (mean * mean))\n",
    "                out[i] = math.sqrt(var)\n",
    "            elif c == 1:\n",
    "                out[i] = 0.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def streak_fraction(x: np.ndarray, w: int, thr: float) -> np.ndarray:\n",
    "        x = np.asarray(x, dtype=np.float64)\n",
    "        n = x.shape[0]\n",
    "        if w <= 1:\n",
    "            return (x >= thr).astype(np.float64)\n",
    "        out = np.full((n,), np.nan, dtype=np.float64)\n",
    "        good = (x >= thr) & np.isfinite(x)\n",
    "        cs = np.zeros(n + 1, dtype=np.int32)\n",
    "        cs[1:] = np.cumsum(good.astype(np.int32))\n",
    "        cnt = np.zeros(n + 1, dtype=np.int32)\n",
    "        cnt[1:] = np.cumsum(np.isfinite(x).astype(np.int32))\n",
    "        for i in range(n):\n",
    "            j0 = max(0, i - w + 1)\n",
    "            g = cs[i + 1] - cs[j0]\n",
    "            c = cnt[i + 1] - cnt[j0]\n",
    "            if c > 0:\n",
    "                out[i] = g / c\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_preproc(x: np.ndarray, mode: str) -> np.ndarray:\n",
    "        mode = (mode or \"raw\").strip().lower()\n",
    "        if mode != \"raw\":\n",
    "            raise ValueError(f\"Unsupported preproc for this minimal script: {mode}\")\n",
    "        return np.asarray(x, dtype=np.float64)\n",
    "\n",
    "    @classmethod\n",
    "    def apply_agg(cls, x_shift: np.ndarray, agg: str, w: int) -> np.ndarray:\n",
    "        agg = (agg or \"\").strip().lower()\n",
    "        if agg == \"std\":\n",
    "            return cls.rolling_std_min1(x_shift, int(w))\n",
    "        if agg.startswith(\"streakthr\"):\n",
    "            thr = float(agg.replace(\"streakthr\", \"\"))\n",
    "            return cls.streak_fraction(x_shift, int(w), thr)\n",
    "        raise ValueError(f\"Unsupported agg for this minimal script: {agg}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_transform(x: np.ndarray, name: str) -> np.ndarray:\n",
    "        x = np.asarray(x, dtype=np.float64)\n",
    "        name = (name or \"\").strip().lower()\n",
    "        if name != \"square\":\n",
    "            raise ValueError(f\"Unsupported transform for this minimal script: {name}\")\n",
    "        return np.sign(x) * (x * x)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Base-rowset engineering\n",
    "# -------------------------\n",
    "class _BaseRowsetBuilder:\n",
    "    risk_categories = [\"heat_stress\", \"unseasonably_cold\", \"excess_precip\", \"drought\"]\n",
    "    windows = [7, 14, 30]\n",
    "\n",
    "    def __init__(self, market_share_df: pd.DataFrame):\n",
    "        self._share = market_share_df\n",
    "\n",
    "    def build(self, daily_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Orchestrate as a step pipeline (different architecture, same logic).\n",
    "        steps = (\n",
    "            self._add_calendar_fields,\n",
    "            self._merge_market_share,\n",
    "            self._add_risk_scores,\n",
    "            self._add_stress_summaries,\n",
    "            self._sort_region_date,\n",
    "            self._add_rolling_features,\n",
    "            self._add_changes_and_accel,\n",
    "            self._merge_country_aggregates,\n",
    "            self._dropna_copy,\n",
    "        )\n",
    "        out = daily_df.copy()\n",
    "        for fn in steps:\n",
    "            out = fn(out)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_calendar_fields(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df[\"day_of_year\"] = df[\"date_on\"].dt.dayofyear\n",
    "        df[\"quarter\"] = df[\"date_on\"].dt.quarter\n",
    "        return df\n",
    "\n",
    "    def _merge_market_share(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.merge(\n",
    "            self._share[[\"region_id\", \"percent_country_production\"]],\n",
    "            on=\"region_id\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        df[\"percent_country_production\"] = df[\"percent_country_production\"].fillna(1.0)\n",
    "        return df\n",
    "\n",
    "    def _add_risk_scores(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        for risk_type in self.risk_categories:\n",
    "            low_col = f\"climate_risk_cnt_locations_{risk_type}_risk_low\"\n",
    "            med_col = f\"climate_risk_cnt_locations_{risk_type}_risk_medium\"\n",
    "            high_col = f\"climate_risk_cnt_locations_{risk_type}_risk_high\"\n",
    "\n",
    "            total_locations = df[low_col] + df[med_col] + df[high_col]\n",
    "            risk_score = (df[med_col] + 2.0 * df[high_col]) / (total_locations + 1e-6)\n",
    "            weighted_risk = risk_score * (df[\"percent_country_production\"] / 100.0)\n",
    "\n",
    "            df[f\"climate_risk_{risk_type}_score\"] = risk_score\n",
    "            df[f\"climate_risk_{risk_type}_weighted\"] = weighted_risk\n",
    "        return df\n",
    "\n",
    "    def _add_stress_summaries(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        temperature_risks = [\"heat_stress\", \"unseasonably_cold\"]\n",
    "        precipitation_risks = [\"excess_precip\", \"drought\"]\n",
    "\n",
    "        temp_scores = [f\"climate_risk_{r}_score\" for r in temperature_risks]\n",
    "        precip_scores = [f\"climate_risk_{r}_score\" for r in precipitation_risks]\n",
    "        all_scores = [f\"climate_risk_{r}_score\" for r in self.risk_categories]\n",
    "\n",
    "        df[\"climate_risk_temperature_stress\"] = df[temp_scores].max(axis=1)\n",
    "        df[\"climate_risk_precipitation_stress\"] = df[precip_scores].max(axis=1)\n",
    "        df[\"climate_risk_overall_stress\"] = df[all_scores].max(axis=1)\n",
    "        df[\"climate_risk_combined_stress\"] = df[all_scores].mean(axis=1)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _sort_region_date(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df.sort_values([\"region_id\", \"date_on\"])\n",
    "\n",
    "    def _add_rolling_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        for window in self.windows:\n",
    "            for risk_type in self.risk_categories:\n",
    "                score_col = f\"climate_risk_{risk_type}_score\"\n",
    "                df[f\"climate_risk_{risk_type}_ma_{window}d\"] = (\n",
    "                    df.groupby(\"region_id\")[score_col]\n",
    "                    .rolling(window=window, min_periods=1)\n",
    "                    .mean()\n",
    "                    .reset_index(level=0, drop=True)\n",
    "                )\n",
    "                df[f\"climate_risk_{risk_type}_max_{window}d\"] = (\n",
    "                    df.groupby(\"region_id\")[score_col]\n",
    "                    .rolling(window=window, min_periods=1)\n",
    "                    .max()\n",
    "                    .reset_index(level=0, drop=True)\n",
    "                )\n",
    "        return df\n",
    "\n",
    "    def _add_changes_and_accel(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        for risk_type in self.risk_categories:\n",
    "            score_col = f\"climate_risk_{risk_type}_score\"\n",
    "            df[f\"climate_risk_{risk_type}_change_1d\"] = df.groupby(\"region_id\")[score_col].diff(1)\n",
    "            df[f\"climate_risk_{risk_type}_change_7d\"] = df.groupby(\"region_id\")[score_col].diff(7)\n",
    "            df[f\"climate_risk_{risk_type}_acceleration\"] = (\n",
    "                df.groupby(\"region_id\")[f\"climate_risk_{risk_type}_change_1d\"].diff(1)\n",
    "            )\n",
    "        return df\n",
    "\n",
    "    def _merge_country_aggregates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        out = df\n",
    "        for risk_type in self.risk_categories:\n",
    "            score_col = f\"climate_risk_{risk_type}_score\"\n",
    "            weighted_col = f\"climate_risk_{risk_type}_weighted\"\n",
    "\n",
    "            country_agg = (\n",
    "                out.groupby([\"country_name\", \"date_on\"]).agg(\n",
    "                    {\n",
    "                        score_col: [\"mean\", \"max\", \"std\"],\n",
    "                        weighted_col: \"sum\",\n",
    "                        \"percent_country_production\": \"sum\",\n",
    "                    }\n",
    "                ).round(4)\n",
    "            )\n",
    "            country_agg.columns = [f\"country_{risk_type}_{'_'.join(col).strip()}\" for col in country_agg.columns]\n",
    "            country_agg = country_agg.reset_index()\n",
    "            out = out.merge(country_agg, on=[\"country_name\", \"date_on\"], how=\"left\")\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def _dropna_copy(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df.dropna().copy()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Country-date signals\n",
    "# -------------------------\n",
    "class _CountryDaySignals:\n",
    "    @staticmethod\n",
    "    def build(df_main: pd.DataFrame, df_share: pd.DataFrame, weight_mode: str) -> pd.DataFrame:\n",
    "        weight_mode = (weight_mode or \"\").strip()\n",
    "\n",
    "        regions = df_main[[\"region_id\", \"country_name\"]].drop_duplicates().merge(\n",
    "            df_share[[\"region_id\", \"percent_country_production\"]],\n",
    "            on=\"region_id\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        def cnt_cols(risk_type: str):\n",
    "            return (\n",
    "                f\"climate_risk_cnt_locations_{risk_type}_risk_low\",\n",
    "                f\"climate_risk_cnt_locations_{risk_type}_risk_medium\",\n",
    "                f\"climate_risk_cnt_locations_{risk_type}_risk_high\",\n",
    "            )\n",
    "\n",
    "        # loc_proxy (used by share_plus_locations)\n",
    "        tot_cols = []\n",
    "        for rt in [\"heat_stress\", \"unseasonably_cold\", \"excess_precip\", \"drought\"]:\n",
    "            L, M, H = cnt_cols(rt)\n",
    "            tot_cols.append(df_main[L] + df_main[M] + df_main[H])\n",
    "        tmp_tot = pd.concat(tot_cols, axis=1)\n",
    "        tmp_tot.columns = [\"tot_heat\", \"tot_cold\", \"tot_wet\", \"tot_dry\"]\n",
    "        loc_proxy = (\n",
    "            pd.concat([df_main[[\"region_id\"]], tmp_tot], axis=1)\n",
    "            .groupby(\"region_id\")[[\"tot_heat\", \"tot_cold\", \"tot_wet\", \"tot_dry\"]]\n",
    "            .mean()\n",
    "            .mean(axis=1)\n",
    "        ).to_dict()\n",
    "        regions[\"loc_proxy\"] = regions[\"region_id\"].map(loc_proxy).fillna(1.0).astype(np.float64)\n",
    "\n",
    "        w_share = regions[\"percent_country_production\"].astype(np.float64).to_numpy()\n",
    "\n",
    "        if weight_mode == \"share_only_norm\":\n",
    "            w_raw = np.where(np.isfinite(w_share), w_share, 0.0)\n",
    "            w_raw = np.where(w_raw <= 0, 0.0, w_raw)\n",
    "            regions[\"_w_raw\"] = w_raw\n",
    "            sums = regions.groupby(\"country_name\")[\"_w_raw\"].transform(\"sum\").to_numpy(np.float64)\n",
    "            w_raw = np.where(sums <= 0, 1.0, w_raw)\n",
    "\n",
    "        elif weight_mode == \"share_plus_locations\":\n",
    "            loc = regions[\"loc_proxy\"].to_numpy(np.float64)\n",
    "            w_raw = np.where(np.isfinite(w_share) & (w_share > 0), w_share, loc)\n",
    "            w_raw = np.where(np.isfinite(w_raw), w_raw, 1.0)\n",
    "            w_raw = np.where(w_raw <= 0, 1.0, w_raw)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown weight_mode: {weight_mode}\")\n",
    "\n",
    "        regions[\"w_raw\"] = w_raw\n",
    "        regions[\"prod_w\"] = regions[\"w_raw\"] / regions.groupby(\"country_name\")[\"w_raw\"].transform(\"sum\")\n",
    "        w_map = regions.set_index(\"region_id\")[\"prod_w\"]\n",
    "\n",
    "        # Needed counts only: heat/cold (sev2) + wet (sev1)\n",
    "        H_L, H_M, H_H = cnt_cols(\"heat_stress\")\n",
    "        C_L, C_M, C_H = cnt_cols(\"unseasonably_cold\")\n",
    "        W_L, W_M, W_H = cnt_cols(\"excess_precip\")\n",
    "\n",
    "        df_feat = df_main[\n",
    "            [\"date_on\", \"country_name\", \"region_id\", H_L, H_M, H_H, C_L, C_M, C_H, W_L, W_M, W_H]\n",
    "        ].copy()\n",
    "        df_feat[\"prod_w\"] = df_feat[\"region_id\"].map(w_map).fillna(0.0).astype(np.float64)\n",
    "\n",
    "        eps = 1e-6\n",
    "        prod = df_feat[\"prod_w\"].to_numpy(np.float64)\n",
    "\n",
    "        def sev2(low, med, high):\n",
    "            tot = low + med + high\n",
    "            return (med + 2.0 * high) / (tot + eps)\n",
    "\n",
    "        def sev1(low, med, high):\n",
    "            tot = low + med + high\n",
    "            return (med + 1.0 * high) / (tot + eps)\n",
    "\n",
    "        heat_sev = sev2(\n",
    "            df_feat[H_L].to_numpy(np.float64),\n",
    "            df_feat[H_M].to_numpy(np.float64),\n",
    "            df_feat[H_H].to_numpy(np.float64),\n",
    "        )\n",
    "        cold_sev = sev2(\n",
    "            df_feat[C_L].to_numpy(np.float64),\n",
    "            df_feat[C_M].to_numpy(np.float64),\n",
    "            df_feat[C_H].to_numpy(np.float64),\n",
    "        )\n",
    "        wet_wapr = sev1(\n",
    "            df_feat[W_L].to_numpy(np.float64),\n",
    "            df_feat[W_M].to_numpy(np.float64),\n",
    "            df_feat[W_H].to_numpy(np.float64),\n",
    "        )\n",
    "\n",
    "        df_feat[\"w_heat_sev\"] = heat_sev * prod\n",
    "        df_feat[\"w_cold_sev\"] = cold_sev * prod\n",
    "        df_feat[\"w_wet_wapr\"] = wet_wapr * prod\n",
    "\n",
    "        cd = (\n",
    "            df_feat.groupby([\"country_name\", \"date_on\"], sort=False)[\n",
    "                [\"prod_w\", \"w_heat_sev\", \"w_cold_sev\", \"w_wet_wapr\"]\n",
    "            ]\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        den = cd[\"prod_w\"].replace(0.0, np.nan).to_numpy(np.float64)\n",
    "        cd[\"wet_wapr_wmean\"] = cd[\"w_wet_wapr\"].to_numpy(np.float64) / den\n",
    "        cd[\"heat_sev_wmean\"] = cd[\"w_heat_sev\"].to_numpy(np.float64) / den\n",
    "        cd[\"cold_sev_wmean\"] = cd[\"w_cold_sev\"].to_numpy(np.float64) / den\n",
    "        cd[\"temp_stress_max\"] = cd[[\"heat_sev_wmean\", \"cold_sev_wmean\"]].max(axis=1)\n",
    "\n",
    "        out = cd[[\"country_name\", \"date_on\", \"wet_wapr_wmean\", \"temp_stress_max\"]].copy()\n",
    "        out = out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# End-to-end job wrapper\n",
    "# -------------------------\n",
    "class _SubmissionJob:\n",
    "    def __init__(self, cfg: dict, feature_col: str):\n",
    "        self.cfg = cfg\n",
    "        self.feature_col = feature_col\n",
    "\n",
    "    def run(self) -> None:\n",
    "        df_main, market_share_df = self._load_inputs()\n",
    "        base = _BaseRowsetBuilder(market_share_df).build(df_main)\n",
    "\n",
    "        df_share = market_share_df[[\"region_id\", \"percent_country_production\"]].copy()\n",
    "        cd_all = self._build_cd_all(base, df_share)\n",
    "\n",
    "        s_feat = self._compute_feature_series(cd_all)\n",
    "        self._attach_and_gate_feature(base, s_feat)\n",
    "\n",
    "        out = self._make_submission_frame(base)\n",
    "        out.to_csv(OUT_PATH, index=False)\n",
    "\n",
    "        print(\"Using CONFIG_KEY =\", CONFIG_KEY)\n",
    "        print(\"Wrote:\", OUT_PATH, \"| rows:\", len(out), \"| cols:\", out.shape[1])\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_inputs():\n",
    "        df = pd.read_csv(MAIN_CSV)\n",
    "        df[\"date_on\"] = pd.to_datetime(df[\"date_on\"], errors=\"coerce\")\n",
    "        market_share_df = pd.read_csv(SHARE_CSV)\n",
    "        return df, market_share_df\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_cd_all(base: pd.DataFrame, df_share: pd.DataFrame) -> pd.DataFrame:\n",
    "        cd_all = None\n",
    "        for wm in (\"share_only_norm\", \"share_plus_locations\"):\n",
    "            cd_wm = _CountryDaySignals.build(base, df_share, wm)\n",
    "            sig_cols = [c for c in cd_wm.columns if c not in (\"country_name\", \"date_on\")]\n",
    "            cd_wm = cd_wm.rename(columns={c: f\"{wm}__{c}\" for c in sig_cols})\n",
    "            cd_all = cd_wm if cd_all is None else cd_all.merge(cd_wm, on=[\"country_name\", \"date_on\"], how=\"inner\")\n",
    "        return cd_all\n",
    "\n",
    "    def _compute_feature_series(self, cd_all: pd.DataFrame) -> pd.Series:\n",
    "        country = self.cfg[\"country\"]\n",
    "        signal = self.cfg[\"signal\"]\n",
    "        preproc = self.cfg.get(\"preproc\", \"raw\")\n",
    "        agg = self.cfg[\"agg\"]\n",
    "        w = int(self.cfg[\"w\"])\n",
    "        shift = int(self.cfg[\"shift\"])\n",
    "        transform = self.cfg[\"transform\"]\n",
    "\n",
    "        cd_c = cd_all.loc[cd_all[\"country_name\"] == country].sort_values(\"date_on\").reset_index(drop=True)\n",
    "        if cd_c.empty:\n",
    "            raise RuntimeError(f\"No rows found for country={country}\")\n",
    "        if signal not in cd_c.columns:\n",
    "            raise RuntimeError(f\"Signal '{signal}' not found in cd_all.\")\n",
    "\n",
    "        x_full = _Ops.apply_preproc(cd_c[signal].to_numpy(np.float64), preproc)\n",
    "        x_shift = _Ops.shift_array(x_full, shift)\n",
    "        x_agg = _Ops.apply_agg(x_shift, agg, w)\n",
    "        x_final = _Ops.apply_transform(x_agg, transform)\n",
    "\n",
    "        s_feat = pd.Series(x_final, index=cd_c[\"date_on\"]).replace([np.inf, -np.inf], np.nan)\n",
    "        s_feat = s_feat.ffill().bfill().fillna(0.0)\n",
    "        return s_feat\n",
    "\n",
    "    def _attach_and_gate_feature(self, base: pd.DataFrame, s_feat: pd.Series) -> None:\n",
    "        country = self.cfg[\"country\"]\n",
    "        month = int(self.cfg[\"month\"])\n",
    "\n",
    "        base[self.feature_col] = 0.0\n",
    "        mask_country = base[\"country_name\"] == country\n",
    "        base.loc[mask_country, self.feature_col] = (\n",
    "            base.loc[mask_country, \"date_on\"].map(s_feat).fillna(0.0).astype(np.float64)\n",
    "        )\n",
    "\n",
    "        mask_gate = mask_country & (base[\"date_on\"].dt.month == month)\n",
    "        base.loc[~mask_gate, self.feature_col] = 0.0\n",
    "\n",
    "        nz = int((base[self.feature_col] != 0).sum())\n",
    "        var_gate = float(base.loc[mask_gate, self.feature_col].var())\n",
    "        if nz < 10 or (not np.isfinite(var_gate)) or var_gate < 1e-12:\n",
    "            raise RuntimeError(\n",
    "                \"Feature looks degenerate (almost all zeros or near-constant in gated bucket). \"\n",
    "                \"This can yield a 0.0000 score.\"\n",
    "            )\n",
    "\n",
    "    def _make_submission_frame(self, base: pd.DataFrame) -> pd.DataFrame:\n",
    "        futures_cols = [c for c in base.columns if c.startswith(\"futures_\")]\n",
    "        if not futures_cols:\n",
    "            raise RuntimeError(\"No futures_* columns found in base.\")\n",
    "\n",
    "        meta_cols = [c for c in base.columns if (not c.startswith(\"climate_risk_\")) and (not c.startswith(\"futures_\"))]\n",
    "        out = base[meta_cols + futures_cols + [self.feature_col]].copy()\n",
    "\n",
    "        out[\"date_on\"] = pd.to_datetime(out[\"date_on\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        clim_cols = [c for c in out.columns if c.startswith(\"climate_risk_\")]\n",
    "        if clim_cols != [self.feature_col]:\n",
    "            raise RuntimeError(f\"Unexpected climate columns present: {clim_cols}\")\n",
    "        if out.isna().any().any():\n",
    "            raise RuntimeError(\"Submission contains NaNs.\")\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Execute\n",
    "# -------------------------\n",
    "_SubmissionJob(cfg, FEATURE_COL).run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ead2c",
   "metadata": {
    "papermill": {
     "duration": 0.003151,
     "end_time": "2026-01-29T09:40:20.500322",
     "exception": false,
     "start_time": "2026-01-29T09:40:20.497171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The notebook above was created by selecting for configurations with nonnegative shifts after running sweep_alt_factor_anomaly.py with the following command: \n",
    "<pre>\n",
    "# --- Threading hygiene (avoid BLAS oversubscription) ---\n",
    "$env:OMP_NUM_THREADS=\"1\"\n",
    "$env:MKL_NUM_THREADS=\"1\"\n",
    "$env:OPENBLAS_NUM_THREADS=\"1\"\n",
    "$env:NUMEXPR_NUM_THREADS=\"1\"\n",
    "\n",
    "# --- Paths (same folder structure as your previous runs) ---\n",
    "$MainCsv  = \".\\forecasting-the-future-the-helios-corn-climate-challenge\\corn_climate_risk_futures_daily_master.csv\"\n",
    "$ShareCsv = \".\\forecasting-the-future-the-helios-corn-climate-challenge\\corn_regional_market_share.csv\"\n",
    "\n",
    "if (!(Test-Path $MainCsv))  { throw \"Missing main CSV:  $MainCsv\" }\n",
    "if (!(Test-Path $ShareCsv)) { throw \"Missing share CSV: $ShareCsv\" }\n",
    "\n",
    "# --- Fresh output dir (do NOT resume into an old config dir) ---\n",
    "$Stamp  = Get-Date -Format \"yyyyMMdd_HHmmss\"\n",
    "$OutDir = \".\\alt_sweep_out_15h_$Stamp\"\n",
    "New-Item -ItemType Directory -Force -Path $OutDir | Out-Null\n",
    "\n",
    "python -u -W ignore::SyntaxWarning .\\sweep_alt_factor_anomaly.py `\n",
    "  --main_csv  $MainCsv `\n",
    "  --share_csv $ShareCsv `\n",
    "  --out_dir   $OutDir `\n",
    "  --n_jobs 24 `\n",
    "  --resume `\n",
    "  --weight_modes \"share_norm_fill1,share_only_norm,share_plus_locations\" `\n",
    "  --variants \"raw,zdoy\" `\n",
    "  --aggs \"ma,std,ewm,streakq85,streakthr0.5\" `\n",
    "  --transforms \"identity,square,signlog1p\" `\n",
    "  --coarse_windows \"2,3,5,7,10,14,21,30,45,60,90,98,102,112,120,140,168,180,200,224,240,252,280,300,330,365,400,450,540,600,730,900,1095,1500,2000,2500\" `\n",
    "  --coarse_shifts \"0,-120,-90,-60,-45,-30,-15,15,30,45,60,90,120\" `\n",
    "  --screen_top_k_per_month 30 `\n",
    "  --screen_keep_per_bucket 8 `\n",
    "  --screen_min_abs_corr 0.15 `\n",
    "  --min_rows_per_month 40 `\n",
    "  --val_years_list \"2,3,4,5\" `\n",
    "  --w_min 2 `\n",
    "  --w_max 2500 `\n",
    "  --shift_min -120 `\n",
    "  --shift_max 120 `\n",
    "  --refine_w_radius 160 `\n",
    "  --refine_shift_radius 75 `\n",
    "  --save_top 20000\n",
    "</pre>\n",
    "\n",
    "This was the sweep_alt_factor_anomaly.py script:\n",
    "\n",
    "<pre>\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"sweep_alt_factor_anomaly.py\n",
    "\n",
    "This script uses a *factor-screen + seasonal de-trending* workflow:\n",
    "\n",
    "1) **Seasonal anomaly variants**\n",
    "   Markets often care about *surprises* vs the seasonal norm. We create\n",
    "   day-of-year z-scores (zDOY) for every country-date climate signal.\n",
    "\n",
    "2) **Futures factor screening**\n",
    "   For each (country, month) bucket, we compute a weighted PC1 factor of the\n",
    "   futures columns and screen candidate climate features by correlation to this\n",
    "   factor. This quickly finds features that should correlate with *many* futures\n",
    "   columns (helpful for SigCount/AvgSig CFCS components).\n",
    "\n",
    "3) **Coordinate-descent refinement**\n",
    "   Instead of scanning huge grids, we refine (window, shift, transform) around\n",
    "   the screened candidates using a few focused sweeps.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# ---- Threading hygiene (avoid oversubscription when using many threads) ----\n",
    "import os as _os\n",
    "_os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "_os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "_os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "_os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "import argparse\n",
    "import heapq\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from joblib import Parallel, delayed\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"joblib is required. Install via: pip install joblib\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Logging\n",
    "# =============================================================================\n",
    "\n",
    "def _now_str() -> str:\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "\n",
    "def log(msg: str) -> None:\n",
    "    print(f\"[{_now_str()}] {msg}\", flush=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Argument parsing helpers\n",
    "# =============================================================================\n",
    "\n",
    "def parse_csv_list(s: str) -> List[str]:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    return [p.strip() for p in s.split(\",\") if p.strip()]\n",
    "\n",
    "\n",
    "def parse_ints(s: str) -> List[int]:\n",
    "    out: List[int] = []\n",
    "    for tok in parse_csv_list(s):\n",
    "        out.append(int(tok))\n",
    "    return out\n",
    "\n",
    "\n",
    "def ensure_dir(p: Path) -> None:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Metric: CFCS (Kaggle-style rounding + significance threshold)\n",
    "# =============================================================================\n",
    "\n",
    "def weighted_corr(x: np.ndarray, y: np.ndarray, w: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    \"\"\"Weighted Pearson correlation.\n",
    "\n",
    "    If you duplicated each observation i exactly w[i] times, the ordinary Pearson\n",
    "    correlation on that expanded dataset matches this weighted correlation.\n",
    "\n",
    "    We treat non-finite values as missing.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    y = np.asarray(y, dtype=np.float64)\n",
    "    w = np.asarray(w, dtype=np.float64)\n",
    "\n",
    "    m = np.isfinite(x) & np.isfinite(y) & np.isfinite(w) & (w > 0)\n",
    "    if m.sum() < 3:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    xv = x[m]\n",
    "    yv = y[m]\n",
    "    wv = w[m]\n",
    "    sw = float(wv.sum())\n",
    "    if sw <= eps:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    mx = float((wv * xv).sum() / sw)\n",
    "    my = float((wv * yv).sum() / sw)\n",
    "\n",
    "    dx = xv - mx\n",
    "    dy = yv - my\n",
    "\n",
    "    num = float((wv * dx * dy).sum())\n",
    "    denx = float((wv * dx * dx).sum())\n",
    "    deny = float((wv * dy * dy).sum())\n",
    "    if denx <= eps or deny <= eps:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    return float(num / math.sqrt(denx * deny))\n",
    "\n",
    "\n",
    "def corr_vector_weighted(z: np.ndarray, Y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Correlation between z (n,) and each Y[:,j] (n,k) under weights w (n,).\"\"\"\n",
    "    z = np.asarray(z, dtype=np.float64)\n",
    "    Y = np.asarray(Y, dtype=np.float64)\n",
    "    w = np.asarray(w, dtype=np.float64)\n",
    "\n",
    "    out = np.full((Y.shape[1],), np.nan, dtype=np.float64)\n",
    "    for j in range(Y.shape[1]):\n",
    "        out[j] = weighted_corr(z, Y[:, j], w)\n",
    "    return out\n",
    "\n",
    "\n",
    "def cfcs_from_corrs(corrs_1d: np.ndarray) -> Tuple[float, Dict[str, Any]]:\n",
    "    s = pd.Series(np.asarray(corrs_1d, dtype=np.float64)).dropna()\n",
    "    if s.empty:\n",
    "        meta = dict(\n",
    "            cfcs_score=0.0,\n",
    "            avg_significant_correlation=0.0,\n",
    "            max_abs_correlation=0.0,\n",
    "            significant_correlations_pct=0.0,\n",
    "            total_correlations=0,\n",
    "            significant_correlations=0,\n",
    "        )\n",
    "        return 0.0, meta\n",
    "\n",
    "    # Kaggle rounds correlations before thresholding\n",
    "    s = s.round(5)\n",
    "    abs_corrs = s.abs()\n",
    "    sig = abs_corrs[abs_corrs >= 0.5]\n",
    "    sig_count = int(sig.shape[0])\n",
    "    total = int(abs_corrs.shape[0])\n",
    "\n",
    "    if sig_count > 0:\n",
    "        avg_sig = float(sig.mean())\n",
    "        avg_sig_score = min(100.0, avg_sig * 100.0)\n",
    "    else:\n",
    "        avg_sig = 0.0\n",
    "        avg_sig_score = 0.0\n",
    "\n",
    "    max_abs = float(abs_corrs.max())\n",
    "    max_score = min(100.0, max_abs * 100.0)\n",
    "\n",
    "    sig_pct = (sig_count / total) * 100.0 if total else 0.0\n",
    "    cfcs = (0.5 * avg_sig_score) + (0.3 * max_score) + (0.2 * sig_pct)\n",
    "\n",
    "    meta = dict(\n",
    "        cfcs_score=float(round(cfcs, 2)),\n",
    "        avg_significant_correlation=float(round(avg_sig, 4)),\n",
    "        max_abs_correlation=float(round(max_abs, 4)),\n",
    "        significant_correlations_pct=float(round(sig_pct, 2)),\n",
    "        total_correlations=total,\n",
    "        significant_correlations=sig_count,\n",
    "    )\n",
    "    return float(round(cfcs, 2)), meta\n",
    "\n",
    "\n",
    "def cfcs_score_weighted(z: np.ndarray, Y: np.ndarray, w: np.ndarray) -> Tuple[float, Dict[str, Any]]:\n",
    "    return cfcs_from_corrs(corr_vector_weighted(z, Y, w))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Rolling/shift/transform utilities (submission-like semantics)\n",
    "# =============================================================================\n",
    "\n",
    "def shift_array(x: np.ndarray, shift: int) -> np.ndarray:\n",
    "    \"\"\"pandas-like shift: out[t] = x[t - shift].\"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    n = x.shape[0]\n",
    "    out = np.full((n,), np.nan, dtype=np.float64)\n",
    "    if shift == 0:\n",
    "        out[:] = x\n",
    "        return out\n",
    "    if shift > 0:\n",
    "        out[shift:] = x[: n - shift]\n",
    "    else:\n",
    "        s = -shift\n",
    "        out[: n - s] = x[s:]\n",
    "    return out\n",
    "\n",
    "\n",
    "def ffill_bfill_fill0(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Forward-fill, then backward-fill, then fill remaining NaNs with 0.\"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64).copy()\n",
    "    x[~np.isfinite(x)] = np.nan\n",
    "\n",
    "    # forward fill\n",
    "    last = np.nan\n",
    "    for i in range(x.shape[0]):\n",
    "        if np.isfinite(x[i]):\n",
    "            last = x[i]\n",
    "        else:\n",
    "            x[i] = last\n",
    "\n",
    "    # backward fill\n",
    "    last = np.nan\n",
    "    for i in range(x.shape[0] - 1, -1, -1):\n",
    "        if np.isfinite(x[i]):\n",
    "            last = x[i]\n",
    "        else:\n",
    "            x[i] = last\n",
    "\n",
    "    x[~np.isfinite(x)] = 0.0\n",
    "    return x\n",
    "\n",
    "\n",
    "def rolling_mean_min1(x: np.ndarray, w: int) -> np.ndarray:\n",
    "    \"\"\"Rolling mean with min_periods=1, ignoring NaNs, vectorized.\"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    n = x.shape[0]\n",
    "    if w <= 1:\n",
    "        return x.copy()\n",
    "\n",
    "    xx = np.where(np.isfinite(x), x, 0.0)\n",
    "    m = np.isfinite(x).astype(np.int32)\n",
    "\n",
    "    cs = np.concatenate([[0.0], np.cumsum(xx)])\n",
    "    cnt = np.concatenate([[0], np.cumsum(m)])\n",
    "\n",
    "    idx = np.arange(n, dtype=np.int32)\n",
    "    j0 = np.maximum(0, idx - w + 1)\n",
    "\n",
    "    s = cs[idx + 1] - cs[j0]\n",
    "    c = cnt[idx + 1] - cnt[j0]\n",
    "\n",
    "    out = np.full((n,), np.nan, dtype=np.float64)\n",
    "    good = c > 0\n",
    "    out[good] = s[good] / c[good]\n",
    "    return out\n",
    "\n",
    "\n",
    "def rolling_std_min1(x: np.ndarray, w: int) -> np.ndarray:\n",
    "    \"\"\"Rolling std with min_periods=1, ignoring NaNs, vectorized.\"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    n = x.shape[0]\n",
    "    if w <= 1:\n",
    "        return np.zeros_like(x, dtype=np.float64)\n",
    "\n",
    "    xx = np.where(np.isfinite(x), x, 0.0)\n",
    "    m = np.isfinite(x).astype(np.int32)\n",
    "\n",
    "    cs = np.concatenate([[0.0], np.cumsum(xx)])\n",
    "    cs2 = np.concatenate([[0.0], np.cumsum(xx * xx)])\n",
    "    cnt = np.concatenate([[0], np.cumsum(m)])\n",
    "\n",
    "    idx = np.arange(n, dtype=np.int32)\n",
    "    j0 = np.maximum(0, idx - w + 1)\n",
    "\n",
    "    s = cs[idx + 1] - cs[j0]\n",
    "    s2 = cs2[idx + 1] - cs2[j0]\n",
    "    c = cnt[idx + 1] - cnt[j0]\n",
    "\n",
    "    out = np.full((n,), np.nan, dtype=np.float64)\n",
    "    good = c > 0\n",
    "    mean = np.zeros_like(out)\n",
    "    mean[good] = s[good] / c[good]\n",
    "    var = np.zeros_like(out)\n",
    "    var[good] = np.maximum(0.0, (s2[good] / c[good]) - (mean[good] * mean[good]))\n",
    "    out[good] = np.sqrt(var[good])\n",
    "    return out\n",
    "\n",
    "\n",
    "def ewm_mean(x: np.ndarray, span: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    n = x.shape[0]\n",
    "    if span <= 1:\n",
    "        return x.copy()\n",
    "    out = np.full((n,), np.nan, dtype=np.float64)\n",
    "    alpha = 2.0 / (span + 1.0)\n",
    "    prev = np.nan\n",
    "    for i in range(n):\n",
    "        xi = x[i]\n",
    "        if not np.isfinite(xi):\n",
    "            out[i] = prev\n",
    "            continue\n",
    "        if not np.isfinite(prev):\n",
    "            prev = xi\n",
    "        else:\n",
    "            prev = (1.0 - alpha) * prev + alpha * xi\n",
    "        out[i] = prev\n",
    "    return out\n",
    "\n",
    "\n",
    "def streak_fraction(x: np.ndarray, w: int, thr: float) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        b = (x >= thr).astype(np.float64)\n",
    "    b[~np.isfinite(x)] = np.nan\n",
    "    return rolling_mean_min1(b, w)\n",
    "\n",
    "\n",
    "def apply_agg(x_shift: np.ndarray, agg: str, w: int) -> np.ndarray:\n",
    "    agg = (agg or \"\").strip().lower()\n",
    "    if agg == \"ma\":\n",
    "        return rolling_mean_min1(x_shift, int(w))\n",
    "    if agg == \"std\":\n",
    "        return rolling_std_min1(x_shift, int(w))\n",
    "    if agg == \"ewm\":\n",
    "        return ewm_mean(x_shift, int(w))\n",
    "    if agg.startswith(\"streakq\"):\n",
    "        q = float(agg.replace(\"streakq\", \"\")) / 100.0\n",
    "        if np.all(~np.isfinite(x_shift)):\n",
    "            thr = 0.0\n",
    "        else:\n",
    "            thr = float(np.nanquantile(x_shift, q))\n",
    "            if not np.isfinite(thr):\n",
    "                thr = 0.0\n",
    "        return streak_fraction(x_shift, int(w), thr)\n",
    "    if agg.startswith(\"streakthr\"):\n",
    "        thr = float(agg.replace(\"streakthr\", \"\"))\n",
    "        return streak_fraction(x_shift, int(w), thr)\n",
    "    raise ValueError(f\"Unsupported agg: {agg}\")\n",
    "\n",
    "\n",
    "def apply_transform(x: np.ndarray, name: str) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    name = (name or \"\").strip().lower()\n",
    "    if name == \"identity\":\n",
    "        return x\n",
    "    if name == \"square\":\n",
    "        return np.sign(x) * (x * x)\n",
    "    if name == \"signlog1p\":\n",
    "        return np.sign(x) * np.log1p(np.abs(x))\n",
    "    raise ValueError(f\"Unknown transform: {name}\")\n",
    "\n",
    "\n",
    "def compute_feature_series(\n",
    "    x_base: np.ndarray,\n",
    "    *,\n",
    "    agg: str,\n",
    "    w: int,\n",
    "    shift: int,\n",
    "    transform: str,\n",
    ") -> np.ndarray:\n",
    "    x_shift = shift_array(x_base, int(shift))\n",
    "    x_agg = apply_agg(x_shift, agg, int(w))\n",
    "    x_tr = apply_transform(x_agg, transform)\n",
    "    x_fill = ffill_bfill_fill0(x_tr)\n",
    "    return x_fill\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Kaggle-aligned row-set (sample-submission style)\n",
    "# =============================================================================\n",
    "\n",
    "RISK_MAP = {\n",
    "    \"heat\": \"heat_stress\",\n",
    "    \"cold\": \"unseasonably_cold\",\n",
    "    \"wet\": \"excess_precip\",\n",
    "    \"dry\": \"drought\",\n",
    "}\n",
    "\n",
    "\n",
    "def build_kaggle_rowset_base(\n",
    "    df: pd.DataFrame,\n",
    "    share_df: pd.DataFrame,\n",
    "    *,\n",
    "    rolling_windows: Sequence[int] = (7, 14, 30),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Recreate the sample-submission engineered dataframe and dropna().\n",
    "\n",
    "    This matches the Kaggle evaluator's effective row-setOW-set (what your submission.py\n",
    "    must map onto), so we get leaderboard-aligned scoring.\n",
    "\n",
    "    We keep this function intentionally close to sample_submission behavior:\n",
    "    - compute region-level risk scores\n",
    "    - rolling mean/max per region\n",
    "    - momentum features (diff/accel) per region\n",
    "    - country-level aggregates (mean/max/std/sums)\n",
    "    - dropna\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"date_on\"] = pd.to_datetime(df[\"date_on\"], errors=\"coerce\")\n",
    "\n",
    "    # merge production shares (sample submission fills missing with 1.0)\n",
    "    share = share_df[[\"region_id\", \"percent_country_production\"]].copy()\n",
    "    df = df.merge(share, on=\"region_id\", how=\"left\")\n",
    "    df[\"percent_country_production\"] = df[\"percent_country_production\"].astype(np.float64).fillna(1.0)\n",
    "\n",
    "    eps = 1e-6\n",
    "\n",
    "    # compute scores per risk type\n",
    "    score_cols: Dict[str, str] = {}\n",
    "    weighted_cols: Dict[str, str] = {}\n",
    "    for short, kind in RISK_MAP.items():\n",
    "        low = f\"climate_risk_cnt_locations_{kind}_risk_low\"\n",
    "        med = f\"climate_risk_cnt_locations_{kind}_risk_medium\"\n",
    "        high = f\"climate_risk_cnt_locations_{kind}_risk_high\"\n",
    "\n",
    "        tot = (df[low] + df[med] + df[high]).astype(np.float64)\n",
    "        score = (df[med].astype(np.float64) + 2.0 * df[high].astype(np.float64)) / (tot + eps)\n",
    "\n",
    "        score_col = f\"climate_risk_{short}_score\"\n",
    "        df[score_col] = score\n",
    "        score_cols[short] = score_col\n",
    "\n",
    "        wcol = f\"climate_risk_{short}_weighted\"\n",
    "        # sample submission uses (percent/100) weights in a *sum* aggregation\n",
    "        df[wcol] = score * (df[\"percent_country_production\"].astype(np.float64) / 100.0)\n",
    "        weighted_cols[short] = wcol\n",
    "\n",
    "    # composite indices\n",
    "    df[\"climate_risk_composite_max\"] = df[list(score_cols.values())].max(axis=1)\n",
    "    df[\"climate_risk_composite_mean\"] = df[list(score_cols.values())].mean(axis=1)\n",
    "\n",
    "    # region-level rolling features + momentum\n",
    "    df = df.sort_values([\"region_id\", \"date_on\"]).reset_index(drop=True)\n",
    "    for short, sc in score_cols.items():\n",
    "        # rolling\n",
    "        for w in rolling_windows:\n",
    "            df[f\"{sc}_ma_{w}\"] = (\n",
    "                df.groupby(\"region_id\", sort=False)[sc]\n",
    "                  .transform(lambda s: s.rolling(window=w, min_periods=1).mean())\n",
    "            )\n",
    "            df[f\"{sc}_max_{w}\"] = (\n",
    "                df.groupby(\"region_id\", sort=False)[sc]\n",
    "                  .transform(lambda s: s.rolling(window=w, min_periods=1).max())\n",
    "            )\n",
    "\n",
    "        # momentum\n",
    "        df[f\"{sc}_change_1d\"] = df.groupby(\"region_id\", sort=False)[sc].diff(1)\n",
    "        df[f\"{sc}_change_7d\"] = df.groupby(\"region_id\", sort=False)[sc].diff(7)\n",
    "        df[f\"{sc}_acceleration\"] = df.groupby(\"region_id\", sort=False)[f\"{sc}_change_1d\"].diff(1)\n",
    "\n",
    "    # country-level aggregates\n",
    "    for short, sc in score_cols.items():\n",
    "        wcol = weighted_cols[short]\n",
    "        agg = (\n",
    "            df.groupby([\"country_name\", \"date_on\"], sort=False)\n",
    "              .agg(\n",
    "                  **{\n",
    "                      f\"country_{short}_score_mean\": (sc, \"mean\"),\n",
    "                      f\"country_{short}_score_max\": (sc, \"max\"),\n",
    "                      f\"country_{short}_score_std\": (sc, \"std\"),\n",
    "                      f\"country_{short}_weighted_sum\": (wcol, \"sum\"),\n",
    "                      f\"country_{short}_production_sum\": (\"percent_country_production\", \"sum\"),\n",
    "                  }\n",
    "              )\n",
    "              .reset_index()\n",
    "        )\n",
    "        df = df.merge(agg, on=[\"country_name\", \"date_on\"], how=\"left\")\n",
    "\n",
    "    # final row-set\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Climate signals (different weighting modes + seasonal variants)\n",
    "# =============================================================================\n",
    "\n",
    "WEIGHT_MODES = (\"share_norm_fill1\", \"share_only_norm\", \"share_plus_locations\")\n",
    "\n",
    "\n",
    "def build_country_day_signals(\n",
    "    df: pd.DataFrame,\n",
    "    share_df: pd.DataFrame,\n",
    "    *,\n",
    "    weight_mode: str,\n",
    "    country_whitelist: Optional[Sequence[str]] = None,\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Build country-date signals (production-weighted means).\n",
    "\n",
    "    Differences vs earlier scripts:\n",
    "    - Supports multiple weighting modes:\n",
    "      * share_norm_fill1: missing share -> 1.0 then normalize (old behavior)\n",
    "      * share_only_norm: missing share -> 0.0 then normalize among known regions\n",
    "      * share_plus_locations: missing share -> mean #locations proxy then normalize\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cd_df: DataFrame with columns [country_name, date_on, year, month, <signals...>]\n",
    "    signal_cols: list of signal column names\n",
    "    \"\"\"\n",
    "\n",
    "    weight_mode = (weight_mode or \"\").strip().lower()\n",
    "    if weight_mode not in WEIGHT_MODES:\n",
    "        raise ValueError(f\"Unknown weight_mode={weight_mode}. Choose from {WEIGHT_MODES}\")\n",
    "\n",
    "    df = df[[\n",
    "        \"country_name\",\n",
    "        \"region_id\",\n",
    "        \"date_on\",\n",
    "        *[f\"climate_risk_cnt_locations_{kind}_risk_{lvl}\" for kind in RISK_MAP.values() for lvl in (\"low\", \"medium\", \"high\")],\n",
    "    ]].copy()\n",
    "\n",
    "    df[\"date_on\"] = pd.to_datetime(df[\"date_on\"], errors=\"coerce\")\n",
    "\n",
    "    # region weights\n",
    "    regions = df[[\"region_id\", \"country_name\"]].drop_duplicates().copy()\n",
    "    regions = regions.merge(share_df[[\"region_id\", \"percent_country_production\"]], on=\"region_id\", how=\"left\")\n",
    "\n",
    "    if weight_mode == \"share_plus_locations\":\n",
    "        # proxy by average total locations in heat-stress counts\n",
    "        heat_low = \"climate_risk_cnt_locations_heat_stress_risk_low\"\n",
    "        heat_med = \"climate_risk_cnt_locations_heat_stress_risk_medium\"\n",
    "        heat_high = \"climate_risk_cnt_locations_heat_stress_risk_high\"\n",
    "        tot_loc = (df[heat_low] + df[heat_med] + df[heat_high]).astype(np.float64)\n",
    "        loc_proxy = df[[\"region_id\"]].copy()\n",
    "        loc_proxy[\"loc_proxy\"] = tot_loc\n",
    "        loc_proxy = loc_proxy.groupby(\"region_id\", sort=False)[\"loc_proxy\"].mean().reset_index()\n",
    "        regions = regions.merge(loc_proxy, on=\"region_id\", how=\"left\")\n",
    "    else:\n",
    "        regions[\"loc_proxy\"] = np.nan\n",
    "\n",
    "    w_raw = regions[\"percent_country_production\"].astype(np.float64)\n",
    "\n",
    "    if weight_mode == \"share_norm_fill1\":\n",
    "        w_raw = w_raw.fillna(1.0)\n",
    "        w_raw = w_raw.where(w_raw > 0, 1.0)\n",
    "    elif weight_mode == \"share_only_norm\":\n",
    "        w_raw = w_raw.fillna(0.0)\n",
    "        w_raw = w_raw.where(w_raw > 0, 0.0)\n",
    "        # fallback: if a country has all zeros, use 1.0 for all its regions\n",
    "        # (avoids producing all-zero signals)\n",
    "        sums = w_raw.groupby(regions[\"country_name\"]).transform(\"sum\")\n",
    "        w_raw = np.where(sums.to_numpy() <= 0, 1.0, w_raw.to_numpy())\n",
    "        w_raw = pd.Series(w_raw, index=regions.index)\n",
    "    elif weight_mode == \"share_plus_locations\":\n",
    "        # if share present and >0 use it; else fallback to loc_proxy; else 1.0\n",
    "        w_loc = regions[\"loc_proxy\"].astype(np.float64)\n",
    "        w_raw = w_raw.where(w_raw.notna() & (w_raw > 0), np.nan)\n",
    "        w_raw = w_raw.fillna(w_loc)\n",
    "        w_raw = w_raw.fillna(1.0)\n",
    "        w_raw = w_raw.where(w_raw > 0, 1.0)\n",
    "\n",
    "    regions[\"w_raw\"] = w_raw.astype(np.float64)\n",
    "\n",
    "    if country_whitelist is not None:\n",
    "        regions = regions[regions[\"country_name\"].isin(list(country_whitelist))].copy()\n",
    "\n",
    "    regions[\"prod_w\"] = regions[\"w_raw\"] / regions.groupby(\"country_name\", sort=False)[\"w_raw\"].transform(\"sum\")\n",
    "    w_map = regions.set_index(\"region_id\")[\"prod_w\"]\n",
    "\n",
    "    df[\"prod_w\"] = df[\"region_id\"].map(w_map).fillna(0.0).astype(np.float64)\n",
    "\n",
    "    eps = 1e-6\n",
    "    tmp = df[[\"country_name\", \"date_on\", \"prod_w\"]].copy()\n",
    "\n",
    "    signal_cols: List[str] = []\n",
    "    for short, kind in RISK_MAP.items():\n",
    "        low = f\"climate_risk_cnt_locations_{kind}_risk_low\"\n",
    "        med = f\"climate_risk_cnt_locations_{kind}_risk_medium\"\n",
    "        high = f\"climate_risk_cnt_locations_{kind}_risk_high\"\n",
    "        tot = (df[low] + df[med] + df[high]).astype(np.float64)\n",
    "\n",
    "        sev2 = (df[med].astype(np.float64) + 2.0 * df[high].astype(np.float64)) / (tot + eps)\n",
    "        sev1 = (df[med].astype(np.float64) + 1.0 * df[high].astype(np.float64)) / (tot + eps)\n",
    "        hi = df[high].astype(np.float64) / (tot + eps)\n",
    "\n",
    "        tmp[f\"w_{short}_sev\"] = tmp[\"prod_w\"] * sev2\n",
    "        tmp[f\"w_{short}_wapr\"] = tmp[\"prod_w\"] * sev1\n",
    "        tmp[f\"w_{short}_high\"] = tmp[\"prod_w\"] * hi\n",
    "\n",
    "    sum_cols = [\"prod_w\"] + [c for c in tmp.columns if c.startswith(\"w_\")]\n",
    "    cd = tmp.groupby([\"country_name\", \"date_on\"], sort=False)[sum_cols].sum().reset_index()\n",
    "\n",
    "    den = cd[\"prod_w\"].to_numpy(np.float64) + 1e-12\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"country_name\": cd[\"country_name\"].astype(str).values,\n",
    "        \"date_on\": pd.to_datetime(cd[\"date_on\"]).values,\n",
    "    })\n",
    "\n",
    "    for short in RISK_MAP.keys():\n",
    "        out[f\"{short}_sev_wmean\"] = (cd[f\"w_{short}_sev\"].to_numpy(np.float64) / den).astype(np.float32)\n",
    "        out[f\"{short}_wapr_wmean\"] = (cd[f\"w_{short}_wapr\"].to_numpy(np.float64) / den).astype(np.float32)\n",
    "        out[f\"{short}_high_wmean\"] = (cd[f\"w_{short}_high\"].to_numpy(np.float64) / den).astype(np.float32)\n",
    "        signal_cols.extend([f\"{short}_sev_wmean\", f\"{short}_wapr_wmean\", f\"{short}_high_wmean\"])\n",
    "\n",
    "    # composites\n",
    "    out[\"wet_dry_diff\"] = (out[\"wet_sev_wmean\"] - out[\"dry_sev_wmean\"]).astype(np.float32)\n",
    "    out[\"wet_dry_wapr_diff\"] = (out[\"wet_wapr_wmean\"] - out[\"dry_wapr_wmean\"]).astype(np.float32)\n",
    "    out[\"temp_stress_max\"] = out[[\"heat_sev_wmean\", \"cold_sev_wmean\"]].max(axis=1).astype(np.float32)\n",
    "    out[\"precip_stress_max\"] = out[[\"wet_sev_wmean\", \"dry_sev_wmean\"]].max(axis=1).astype(np.float32)\n",
    "    out[\"overall_stress_max\"] = out[[\"heat_sev_wmean\", \"cold_sev_wmean\", \"wet_sev_wmean\", \"dry_sev_wmean\"]].max(axis=1).astype(np.float32)\n",
    "    out[\"overall_stress_mean\"] = out[[\"heat_sev_wmean\", \"cold_sev_wmean\", \"wet_sev_wmean\", \"dry_sev_wmean\"]].mean(axis=1).astype(np.float32)\n",
    "    signal_cols.extend([\"wet_dry_diff\", \"wet_dry_wapr_diff\", \"temp_stress_max\", \"precip_stress_max\", \"overall_stress_max\", \"overall_stress_mean\"])\n",
    "\n",
    "    out[\"date_on\"] = pd.to_datetime(out[\"date_on\"])\n",
    "    out[\"year\"] = out[\"date_on\"].dt.year.astype(np.int16)\n",
    "    out[\"month\"] = out[\"date_on\"].dt.month.astype(np.int8)\n",
    "\n",
    "    out = out.sort_values([\"country_name\", \"date_on\"]).reset_index(drop=True)\n",
    "    return out, signal_cols\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Futures factor (PC1) per (country, month)\n",
    "# =============================================================================\n",
    "\n",
    "def futures_pc1_factor(Y: np.ndarray, w: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"Compute a weighted PC1 score series for futures matrix Y (n,k).\"\"\"\n",
    "    Y = np.asarray(Y, dtype=np.float64)\n",
    "    w = np.asarray(w, dtype=np.float64)\n",
    "\n",
    "    m = np.isfinite(Y).all(axis=1) & np.isfinite(w) & (w > 0)\n",
    "    if m.sum() < 5:\n",
    "        return np.full((Y.shape[0],), np.nan, dtype=np.float64)\n",
    "\n",
    "    Ym = Y[m]\n",
    "    wm = w[m]\n",
    "    sw = float(wm.sum())\n",
    "\n",
    "    mu = (wm[:, None] * Ym).sum(axis=0) / sw\n",
    "    Z = Ym - mu\n",
    "    sig = np.sqrt((wm[:, None] * (Z * Z)).sum(axis=0) / sw)\n",
    "    sig = np.where(sig > eps, sig, 1.0)\n",
    "    Z = Z / sig\n",
    "\n",
    "    # weighted covariance (k x k)\n",
    "    C = (Z.T * wm) @ Z / sw\n",
    "\n",
    "    # eigenvector of largest eigenvalue\n",
    "    vals, vecs = np.linalg.eigh(C)\n",
    "    v = vecs[:, int(np.argmax(vals))]\n",
    "\n",
    "    scores = Z @ v\n",
    "\n",
    "    out = np.full((Y.shape[0],), np.nan, dtype=np.float64)\n",
    "    out[m] = scores\n",
    "    # fill gaps to avoid NaNs in correlation\n",
    "    out = ffill_bfill_fill0(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Country data container\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class CountryData:\n",
    "    country: str\n",
    "    dates: np.ndarray          # (n,) datetime64\n",
    "    year: np.ndarray           # (n,) int16\n",
    "    month: np.ndarray          # (n,) int8\n",
    "    weight: np.ndarray         # (n,) float64\n",
    "    Y: np.ndarray              # (n,k) float32\n",
    "    X_raw: np.ndarray          # (n,s) float32\n",
    "    X_zdoy: np.ndarray         # (n,s) float32\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Candidate containers\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ScreenHit:\n",
    "    country: str\n",
    "    month: int\n",
    "    variant: str\n",
    "    signal: str\n",
    "    agg: str\n",
    "    w: int\n",
    "    shift: int\n",
    "    corr_abs: float\n",
    "    corr_signed: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RefinedCandidate:\n",
    "    country: str\n",
    "    month: int\n",
    "    variant: str\n",
    "    signal: str\n",
    "    agg: str\n",
    "    w: int\n",
    "    shift: int\n",
    "    transform: str\n",
    "\n",
    "    proxy_corr_abs: float\n",
    "\n",
    "    # CFCS scores\n",
    "    cfcs_all: float\n",
    "    cfcs_vy2: float\n",
    "    cfcs_vy3: float\n",
    "    cfcs_vy4: float\n",
    "    cfcs_vy5: float\n",
    "    robust_min: float\n",
    "\n",
    "    # optional robustness checks\n",
    "    detrended_vy2: float\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Time subset helpers\n",
    "# =============================================================================\n",
    "\n",
    "def last_n_years_mask(year_arr: np.ndarray, idx: np.ndarray, n: int) -> Optional[np.ndarray]:\n",
    "    \"\"\"Return boolean mask over idx selecting last n unique years within idx.\"\"\"\n",
    "    years = year_arr[idx]\n",
    "    uniq = np.array(sorted(set(int(y) for y in years if np.isfinite(y))), dtype=np.int16)\n",
    "    if uniq.size < n:\n",
    "        return None\n",
    "    last = set(uniq[-n:].tolist())\n",
    "    return np.isin(year_arr, np.array(list(last), dtype=np.int16)) & idx\n",
    "\n",
    "\n",
    "def detrend_by_year(x: np.ndarray, y: np.ndarray, years: np.ndarray, w: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Year-demean x and each column of y within each year (weighted).\"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    y = np.asarray(y, dtype=np.float64)\n",
    "    years = np.asarray(years, dtype=np.int16)\n",
    "    w = np.asarray(w, dtype=np.float64)\n",
    "\n",
    "    xd = x.copy()\n",
    "    yd = y.copy()\n",
    "\n",
    "    for yr in np.unique(years):\n",
    "        m = (years == yr) & np.isfinite(w) & (w > 0)\n",
    "        if m.sum() < 3:\n",
    "            continue\n",
    "        sw = w[m].sum()\n",
    "        if sw <= 0:\n",
    "            continue\n",
    "        mx = (w[m] * xd[m]).sum() / sw\n",
    "        xd[m] = xd[m] - mx\n",
    "\n",
    "        my = (w[m][:, None] * yd[m]).sum(axis=0) / sw\n",
    "        yd[m] = yd[m] - my\n",
    "\n",
    "    return xd, yd, w\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Scoring candidate for a bucket\n",
    "# =============================================================================\n",
    "\n",
    "def score_bucket_cfcs(\n",
    "    *,\n",
    "    z: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    year: np.ndarray,\n",
    "    weight: np.ndarray,\n",
    "    idx_month: np.ndarray,\n",
    "    val_years_list: Sequence[int],\n",
    "    min_rows: int = 40,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Compute CFCS on full data and on last-n-years slices.\"\"\"\n",
    "\n",
    "    out: Dict[str, float] = {}\n",
    "\n",
    "    # full\n",
    "    if idx_month.sum() >= min_rows:\n",
    "        out[\"all\"] = cfcs_score_weighted(z[idx_month], Y[idx_month], weight[idx_month])[0]\n",
    "    else:\n",
    "        out[\"all\"] = float(\"nan\")\n",
    "\n",
    "    for vy in val_years_list:\n",
    "        m = last_n_years_mask(year, idx_month, int(vy))\n",
    "        if m is None or m.sum() < min_rows:\n",
    "            out[f\"vy{vy}\"] = float(\"nan\")\n",
    "        else:\n",
    "            out[f\"vy{vy}\"] = cfcs_score_weighted(z[m], Y[m], weight[m])[0]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def score_bucket_detrended_vy2(\n",
    "    *,\n",
    "    z: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    year: np.ndarray,\n",
    "    weight: np.ndarray,\n",
    "    idx_month: np.ndarray,\n",
    "    min_rows: int = 40,\n",
    ") -> float:\n",
    "    \"\"\"Extra robustness check: CFCS on last-2-years after year-demeaning.\"\"\"\n",
    "    m = last_n_years_mask(year, idx_month, 2)\n",
    "    if m is None or m.sum() < min_rows:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    zd, Yd, wd = detrend_by_year(z[m], Y[m], year[m], weight[m])\n",
    "    return cfcs_score_weighted(zd, Yd, wd)[0]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Screening: stream through coarse candidates, keep top K per month\n",
    "# =============================================================================\n",
    "\n",
    "def screen_one_country(\n",
    "    cdata: CountryData,\n",
    "    *,\n",
    "    signal_names: Sequence[str],\n",
    "    variants: Sequence[str],\n",
    "    aggs: Sequence[str],\n",
    "    windows: Sequence[int],\n",
    "    shifts: Sequence[int],\n",
    "    top_k_per_month: int,\n",
    "    min_abs_corr: float,\n",
    "    min_rows_per_month: int,\n",
    ") -> List[ScreenHit]:\n",
    "    country = cdata.country\n",
    "\n",
    "    # month indices and futures factors\n",
    "    month_idx: Dict[int, np.ndarray] = {}\n",
    "    month_factor: Dict[int, np.ndarray] = {}\n",
    "    for m in range(1, 13):\n",
    "        idx = (cdata.month == m)\n",
    "        if idx.sum() < max(5, min_rows_per_month):\n",
    "            continue\n",
    "        month_idx[m] = idx\n",
    "        month_factor[m] = futures_pc1_factor(cdata.Y[idx], cdata.weight[idx])\n",
    "\n",
    "    if not month_idx:\n",
    "        return []\n",
    "\n",
    "    # Heaps per month\n",
    "    # NOTE: heapq breaks ties by comparing the next tuple element. Since ScreenHit\n",
    "    # is not orderable, include a deterministic tiebreaker integer.\n",
    "    heaps: Dict[int, List[Tuple[float, int, ScreenHit]]] = {m: [] for m in month_idx.keys()}\n",
    "    tie_counter = 0\n",
    "\n",
    "    def push(m: int, hit: ScreenHit) -> None:\n",
    "        nonlocal tie_counter\n",
    "        h = heaps[m]\n",
    "        key = hit.corr_abs\n",
    "        tie_counter += 1\n",
    "        item = (key, tie_counter, hit)\n",
    "        if len(h) < top_k_per_month:\n",
    "            heapq.heappush(h, item)\n",
    "        else:\n",
    "            # min-heap by corr_abs\n",
    "            if key > h[0][0]:\n",
    "                heapq.heapreplace(h, item)\n",
    "\n",
    "\n",
    "    # variant -> matrix\n",
    "    X_by_variant = {\n",
    "        \"raw\": cdata.X_raw,\n",
    "        \"zdoy\": cdata.X_zdoy,\n",
    "    }\n",
    "\n",
    "    for variant in variants:\n",
    "        if variant not in X_by_variant:\n",
    "            continue\n",
    "        X = X_by_variant[variant]\n",
    "\n",
    "        for j, sig_name in enumerate(signal_names):\n",
    "            x_base = X[:, j].astype(np.float64, copy=False)\n",
    "\n",
    "            for shift in shifts:\n",
    "                x_shift = shift_array(x_base, int(shift))\n",
    "\n",
    "                for agg in aggs:\n",
    "                    for w in windows:\n",
    "                        try:\n",
    "                            x_feat = compute_feature_series(\n",
    "                                x_shift,  # already shifted; compute_feature_series will shift again if we pass shift\n",
    "                                agg=agg,\n",
    "                                w=int(w),\n",
    "                                shift=0,\n",
    "                                transform=\"identity\",\n",
    "                            )\n",
    "                        except Exception:\n",
    "                            continue\n",
    "\n",
    "                        # Evaluate in each month bucket\n",
    "                        for m, idx in month_idx.items():\n",
    "                            fac = month_factor[m]\n",
    "                            corr = weighted_corr(x_feat[idx], fac, cdata.weight[idx])\n",
    "                            if not np.isfinite(corr):\n",
    "                                continue\n",
    "                            ac = abs(float(corr))\n",
    "                            if ac < min_abs_corr:\n",
    "                                continue\n",
    "\n",
    "                            push(\n",
    "                                m,\n",
    "                                ScreenHit(\n",
    "                                    country=country,\n",
    "                                    month=int(m),\n",
    "                                    variant=variant,\n",
    "                                    signal=sig_name,\n",
    "                                    agg=agg,\n",
    "                                    w=int(w),\n",
    "                                    shift=int(shift),\n",
    "                                    corr_abs=ac,\n",
    "                                    corr_signed=float(corr),\n",
    "                                ),\n",
    "                            )\n",
    "\n",
    "    hits: List[ScreenHit] = []\n",
    "    for m, h in heaps.items():\n",
    "        # sort descending by corr_abs\n",
    "        hits.extend([t[2] for t in sorted(h, key=lambda z: z[0], reverse=True)])\n",
    "    return hits\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Refinement: coordinate descent around a screened hit\n",
    "# =============================================================================\n",
    "\n",
    "def refine_one_hit(\n",
    "    cdata: CountryData,\n",
    "    *,\n",
    "    signal_names: Sequence[str],\n",
    "    hit: ScreenHit,\n",
    "    transforms: Sequence[str],\n",
    "    val_years_list: Sequence[int],\n",
    "    w_min: int,\n",
    "    w_max: int,\n",
    "    shift_min: int,\n",
    "    shift_max: int,\n",
    "    refine_w_radius: int,\n",
    "    refine_shift_radius: int,\n",
    "    min_rows: int,\n",
    ") -> Optional[RefinedCandidate]:\n",
    "\n",
    "    # locate signal column\n",
    "    try:\n",
    "        j = list(signal_names).index(hit.signal)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "    X = cdata.X_raw if hit.variant == \"raw\" else cdata.X_zdoy\n",
    "    x_base = X[:, j].astype(np.float64, copy=False)\n",
    "\n",
    "    month = int(hit.month)\n",
    "    idx_month = (cdata.month == month)\n",
    "    if idx_month.sum() < min_rows:\n",
    "        return None\n",
    "\n",
    "    # objective helper\n",
    "    def eval_params(w: int, shift: int, transform: str) -> Tuple[float, Dict[str, float], float, np.ndarray]:\n",
    "        z = compute_feature_series(x_base, agg=hit.agg, w=w, shift=shift, transform=transform)\n",
    "        scores = score_bucket_cfcs(\n",
    "            z=z,\n",
    "            Y=cdata.Y,\n",
    "            year=cdata.year,\n",
    "            weight=cdata.weight,\n",
    "            idx_month=idx_month,\n",
    "            val_years_list=val_years_list,\n",
    "            min_rows=min_rows,\n",
    "        )\n",
    "        # robust objective: min over vy scores (ignore NaNs)\n",
    "        vy_scores = [scores.get(f\"vy{vy}\") for vy in val_years_list]\n",
    "        vy_scores = [s for s in vy_scores if np.isfinite(s)]\n",
    "        robust = float(min(vy_scores)) if vy_scores else float(\"nan\")\n",
    "\n",
    "        # proxy = vy2 if present else all\n",
    "        proxy = float(scores.get(\"vy2\", scores.get(\"all\", float(\"nan\"))))\n",
    "\n",
    "        # combined objective (robust-first)\n",
    "        obj = robust + 0.02 * proxy if np.isfinite(robust) and np.isfinite(proxy) else float(\"nan\")\n",
    "        return obj, scores, robust, z\n",
    "\n",
    "    # coordinate descent parameters\n",
    "    w0 = int(hit.w)\n",
    "    s0 = int(hit.shift)\n",
    "\n",
    "    w0 = max(w_min, min(w_max, w0))\n",
    "    s0 = max(shift_min, min(shift_max, s0))\n",
    "\n",
    "    best_overall: Optional[RefinedCandidate] = None\n",
    "    best_obj = -1e18\n",
    "\n",
    "    for transform in transforms:\n",
    "        # start\n",
    "        w_cur, s_cur = w0, s0\n",
    "\n",
    "        # coarse w sweep\n",
    "        w_lo = max(w_min, w_cur - refine_w_radius)\n",
    "        w_hi = min(w_max, w_cur + refine_w_radius)\n",
    "        w_grid = list(range(w_lo, w_hi + 1, 10))\n",
    "        if w_cur not in w_grid:\n",
    "            w_grid.append(w_cur)\n",
    "        w_grid = sorted(set(w_grid))\n",
    "\n",
    "        best_local_obj = -1e18\n",
    "        best_local_scores: Dict[str, float] = {}\n",
    "        best_local_robust = float(\"nan\")\n",
    "        best_local_z = None\n",
    "\n",
    "        for w in w_grid:\n",
    "            obj, scores, robust, z = eval_params(w, s_cur, transform)\n",
    "            if np.isfinite(obj) and obj > best_local_obj:\n",
    "                best_local_obj = obj\n",
    "                w_cur = w\n",
    "                best_local_scores = scores\n",
    "                best_local_robust = robust\n",
    "                best_local_z = z\n",
    "\n",
    "        # coarse shift sweep\n",
    "        s_lo = max(shift_min, s_cur - refine_shift_radius)\n",
    "        s_hi = min(shift_max, s_cur + refine_shift_radius)\n",
    "        s_grid = list(range(s_lo, s_hi + 1, 3))\n",
    "        if s_cur not in s_grid:\n",
    "            s_grid.append(s_cur)\n",
    "        s_grid = sorted(set(s_grid))\n",
    "\n",
    "        for s in s_grid:\n",
    "            obj, scores, robust, z = eval_params(w_cur, s, transform)\n",
    "            if np.isfinite(obj) and obj > best_local_obj:\n",
    "                best_local_obj = obj\n",
    "                s_cur = s\n",
    "                best_local_scores = scores\n",
    "                best_local_robust = robust\n",
    "                best_local_z = z\n",
    "\n",
    "        # fine w sweep around current +/- 10\n",
    "        w_lo2 = max(w_min, w_cur - 10)\n",
    "        w_hi2 = min(w_max, w_cur + 10)\n",
    "        for w in range(w_lo2, w_hi2 + 1):\n",
    "            obj, scores, robust, z = eval_params(w, s_cur, transform)\n",
    "            if np.isfinite(obj) and obj > best_local_obj:\n",
    "                best_local_obj = obj\n",
    "                w_cur = w\n",
    "                best_local_scores = scores\n",
    "                best_local_robust = robust\n",
    "                best_local_z = z\n",
    "\n",
    "        # fine shift sweep around current +/- 5\n",
    "        s_lo2 = max(shift_min, s_cur - 5)\n",
    "        s_hi2 = min(shift_max, s_cur + 5)\n",
    "        for s in range(s_lo2, s_hi2 + 1):\n",
    "            obj, scores, robust, z = eval_params(w_cur, s, transform)\n",
    "            if np.isfinite(obj) and obj > best_local_obj:\n",
    "                best_local_obj = obj\n",
    "                s_cur = s\n",
    "                best_local_scores = scores\n",
    "                best_local_robust = robust\n",
    "                best_local_z = z\n",
    "\n",
    "        if best_local_z is None:\n",
    "            continue\n",
    "\n",
    "        # compute detrended vy2 robustness check\n",
    "        detr2 = score_bucket_detrended_vy2(\n",
    "            z=best_local_z,\n",
    "            Y=cdata.Y,\n",
    "            year=cdata.year,\n",
    "            weight=cdata.weight,\n",
    "            idx_month=idx_month,\n",
    "            min_rows=min_rows,\n",
    "        )\n",
    "\n",
    "        # proxy corr (factor) at final params (for debugging)\n",
    "        fac = futures_pc1_factor(cdata.Y[idx_month], cdata.weight[idx_month])\n",
    "        corr = weighted_corr(best_local_z[idx_month], fac, cdata.weight[idx_month])\n",
    "        corr_abs = abs(float(corr)) if np.isfinite(corr) else float(\"nan\")\n",
    "\n",
    "        cand = RefinedCandidate(\n",
    "            country=cdata.country,\n",
    "            month=month,\n",
    "            variant=hit.variant,\n",
    "            signal=hit.signal,\n",
    "            agg=hit.agg,\n",
    "            w=int(w_cur),\n",
    "            shift=int(s_cur),\n",
    "            transform=transform,\n",
    "            proxy_corr_abs=float(corr_abs),\n",
    "            cfcs_all=float(best_local_scores.get(\"all\", float(\"nan\"))),\n",
    "            cfcs_vy2=float(best_local_scores.get(\"vy2\", float(\"nan\"))),\n",
    "            cfcs_vy3=float(best_local_scores.get(\"vy3\", float(\"nan\"))),\n",
    "            cfcs_vy4=float(best_local_scores.get(\"vy4\", float(\"nan\"))),\n",
    "            cfcs_vy5=float(best_local_scores.get(\"vy5\", float(\"nan\"))),\n",
    "            robust_min=float(best_local_robust),\n",
    "            detrended_vy2=float(detr2),\n",
    "        )\n",
    "\n",
    "        if np.isfinite(best_local_obj) and best_local_obj > best_obj:\n",
    "            best_obj = best_local_obj\n",
    "            best_overall = cand\n",
    "\n",
    "    return best_overall\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Caches\n",
    "# =============================================================================\n",
    "\n",
    "def load_or_build_cache(\n",
    "    *,\n",
    "    main_csv: str,\n",
    "    share_csv: str,\n",
    "    out_dir: Path,\n",
    "    resume: bool,\n",
    "    rebuild_cache: bool,\n",
    "    weight_modes: Sequence[str],\n",
    ") -> Tuple[pd.DataFrame, List[str], pd.DataFrame, List[str]]:\n",
    "    \"\"\"Return (base_cd, futures_cols, signals_df, signal_cols).\"\"\"\n",
    "\n",
    "    cache_dir = out_dir / \"cache\"\n",
    "    ensure_dir(cache_dir)\n",
    "\n",
    "    base_path = cache_dir / \"base_country_date.csv.gz\"\n",
    "    sig_path = cache_dir / f\"cd_signals_{'_'.join([m.replace(',', '') for m in weight_modes])}.csv.gz\"\n",
    "    meta_path = cache_dir / \"cache_meta.json\"\n",
    "\n",
    "    if resume and (not rebuild_cache) and base_path.exists() and sig_path.exists() and meta_path.exists():\n",
    "        log(\"Loading caches...\")\n",
    "        base_cd = pd.read_csv(base_path, compression=\"gzip\", parse_dates=[\"date_on\"])\n",
    "        signals = pd.read_csv(sig_path, compression=\"gzip\", parse_dates=[\"date_on\"])\n",
    "        meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
    "        futures_cols = meta[\"futures_cols\"]\n",
    "        signal_cols = meta[\"signal_cols\"]\n",
    "        return base_cd, futures_cols, signals, signal_cols\n",
    "\n",
    "    log(\"Reading CSVs...\")\n",
    "    df_main = pd.read_csv(main_csv)\n",
    "    df_share = pd.read_csv(share_csv)\n",
    "\n",
    "    futures_cols = [c for c in df_main.columns if c.startswith(\"futures_\")]\n",
    "    if not futures_cols:\n",
    "        raise RuntimeError(\"No futures_* columns found in main_csv\")\n",
    "\n",
    "    log(\"Building Kaggle-aligned base row-set (sample-submission-style + dropna)...\")\n",
    "    base = build_kaggle_rowset_base(df_main, df_share, rolling_windows=(7, 14, 30))\n",
    "    log(f\"Base rows after dropna: {len(base):,}\")\n",
    "\n",
    "    keep = [\"country_name\", \"date_on\", \"region_id\"] + futures_cols\n",
    "    base = base[keep].copy()\n",
    "\n",
    "    log(\"Collapsing to country-date (with region-row weights)...\")\n",
    "    agg_dict = {c: \"first\" for c in futures_cols}\n",
    "    agg_dict[\"region_id\"] = \"size\"  # weight\n",
    "    base_cd = (\n",
    "        base.groupby([\"country_name\", \"date_on\"], sort=False)\n",
    "            .agg(agg_dict)\n",
    "            .reset_index()\n",
    "            .rename(columns={\"region_id\": \"weight\"})\n",
    "    )\n",
    "    base_cd[\"date_on\"] = pd.to_datetime(base_cd[\"date_on\"], errors=\"coerce\")\n",
    "    base_cd[\"year\"] = base_cd[\"date_on\"].dt.year.astype(np.int16)\n",
    "    base_cd[\"month\"] = base_cd[\"date_on\"].dt.month.astype(np.int8)\n",
    "\n",
    "    countries = sorted(base_cd[\"country_name\"].astype(str).unique().tolist())\n",
    "\n",
    "    log(f\"Building climate signals for weight_modes={list(weight_modes)}...\")\n",
    "    all_sig = None\n",
    "    signal_cols: List[str] = []\n",
    "\n",
    "    for mode in weight_modes:\n",
    "        cd_df, cols = build_country_day_signals(df_main, df_share, weight_mode=mode, country_whitelist=countries)\n",
    "        # prefix signal names so they are unique across modes\n",
    "        rename = {c: f\"{mode}__{c}\" for c in cols}\n",
    "        cd_df = cd_df[[\"country_name\", \"date_on\"] + cols].copy()\n",
    "        cd_df = cd_df.rename(columns=rename)\n",
    "        cols2 = [rename[c] for c in cols]\n",
    "\n",
    "        if all_sig is None:\n",
    "            all_sig = cd_df\n",
    "        else:\n",
    "            all_sig = all_sig.merge(cd_df, on=[\"country_name\", \"date_on\"], how=\"outer\")\n",
    "\n",
    "        signal_cols.extend(cols2)\n",
    "\n",
    "    assert all_sig is not None\n",
    "    all_sig[\"date_on\"] = pd.to_datetime(all_sig[\"date_on\"], errors=\"coerce\")\n",
    "    all_sig = all_sig.sort_values([\"country_name\", \"date_on\"]).reset_index(drop=True)\n",
    "\n",
    "    meta = dict(\n",
    "        built_utc=time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "        futures_cols=futures_cols,\n",
    "        signal_cols=signal_cols,\n",
    "        n_base_country_dates=int(base_cd.shape[0]),\n",
    "        n_signal_rows=int(all_sig.shape[0]),\n",
    "        n_signal_cols=int(len(signal_cols)),\n",
    "        weight_modes=list(weight_modes),\n",
    "    )\n",
    "    meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    log(f\"Writing base cache -> {base_path}\")\n",
    "    base_cd.to_csv(base_path, index=False, compression=\"gzip\")\n",
    "\n",
    "    log(f\"Writing signals cache -> {sig_path}\")\n",
    "    all_sig.to_csv(sig_path, index=False, compression=\"gzip\")\n",
    "\n",
    "    return base_cd, futures_cols, all_sig, signal_cols\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Build per-country arrays and zDOY variants\n",
    "# =============================================================================\n",
    "\n",
    "def build_country_data(\n",
    "    base_cd: pd.DataFrame,\n",
    "    futures_cols: Sequence[str],\n",
    "    signals: pd.DataFrame,\n",
    "    signal_cols: Sequence[str],\n",
    ") -> Tuple[List[CountryData], List[str]]:\n",
    "\n",
    "    futures_cols = list(futures_cols)\n",
    "    signal_cols = list(signal_cols)\n",
    "\n",
    "    countries = sorted(base_cd[\"country_name\"].astype(str).unique().tolist())\n",
    "    out: List[CountryData] = []\n",
    "\n",
    "    for country in countries:\n",
    "        b = base_cd.loc[base_cd[\"country_name\"] == country].sort_values(\"date_on\").reset_index(drop=True)\n",
    "        if b.empty:\n",
    "            continue\n",
    "\n",
    "        s = signals.loc[signals[\"country_name\"] == country].sort_values(\"date_on\").reset_index(drop=True)\n",
    "        m = b[[\"date_on\"]].merge(s, on=\"date_on\", how=\"left\")\n",
    "\n",
    "        # fill missing signal values\n",
    "        m = m.sort_values(\"date_on\").reset_index(drop=True)\n",
    "        for c in signal_cols:\n",
    "            if c not in m.columns:\n",
    "                m[c] = np.nan\n",
    "        m[signal_cols] = m[signal_cols].ffill().bfill().fillna(0.0)\n",
    "\n",
    "        X_raw = m[signal_cols].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "        # zDOY\n",
    "        dt = pd.to_datetime(m[\"date_on\"], errors=\"coerce\")\n",
    "        doy = dt.dt.dayofyear.to_numpy(dtype=np.int16)\n",
    "\n",
    "        X_z = np.zeros_like(X_raw, dtype=np.float32)\n",
    "        for j in range(X_raw.shape[1]):\n",
    "            x = X_raw[:, j].astype(np.float64)\n",
    "            # compute per-DOY mean/std\n",
    "            df_tmp = pd.DataFrame({\"doy\": doy, \"x\": x})\n",
    "            g = df_tmp.groupby(\"doy\", sort=False)[\"x\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "            mu_map = dict(zip(g[\"doy\"].astype(int).tolist(), g[\"mean\"].astype(float).tolist()))\n",
    "            sd_map = dict(zip(g[\"doy\"].astype(int).tolist(), g[\"std\"].astype(float).fillna(0.0).tolist()))\n",
    "\n",
    "            mu = np.array([mu_map.get(int(d), 0.0) for d in doy], dtype=np.float64)\n",
    "            sd = np.array([sd_map.get(int(d), 0.0) for d in doy], dtype=np.float64)\n",
    "            sd = np.where(sd > 1e-6, sd, 1.0)\n",
    "            z = (x - mu) / sd\n",
    "            z = np.clip(z, -10.0, 10.0)\n",
    "            X_z[:, j] = z.astype(np.float32)\n",
    "\n",
    "        cdata = CountryData(\n",
    "            country=country,\n",
    "            dates=b[\"date_on\"].to_numpy(),\n",
    "            year=b[\"year\"].to_numpy(dtype=np.int16, copy=False),\n",
    "            month=b[\"month\"].to_numpy(dtype=np.int8, copy=False),\n",
    "            weight=b[\"weight\"].to_numpy(dtype=np.float64, copy=False),\n",
    "            Y=b[list(futures_cols)].to_numpy(dtype=np.float32, copy=False),\n",
    "            X_raw=X_raw,\n",
    "            X_zdoy=X_z,\n",
    "        )\n",
    "        out.append(cdata)\n",
    "\n",
    "    return out, list(signal_cols)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Recommendations writer\n",
    "# =============================================================================\n",
    "\n",
    "def write_recommendations(path: Path, title: str, rows: pd.DataFrame, top_n: int = 25) -> None:\n",
    "    lines: List[str] = []\n",
    "    lines.append(title)\n",
    "    lines.append(\"=\" * len(title))\n",
    "    lines.append(\"\")\n",
    "\n",
    "    if rows.empty:\n",
    "        lines.append(\"(no rows)\")\n",
    "        path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "        return\n",
    "\n",
    "    show = rows.head(top_n).copy()\n",
    "    for i, r in show.iterrows():\n",
    "        lines.append(\n",
    "            f\"#{i+1:03d}  {r['country']}  m{int(r['month']):02d}  {r['variant']}  {r['signal']}  \"\n",
    "            f\"agg={r['agg']} w={int(r['w'])} shift={int(r['shift'])} transform={r['transform']}\"\n",
    "        )\n",
    "        lines.append(\n",
    "            f\"      robust_min={r['robust_min']:.2f}  vy2={r['cfcs_vy2']:.2f}  all={r['cfcs_all']:.2f}  detrended_vy2={r['detrended_vy2']:.2f}\"\n",
    "        )\n",
    "    path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main\n",
    "# =============================================================================\n",
    "\n",
    "def main() -> None:\n",
    "    ap = argparse.ArgumentParser()\n",
    "\n",
    "    ap.add_argument(\"--main_csv\", required=True)\n",
    "    ap.add_argument(\"--share_csv\", required=True)\n",
    "    ap.add_argument(\"--out_dir\", required=True)\n",
    "\n",
    "    ap.add_argument(\"--n_jobs\", type=int, default=24)\n",
    "    ap.add_argument(\"--resume\", action=\"store_true\")\n",
    "    ap.add_argument(\"--rebuild_cache\", action=\"store_true\")\n",
    "\n",
    "    ap.add_argument(\"--weight_modes\", type=str, default=\"share_only_norm,share_norm_fill1\")\n",
    "\n",
    "    ap.add_argument(\"--variants\", type=str, default=\"raw,zdoy\")\n",
    "    ap.add_argument(\"--aggs\", type=str, default=\"ma\")\n",
    "    ap.add_argument(\"--transforms\", type=str, default=\"identity,square,signlog1p\")\n",
    "\n",
    "    ap.add_argument(\"--coarse_windows\", type=str, default=\"2,3,5,7,10,14,21,30,45,60,90,120,180,240,300,365,450,600,730,900,1095,1500,2000,2500\")\n",
    "    ap.add_argument(\"--coarse_shifts\", type=str, default=\"-60,-30,-15,0,15,30,60\")\n",
    "\n",
    "    ap.add_argument(\"--screen_top_k_per_month\", type=int, default=8)\n",
    "    ap.add_argument(\"--screen_keep_per_bucket\", type=int, default=3)\n",
    "    ap.add_argument(\"--screen_min_abs_corr\", type=float, default=0.25)\n",
    "    ap.add_argument(\"--min_rows_per_month\", type=int, default=40)\n",
    "\n",
    "    ap.add_argument(\"--val_years_list\", type=str, default=\"2,3,4,5\")\n",
    "\n",
    "    ap.add_argument(\"--w_min\", type=int, default=2)\n",
    "    ap.add_argument(\"--w_max\", type=int, default=2500)\n",
    "    ap.add_argument(\"--shift_min\", type=int, default=-60)\n",
    "    ap.add_argument(\"--shift_max\", type=int, default=60)\n",
    "\n",
    "    ap.add_argument(\"--refine_w_radius\", type=int, default=80)\n",
    "    ap.add_argument(\"--refine_shift_radius\", type=int, default=45)\n",
    "\n",
    "    ap.add_argument(\"--save_top\", type=int, default=5000)\n",
    "\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    out_dir = Path(args.out_dir)\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    weight_modes = [m.strip() for m in parse_csv_list(args.weight_modes) if m.strip()]\n",
    "    for m in weight_modes:\n",
    "        if m not in WEIGHT_MODES:\n",
    "            raise ValueError(f\"Unknown weight_mode {m}. Choose from {WEIGHT_MODES}\")\n",
    "\n",
    "    variants = [v.strip().lower() for v in parse_csv_list(args.variants) if v.strip()]\n",
    "    aggs = [a.strip().lower() for a in parse_csv_list(args.aggs) if a.strip()]\n",
    "    transforms = [t.strip().lower() for t in parse_csv_list(args.transforms) if t.strip()]\n",
    "\n",
    "    windows = [int(w) for w in parse_ints(args.coarse_windows) if int(w) >= 1]\n",
    "    shifts = [int(s) for s in parse_ints(args.coarse_shifts)]\n",
    "\n",
    "    val_years_list = [int(v) for v in parse_ints(args.val_years_list)]\n",
    "\n",
    "    base_cd, futures_cols, sig_df, signal_cols = load_or_build_cache(\n",
    "        main_csv=args.main_csv,\n",
    "        share_csv=args.share_csv,\n",
    "        out_dir=out_dir,\n",
    "        resume=bool(args.resume),\n",
    "        rebuild_cache=bool(args.rebuild_cache),\n",
    "        weight_modes=weight_modes,\n",
    "    )\n",
    "\n",
    "    log(\"Preparing per-country arrays (raw + zDOY)...\")\n",
    "    countries_data, signal_cols_used = build_country_data(base_cd, futures_cols, sig_df, signal_cols)\n",
    "    log(f\"Prepared {len(countries_data)} countries | signals={len(signal_cols_used)}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Stage 0: screening\n",
    "    # ---------------------------\n",
    "    baseline_path = out_dir / \"baseline_screen.csv\"\n",
    "    if args.resume and baseline_path.exists():\n",
    "        log(f\"Loading existing baseline_screen.csv -> {baseline_path}\")\n",
    "        baseline_df = pd.read_csv(baseline_path)\n",
    "    else:\n",
    "        log(\"Stage 0: factor-screening coarse candidates...\")\n",
    "\n",
    "        hits_nested = Parallel(n_jobs=int(args.n_jobs), backend=\"threading\")(\n",
    "            delayed(screen_one_country)(\n",
    "                c,\n",
    "                signal_names=signal_cols_used,\n",
    "                variants=variants,\n",
    "                aggs=aggs,\n",
    "                windows=windows,\n",
    "                shifts=shifts,\n",
    "                top_k_per_month=int(args.screen_top_k_per_month),\n",
    "                min_abs_corr=float(args.screen_min_abs_corr),\n",
    "                min_rows_per_month=int(args.min_rows_per_month),\n",
    "            )\n",
    "            for c in countries_data\n",
    "        )\n",
    "\n",
    "        hits: List[ScreenHit] = [h for sub in hits_nested for h in sub]\n",
    "        baseline_df = pd.DataFrame([h.__dict__ for h in hits])\n",
    "        baseline_df = baseline_df.sort_values([\"corr_abs\"], ascending=False).reset_index(drop=True)\n",
    "        baseline_df.to_csv(baseline_path, index=False)\n",
    "        log(f\"Wrote: {baseline_path} | rows={len(baseline_df):,}\")\n",
    "\n",
    "    if baseline_df.empty:\n",
    "        log(\"No candidates passed screening; try lowering --screen_min_abs_corr\")\n",
    "        return\n",
    "\n",
    "    # choose candidates to refine: top K per (country, month)\n",
    "    baseline_df[\"bucket\"] = baseline_df[\"country\"].astype(str) + \"|\" + baseline_df[\"month\"].astype(str)\n",
    "    baseline_df[\"rank_in_bucket\"] = baseline_df.groupby(\"bucket\")[\"corr_abs\"].rank(method=\"first\", ascending=False)\n",
    "    refine_df = baseline_df.loc[baseline_df[\"rank_in_bucket\"] <= int(args.screen_keep_per_bucket)].copy()\n",
    "    refine_df = refine_df.sort_values([\"corr_abs\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Stage 1: refinement\n",
    "    # ---------------------------\n",
    "    grid_path = out_dir / \"grid_all_candidates.csv\"\n",
    "    existing_keys: set = set()\n",
    "    if args.resume and grid_path.exists():\n",
    "        try:\n",
    "            existing = pd.read_csv(grid_path)\n",
    "            for _, r in existing.iterrows():\n",
    "                k = (r.get(\"country\"), int(r.get(\"month\")), r.get(\"variant\"), r.get(\"signal\"), r.get(\"agg\"))\n",
    "                existing_keys.add(k)\n",
    "        except Exception:\n",
    "            existing_keys = set()\n",
    "\n",
    "    tasks: List[Tuple[CountryData, ScreenHit]] = []\n",
    "    country_map = {c.country: c for c in countries_data}\n",
    "    for _, r in refine_df.iterrows():\n",
    "        k = (r[\"country\"], int(r[\"month\"]), r[\"variant\"], r[\"signal\"], r[\"agg\"])\n",
    "        if k in existing_keys:\n",
    "            continue\n",
    "        cdata = country_map.get(r[\"country\"])\n",
    "        if cdata is None:\n",
    "            continue\n",
    "        tasks.append(\n",
    "            (\n",
    "                cdata,\n",
    "                ScreenHit(\n",
    "                    country=r[\"country\"],\n",
    "                    month=int(r[\"month\"]),\n",
    "                    variant=r[\"variant\"],\n",
    "                    signal=r[\"signal\"],\n",
    "                    agg=r[\"agg\"],\n",
    "                    w=int(r[\"w\"]),\n",
    "                    shift=int(r[\"shift\"]),\n",
    "                    corr_abs=float(r[\"corr_abs\"]),\n",
    "                    corr_signed=float(r[\"corr_signed\"]),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    log(f\"Stage 1: refining {len(tasks)} screened candidates...\")\n",
    "\n",
    "    refined_list = Parallel(n_jobs=int(args.n_jobs), backend=\"threading\")(\n",
    "        delayed(refine_one_hit)(\n",
    "            cdata,\n",
    "            signal_names=signal_cols_used,\n",
    "            hit=hit,\n",
    "            transforms=transforms,\n",
    "            val_years_list=val_years_list,\n",
    "            w_min=int(args.w_min),\n",
    "            w_max=int(args.w_max),\n",
    "            shift_min=int(args.shift_min),\n",
    "            shift_max=int(args.shift_max),\n",
    "            refine_w_radius=int(args.refine_w_radius),\n",
    "            refine_shift_radius=int(args.refine_shift_radius),\n",
    "            min_rows=int(args.min_rows_per_month),\n",
    "        )\n",
    "        for (cdata, hit) in tasks\n",
    "    )\n",
    "\n",
    "    refined = [r for r in refined_list if r is not None]\n",
    "    refined_df = pd.DataFrame([r.__dict__ for r in refined])\n",
    "\n",
    "    if refined_df.empty:\n",
    "        log(\"No refined candidates. Try increasing --screen_keep_per_bucket or lowering screening threshold.\")\n",
    "        return\n",
    "\n",
    "    # If resuming, append\n",
    "    if args.resume and grid_path.exists():\n",
    "        try:\n",
    "            old = pd.read_csv(grid_path)\n",
    "            refined_df = pd.concat([old, refined_df], ignore_index=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # keep top N overall by robust_min (and proxy as tie-break)\n",
    "    refined_df = refined_df.sort_values([\"robust_min\", \"cfcs_vy2\"], ascending=False).reset_index(drop=True)\n",
    "    refined_df = refined_df.head(int(args.save_top)).copy()\n",
    "\n",
    "    refined_df.to_csv(grid_path, index=False)\n",
    "    log(f\"Wrote: {grid_path} | rows={len(refined_df):,}\")\n",
    "\n",
    "    # recommendations\n",
    "    rec_public = refined_df.sort_values([\"cfcs_vy2\", \"robust_min\"], ascending=False).reset_index(drop=True)\n",
    "    rec_robust = refined_df.sort_values([\"robust_min\", \"cfcs_vy2\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    write_recommendations(out_dir / \"recommendations_public.txt\", \"Recommendations (public-chasing: sort by vy2)\", rec_public)\n",
    "    write_recommendations(out_dir / \"recommendations_robust.txt\", \"Recommendations (robust: sort by robust_min)\", rec_robust)\n",
    "\n",
    "    log(f\"Wrote: {out_dir / 'recommendations_public.txt'}\")\n",
    "    log(f\"Wrote: {out_dir / 'recommendations_robust.txt'}\")\n",
    "\n",
    "    log(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16a4c5",
   "metadata": {
    "papermill": {
     "duration": 0.003035,
     "end_time": "2026-01-29T09:40:20.506376",
     "exception": false,
     "start_time": "2026-01-29T09:40:20.503341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15008526,
     "sourceId": 126158,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27.341539,
   "end_time": "2026-01-29T09:40:21.032921",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-29T09:39:53.691382",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
