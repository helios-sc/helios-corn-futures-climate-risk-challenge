{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3fae7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T01:22:29.751327Z",
     "iopub.status.busy": "2026-01-31T01:22:29.750828Z",
     "iopub.status.idle": "2026-01-31T01:22:29.757514Z",
     "shell.execute_reply": "2026-01-31T01:22:29.756340Z"
    },
    "papermill": {
     "duration": 0.014014,
     "end_time": "2026-01-31T01:22:29.759604",
     "exception": false,
     "start_time": "2026-01-31T01:22:29.745590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5_Submission_AE_stats_valid (Kaggle)\n",
    "# Goal: generate a submit-ready submission.csv on Kaggle (AE version, cfcs top4 valid)\n",
    "#\n",
    "# Place these files in the Kaggle Dataset or Notebook input:\n",
    "# - Data/Selection/selected_features_ae_{AE_TAG}.parquet\n",
    "# - Data/Selection/ae_input_features_{AE_TAG}.parquet\n",
    "# - Data/Selection/ae_model_{AE_TAG}.pth\n",
    "# - Data/Selection/ae_scaler_{AE_TAG}.joblib\n",
    "#\n",
    "# This notebook will:\n",
    "# - Read the competition raw CSV\n",
    "# - Reproduce Processing + Feature Engineering\n",
    "# - Compute AE features (z1/z2/z3 + recon_error)\n",
    "# - Subset columns using the uploaded feature list\n",
    "# - Export submission.csv (climate_risk_* + required fields only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e01dea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T01:22:29.766779Z",
     "iopub.status.busy": "2026-01-31T01:22:29.766421Z",
     "iopub.status.idle": "2026-01-31T01:22:36.214366Z",
     "shell.execute_reply": "2026-01-31T01:22:36.213196Z"
    },
    "papermill": {
     "duration": 6.454175,
     "end_time": "2026-01-31T01:22:36.216567",
     "exception": false,
     "start_time": "2026-01-31T01:22:29.762392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEAT_PATH: /kaggle/input/reproduce-ae-cfcs-0-5-top4-2-valid/selected_features_ae_ae_cfcs_0.5_top4_2_valid.parquet\n",
      "AE_INPUT_PATH: /kaggle/input/reproduce-ae-cfcs-0-5-top4-2-valid/ae_input_features_ae_cfcs_0.5_top4_2_valid.parquet\n",
      "AE_MODEL_PATH: /kaggle/input/reproduce-ae-cfcs-0-5-top4-2-valid/ae_model_ae_cfcs_0.5_top4_2_valid.pth\n",
      "AE_SCALER_PATH: /kaggle/input/reproduce-ae-cfcs-0-5-top4-2-valid/ae_scaler_ae_cfcs_0.5_top4_2_valid.joblib\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "RISK_CATEGORIES = [\"heat_stress\", \"unseasonably_cold\", \"excess_precip\", \"drought\"]\n",
    "\n",
    "DATA_DIR = Path(\"/kaggle/input/forecasting-the-future-the-helios-corn-climate-challenge\")\n",
    "WORK_DIR = Path(\"/kaggle/working\")\n",
    "INPUT_DIR = Path(\"/kaggle/input/reproduce-ae-cfcs-0-5-top4-2-valid\")\n",
    "\n",
    "RAW_MAIN = DATA_DIR / \"corn_climate_risk_futures_daily_master.csv\"\n",
    "RAW_SHARE = DATA_DIR / \"corn_regional_market_share.csv\"\n",
    "\n",
    "# Select AE model and feature files\n",
    "SIGNIFICANCE_THRESHOLD = 0.5\n",
    "THRESH_TAG_2 = f\"{SIGNIFICANCE_THRESHOLD:.2f}\".rstrip(\"0\").rstrip(\".\")\n",
    "TAG_2 = f\"cfcs_{THRESH_TAG_2}_top4_2\"\n",
    "AE_TAG = f\"ae_{TAG_2}_valid\"\n",
    "\n",
    "\n",
    "def resolve_input_path(filename: str) -> Path:\n",
    "    direct = INPUT_DIR / filename\n",
    "    if direct.exists():\n",
    "        return direct\n",
    "    # Try to auto-find under /kaggle/input\n",
    "    matches = list(Path(\"/kaggle/input\").rglob(filename))\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    return direct\n",
    "\n",
    "\n",
    "FEAT_PATH = resolve_input_path(f\"selected_features_ae_{AE_TAG}.parquet\")\n",
    "AE_INPUT_PATH = resolve_input_path(f\"ae_input_features_{AE_TAG}.parquet\")\n",
    "AE_MODEL_PATH = resolve_input_path(f\"ae_model_{AE_TAG}.pth\")\n",
    "AE_SCALER_PATH = resolve_input_path(f\"ae_scaler_{AE_TAG}.joblib\")\n",
    "\n",
    "print(\"FEAT_PATH:\", FEAT_PATH)\n",
    "print(\"AE_INPUT_PATH:\", AE_INPUT_PATH)\n",
    "print(\"AE_MODEL_PATH:\", AE_MODEL_PATH)\n",
    "print(\"AE_SCALER_PATH:\", AE_SCALER_PATH)\n",
    "\n",
    "\n",
    "assert RAW_MAIN.exists()\n",
    "assert RAW_SHARE.exists()\n",
    "assert FEAT_PATH.exists()\n",
    "assert AE_INPUT_PATH.exists()\n",
    "assert AE_MODEL_PATH.exists()\n",
    "assert AE_SCALER_PATH.exists()\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, latent_dim: int = 3):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, latent_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492a7d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T01:22:36.223888Z",
     "iopub.status.busy": "2026-01-31T01:22:36.223152Z",
     "iopub.status.idle": "2026-01-31T01:22:40.549078Z",
     "shell.execute_reply": "2026-01-31T01:22:40.547764Z"
    },
    "papermill": {
     "duration": 4.331893,
     "end_time": "2026-01-31T01:22:40.551225",
     "exception": false,
     "start_time": "2026-01-31T01:22:36.219332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw: (320661, 41)\n",
      "after processing: (320447, 41)\n"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "\n",
    "df = pd.read_csv(RAW_MAIN, low_memory=False)\n",
    "df[\"date_on\"] = pd.to_datetime(df[\"date_on\"], errors=\"coerce\")\n",
    "share = pd.read_csv(RAW_SHARE, low_memory=False)\n",
    "\n",
    "print(\"raw:\", df.shape)\n",
    "\n",
    "# Processing: ffill + dropna(futures)\n",
    "climate_cols = [c for c in df.columns if c.startswith(\"climate_risk_\")]\n",
    "futures_cols = [c for c in df.columns if c.startswith(\"futures_\")]\n",
    "\n",
    "ffill_cols = [c for c in (climate_cols + futures_cols) if c not in [\"ID\", \"region_id\", \"date_on\"]]\n",
    "\n",
    "df = df.sort_values([\"region_id\", \"date_on\"]).copy()\n",
    "df[ffill_cols] = df.groupby(\"region_id\")[ffill_cols].ffill()\n",
    "df[climate_cols] = df[climate_cols].fillna(0)\n",
    "\n",
    "df = df.dropna(subset=futures_cols).copy()\n",
    "\n",
    "print(\"after processing:\", df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94715c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T01:22:40.559605Z",
     "iopub.status.busy": "2026-01-31T01:22:40.558812Z",
     "iopub.status.idle": "2026-01-31T01:26:24.559251Z",
     "shell.execute_reply": "2026-01-31T01:26:24.558147Z"
    },
    "papermill": {
     "duration": 224.007037,
     "end_time": "2026-01-31T01:26:24.561285",
     "exception": false,
     "start_time": "2026-01-31T01:22:40.554248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/773190809.py:321: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_country_metrics)\n",
      "/tmp/ipykernel_17/773190809.py:321: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_country_metrics)\n",
      "/tmp/ipykernel_17/773190809.py:321: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_country_metrics)\n",
      "/tmp/ipykernel_17/773190809.py:321: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_country_metrics)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engineered: 314 | df: (320447, 365)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.8.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "df = df.merge(share[[\"region_id\", \"percent_country_production\"]], on=\"region_id\", how=\"left\")\n",
    "df[\"percent_country_production\"] = df[\"percent_country_production\"].fillna(1.0)\n",
    "\n",
    "\n",
    "def build_features(base: pd.DataFrame, seasonal_stats: pd.DataFrame | None = None, thresholds: dict | None = None):\n",
    "    df = base.copy()\n",
    "    new_cols: list[str] = []\n",
    "\n",
    "    # Collect features first, then concatenate once to avoid DataFrame fragmentation\n",
    "    feats: dict[str, pd.Series] = {}\n",
    "\n",
    "    # Time features\n",
    "    feats[\"climate_risk_day_of_year\"] = df[\"date_on\"].dt.dayofyear\n",
    "    feats[\"climate_risk_quarter\"] = df[\"date_on\"].dt.quarter\n",
    "    feats[\"climate_risk_season_sin\"] = np.sin(2 * np.pi * feats[\"climate_risk_day_of_year\"] / 365.0)\n",
    "    feats[\"climate_risk_season_cos\"] = np.cos(2 * np.pi * feats[\"climate_risk_day_of_year\"] / 365.0)\n",
    "\n",
    "    # Hemisphere-aligned seasonal features & key window weights\n",
    "    south_countries = {\n",
    "        \"Brazil\",\n",
    "        \"Argentina\",\n",
    "        \"Australia\",\n",
    "        \"South Africa\",\n",
    "        \"Uruguay\",\n",
    "        \"Paraguay\",\n",
    "        \"Bolivia\",\n",
    "        \"Chile\",\n",
    "        \"Peru\",\n",
    "        \"New Zealand\",\n",
    "    }\n",
    "    is_south = df[\"country_name\"].isin(south_countries)\n",
    "    month = df[\"date_on\"].dt.month\n",
    "    month_adj = np.where(is_south, ((month + 5) % 12) + 1, month)\n",
    "    season_weight = np.where(\n",
    "        np.isin(month_adj, [7, 8]),\n",
    "        1.5,\n",
    "        np.where(np.isin(month_adj, [6, 9]), 1.2, 1.0),\n",
    "    )\n",
    "    feats[\"climate_risk_season_weight\"] = season_weight\n",
    "\n",
    "    doy_shift = np.where(is_south, 182, 0)\n",
    "    doy_shifted = (feats[\"climate_risk_day_of_year\"] + doy_shift) % 365\n",
    "    feats[\"climate_risk_season_sin_shift\"] = np.sin(2 * np.pi * doy_shifted / 365.0)\n",
    "    feats[\"climate_risk_season_cos_shift\"] = np.cos(2 * np.pi * doy_shifted / 365.0)\n",
    "\n",
    "    # Base risk scores + production weighting\n",
    "    for risk_type in RISK_CATEGORIES:\n",
    "        low_col = f\"climate_risk_cnt_locations_{risk_type}_risk_low\"\n",
    "        med_col = f\"climate_risk_cnt_locations_{risk_type}_risk_medium\"\n",
    "        high_col = f\"climate_risk_cnt_locations_{risk_type}_risk_high\"\n",
    "\n",
    "        total = df[low_col] + df[med_col] + df[high_col]\n",
    "        score = (df[med_col] + 2.0 * df[high_col]) / (total + 1e-6)\n",
    "        high_share = df[high_col] / (total + 1e-6)\n",
    "        med_share = df[med_col] / (total + 1e-6)\n",
    "        low_share = df[low_col] / (total + 1e-6)\n",
    "        severity_051 = (0.5 * df[med_col] + 1.0 * df[high_col]) / (total + 1e-6)\n",
    "        balance = (df[high_col] - df[low_col]) / (total + 1e-6)\n",
    "        entropy = -(\n",
    "            low_share * np.log(low_share + 1e-6)\n",
    "            + med_share * np.log(med_share + 1e-6)\n",
    "            + high_share * np.log(high_share + 1e-6)\n",
    "        )\n",
    "\n",
    "        weight = df[\"percent_country_production\"] / 100.0\n",
    "        weighted = score * weight\n",
    "        weighted_high = high_share * weight\n",
    "        weighted_med = med_share * weight\n",
    "        weighted_sev051 = severity_051 * weight\n",
    "        weighted_balance = balance * weight\n",
    "\n",
    "        feats[f\"climate_risk_{risk_type}_score\"] = score\n",
    "        feats[f\"climate_risk_{risk_type}_weighted\"] = weighted\n",
    "        feats[f\"climate_risk_{risk_type}_high_share\"] = high_share\n",
    "        feats[f\"climate_risk_{risk_type}_med_share\"] = med_share\n",
    "        feats[f\"climate_risk_{risk_type}_low_share\"] = low_share\n",
    "        feats[f\"climate_risk_{risk_type}_severity_051\"] = severity_051\n",
    "        feats[f\"climate_risk_{risk_type}_balance\"] = balance\n",
    "        feats[f\"climate_risk_{risk_type}_entropy\"] = entropy\n",
    "        feats[f\"climate_risk_{risk_type}_weighted_high_share\"] = weighted_high\n",
    "        feats[f\"climate_risk_{risk_type}_weighted_med_share\"] = weighted_med\n",
    "        feats[f\"climate_risk_{risk_type}_weighted_severity_051\"] = weighted_sev051\n",
    "        feats[f\"climate_risk_{risk_type}_weighted_balance\"] = weighted_balance\n",
    "\n",
    "    # Concatenate base features once\n",
    "    base_feat_df = pd.DataFrame(feats, index=df.index)\n",
    "    df = df.join(base_feat_df)\n",
    "    new_cols.extend(list(base_feat_df.columns))\n",
    "\n",
    "    # Sort for time-series features (by region_id)\n",
    "    df = df.sort_values([\"region_id\", \"date_on\"]).copy()\n",
    "\n",
    "    # Rolling / Lag / EMA / Momentum\n",
    "    roll_feats: dict[str, pd.Series] = {}\n",
    "    ROLL_WINDOWS = [7, 14, 30, 60, 90]\n",
    "    LAGS = [1, 7, 14, 30]\n",
    "    EMAS = [7, 14, 30, 60]\n",
    "\n",
    "    for window in ROLL_WINDOWS:\n",
    "        for risk_type in RISK_CATEGORIES:\n",
    "            score_col = f\"climate_risk_{risk_type}_score\"\n",
    "\n",
    "            roll_feats[f\"climate_risk_{risk_type}_ma_{window}d\"] = (\n",
    "                df.groupby(\"region_id\")[score_col].transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "            )\n",
    "            roll_feats[f\"climate_risk_{risk_type}_max_{window}d\"] = (\n",
    "                df.groupby(\"region_id\")[score_col].transform(lambda x: x.rolling(window, min_periods=1).max())\n",
    "            )\n",
    "            roll_feats[f\"climate_risk_{risk_type}_vol_{window}d\"] = (\n",
    "                df.groupby(\"region_id\")[score_col].transform(lambda x: x.rolling(window, min_periods=2).std())\n",
    "            )\n",
    "            roll_feats[f\"climate_risk_{risk_type}_cumsum_{window}d\"] = (\n",
    "                df.groupby(\"region_id\")[score_col].transform(lambda x: x.rolling(window, min_periods=1).sum())\n",
    "            )\n",
    "\n",
    "    for lag in LAGS:\n",
    "        for risk_type in RISK_CATEGORIES:\n",
    "            score_col = f\"climate_risk_{risk_type}_score\"\n",
    "            roll_feats[f\"climate_risk_{risk_type}_lag_{lag}d\"] = df.groupby(\"region_id\")[score_col].shift(lag)\n",
    "\n",
    "    for span in EMAS:\n",
    "        for risk_type in RISK_CATEGORIES:\n",
    "            score_col = f\"climate_risk_{risk_type}_score\"\n",
    "            roll_feats[f\"climate_risk_{risk_type}_ema_{span}d\"] = (\n",
    "                df.groupby(\"region_id\")[score_col].transform(lambda x: x.ewm(span=span, min_periods=1).mean())\n",
    "            )\n",
    "\n",
    "    # Momentum / Acceleration\n",
    "    for risk_type in RISK_CATEGORIES:\n",
    "        score_col = f\"climate_risk_{risk_type}_score\"\n",
    "        c1 = df.groupby(\"region_id\")[score_col].diff(1)\n",
    "        roll_feats[f\"climate_risk_{risk_type}_change_1d\"] = c1\n",
    "        roll_feats[f\"climate_risk_{risk_type}_change_7d\"] = df.groupby(\"region_id\")[score_col].diff(7)\n",
    "        roll_feats[f\"climate_risk_{risk_type}_acceleration\"] = c1.groupby(df[\"region_id\"]).diff(1)\n",
    "\n",
    "        lag7 = df.groupby(\"region_id\")[score_col].shift(7)\n",
    "        roll_feats[f\"climate_risk_{risk_type}_roc_7d\"] = (df[score_col] - lag7) / (lag7 + 1e-6)\n",
    "\n",
    "    # Event and persistence features\n",
    "    EVENT_TAUS = [0.2, 0.3]\n",
    "    EVENT_WINDOWS = [7, 14, 30]\n",
    "    SPIKE_WINDOW = 30\n",
    "    SPIKE_Z = 2.0\n",
    "\n",
    "    for risk_type in RISK_CATEGORIES:\n",
    "        hs_col = f\"climate_risk_{risk_type}_high_share\"\n",
    "\n",
    "        for tau in EVENT_TAUS:\n",
    "            runlen = df.groupby(\"region_id\")[hs_col].transform(\n",
    "                lambda s: (s > tau).groupby((~(s > tau)).cumsum()).cumsum()\n",
    "            )\n",
    "            roll_feats[f\"climate_risk_{risk_type}_high_runlen_{int(tau*100)}\"] = runlen\n",
    "\n",
    "            time_since = df.groupby(\"region_id\")[hs_col].transform(\n",
    "                lambda s: (~(s > tau)).groupby((s > tau).cumsum()).cumcount()\n",
    "            )\n",
    "            roll_feats[f\"climate_risk_{risk_type}_time_since_high_{int(tau*100)}\"] = time_since\n",
    "\n",
    "            for window in EVENT_WINDOWS:\n",
    "                auc = df.groupby(\"region_id\")[hs_col].transform(\n",
    "                    lambda s: (s - tau).clip(lower=0).rolling(window, min_periods=1).sum()\n",
    "                )\n",
    "                roll_feats[f\"climate_risk_{risk_type}_event_auc_{int(tau*100)}_{window}d\"] = auc\n",
    "\n",
    "        mean_ = df.groupby(\"region_id\")[hs_col].transform(lambda s: s.rolling(SPIKE_WINDOW, min_periods=2).mean())\n",
    "        std_ = df.groupby(\"region_id\")[hs_col].transform(lambda s: s.rolling(SPIKE_WINDOW, min_periods=2).std())\n",
    "        roll_feats[f\"climate_risk_{risk_type}_spike_{SPIKE_WINDOW}d\"] = (\n",
    "            (df[hs_col] - mean_) > (SPIKE_Z * std_)\n",
    "        ).astype(int)\n",
    "\n",
    "    roll_feat_df = pd.DataFrame(roll_feats, index=df.index)\n",
    "    df = df.join(roll_feat_df)\n",
    "    new_cols.extend(list(roll_feat_df.columns))\n",
    "\n",
    "    # Non-linear & Combination\n",
    "    combo_feats: dict[str, pd.Series] = {}\n",
    "    score_cols = [f\"climate_risk_{r}_score\" for r in RISK_CATEGORIES]\n",
    "    weighted_cols = [f\"climate_risk_{r}_weighted\" for r in RISK_CATEGORIES]\n",
    "\n",
    "    combo_feats[\"climate_risk_temperature_stress\"] = df[[\"climate_risk_heat_stress_score\", \"climate_risk_unseasonably_cold_score\"]].max(axis=1)\n",
    "    combo_feats[\"climate_risk_precipitation_stress\"] = df[[\"climate_risk_excess_precip_score\", \"climate_risk_drought_score\"]].max(axis=1)\n",
    "    combo_feats[\"climate_risk_overall_stress\"] = df[score_cols].max(axis=1)\n",
    "    combo_feats[\"climate_risk_combined_stress\"] = df[score_cols].mean(axis=1)\n",
    "\n",
    "    combo_feats[\"climate_risk_temperature_stress_weighted\"] = df[[\"climate_risk_heat_stress_weighted\", \"climate_risk_unseasonably_cold_weighted\"]].max(axis=1)\n",
    "    combo_feats[\"climate_risk_precipitation_stress_weighted\"] = df[[\"climate_risk_excess_precip_weighted\", \"climate_risk_drought_weighted\"]].max(axis=1)\n",
    "    combo_feats[\"climate_risk_overall_weighted\"] = df[weighted_cols].max(axis=1)\n",
    "    combo_feats[\"climate_risk_combined_weighted\"] = df[weighted_cols].mean(axis=1)\n",
    "\n",
    "    combo_feats[\"climate_risk_precip_drought_diff\"] = df[\"climate_risk_excess_precip_score\"] - df[\"climate_risk_drought_score\"]\n",
    "    combo_feats[\"climate_risk_temp_diff\"] = df[\"climate_risk_heat_stress_score\"] - df[\"climate_risk_unseasonably_cold_score\"]\n",
    "    combo_feats[\"climate_risk_precip_drought_ratio\"] = df[\"climate_risk_excess_precip_score\"] / (df[\"climate_risk_drought_score\"] + 0.01)\n",
    "\n",
    "    combo_feats[\"climate_risk_heat_cold_sum\"] = df[\"climate_risk_heat_stress_score\"] + df[\"climate_risk_unseasonably_cold_score\"]\n",
    "    combo_feats[\"climate_risk_drought_excess_sum\"] = df[\"climate_risk_drought_score\"] + df[\"climate_risk_excess_precip_score\"]\n",
    "    combo_feats[\"climate_risk_heat_drought_interact\"] = df[\"climate_risk_heat_stress_score\"] * df[\"climate_risk_drought_score\"]\n",
    "    combo_feats[\"climate_risk_cold_excess_interact\"] = df[\"climate_risk_unseasonably_cold_score\"] * df[\"climate_risk_excess_precip_score\"]\n",
    "\n",
    "    # season/time modulation\n",
    "    for risk_type in RISK_CATEGORIES:\n",
    "        score_col = f\"climate_risk_{risk_type}_score\"\n",
    "        weighted_col = f\"climate_risk_{risk_type}_weighted\"\n",
    "        combo_feats[f\"climate_risk_{risk_type}_score_season_sin\"] = df[score_col] * df[\"climate_risk_season_sin\"]\n",
    "        combo_feats[f\"climate_risk_{risk_type}_score_season_cos\"] = df[score_col] * df[\"climate_risk_season_cos\"]\n",
    "        combo_feats[f\"climate_risk_{risk_type}_weighted_season_sin\"] = df[weighted_col] * df[\"climate_risk_season_sin\"]\n",
    "        combo_feats[f\"climate_risk_{risk_type}_weighted_season_cos\"] = df[weighted_col] * df[\"climate_risk_season_cos\"]\n",
    "        combo_feats[f\"climate_risk_{risk_type}_score_qtr\"] = df[score_col] * df[\"climate_risk_quarter\"]\n",
    "        combo_feats[f\"climate_risk_{risk_type}_score_doy\"] = df[score_col] * (df[\"climate_risk_day_of_year\"] / 365.0)\n",
    "        combo_feats[f\"climate_risk_{risk_type}_season_w\"] = df[score_col] * df[\"climate_risk_season_weight\"]\n",
    "\n",
    "    combo_df = pd.DataFrame(combo_feats, index=df.index)\n",
    "    df = df.join(combo_df)\n",
    "    new_cols.extend(list(combo_df.columns))\n",
    "\n",
    "    # Seasonal anomalies (z-score by country_name + month)\n",
    "    df[\"month\"] = df[\"date_on\"].dt.month\n",
    "    if seasonal_stats is None:\n",
    "        seasonal_stats = (\n",
    "            df.groupby([\"country_name\", \"month\"])\n",
    "            .agg(**{f\"{r}_mean\": (f\"climate_risk_{r}_score\", \"mean\") for r in RISK_CATEGORIES},\n",
    "                 **{f\"{r}_std\": (f\"climate_risk_{r}_score\", \"std\") for r in RISK_CATEGORIES})\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "    df = df.merge(seasonal_stats, on=[\"country_name\", \"month\"], how=\"left\")\n",
    "\n",
    "    z_feats: dict[str, pd.Series] = {}\n",
    "    for r in RISK_CATEGORIES:\n",
    "        mean_c = f\"{r}_mean\"\n",
    "        std_c = f\"{r}_std\"\n",
    "        z_feats[f\"climate_risk_{r}_seasonal_z\"] = (df[f\"climate_risk_{r}_score\"] - df[mean_c]) / (df[std_c] + 1e-6)\n",
    "\n",
    "    z_df = pd.DataFrame(z_feats, index=df.index)\n",
    "    df = df.join(z_df)\n",
    "    new_cols.extend(list(z_df.columns))\n",
    "\n",
    "    # Extreme risk thresholds (computed on full data)\n",
    "    if thresholds is None:\n",
    "        thresholds = {}\n",
    "        for r in RISK_CATEGORIES:\n",
    "            thresholds[r] = df[f\"climate_risk_{r}_high_share\"].quantile(0.9)\n",
    "\n",
    "    flag_feats: dict[str, pd.Series] = {}\n",
    "    for r in RISK_CATEGORIES:\n",
    "        thr = thresholds[r]\n",
    "        flag_feats[f\"climate_risk_{r}_extreme\"] = (df[f\"climate_risk_{r}_high_share\"] >= thr).astype(int)\n",
    "\n",
    "    flag_df = pd.DataFrame(flag_feats, index=df.index)\n",
    "    df = df.join(flag_df)\n",
    "    new_cols.extend(list(flag_df.columns))\n",
    "\n",
    "    # NaN 处理（一次性）\n",
    "    df[new_cols] = df[new_cols].fillna(0)\n",
    "\n",
    "    return df, seasonal_stats, thresholds, new_cols\n",
    "\n",
    "\n",
    "def add_country_agg(df: pd.DataFrame) -> tuple[pd.DataFrame, list[str]]:\n",
    "    country_feats = []\n",
    "    for risk_type in RISK_CATEGORIES:\n",
    "        score_col = f\"climate_risk_{risk_type}_score\"\n",
    "        weighted_col = f\"climate_risk_{risk_type}_weighted\"\n",
    "\n",
    "        country_agg = (\n",
    "            df.groupby([\"country_name\", \"date_on\"])\n",
    "            .agg(\n",
    "                **{\n",
    "                    f\"climate_risk_country_{risk_type}_score_mean\": (score_col, \"mean\"),\n",
    "                    f\"climate_risk_country_{risk_type}_score_max\": (score_col, \"max\"),\n",
    "                    f\"climate_risk_country_{risk_type}_score_std\": (score_col, \"std\"),\n",
    "                    f\"climate_risk_country_{risk_type}_weighted_sum\": (weighted_col, \"sum\"),\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        base_cols = [c for c in country_agg.columns if c not in [\"country_name\", \"date_on\"]]\n",
    "        country_feats.extend(base_cols)\n",
    "        df = df.merge(country_agg, on=[\"country_name\", \"date_on\"], how=\"left\")\n",
    "\n",
    "        high_share_col = f\"climate_risk_{risk_type}_high_share\"\n",
    "\n",
    "        def _country_metrics(g: pd.DataFrame) -> pd.Series:\n",
    "            w = g[\"percent_country_production\"].fillna(0) / 100.0\n",
    "            x = g[score_col]\n",
    "            hs = g[high_share_col]\n",
    "            w_sum = w.sum()\n",
    "\n",
    "            if w_sum > 0:\n",
    "                w_mean = np.average(x, weights=w)\n",
    "                w_std = np.sqrt(np.average((x - w_mean) ** 2, weights=w))\n",
    "            else:\n",
    "                w_mean = x.mean()\n",
    "                w_std = x.std(ddof=0)\n",
    "\n",
    "            s = w * x\n",
    "            denom = (s.sum() ** 2) + 1e-6\n",
    "            hhi = (s ** 2).sum() / denom\n",
    "            exposed = (w * (hs > 0.3)).sum()\n",
    "\n",
    "            if s.size <= 1:\n",
    "                top_diff = float(s.max()) if s.size == 1 else 0.0\n",
    "            else:\n",
    "                s_sorted = np.sort(s.to_numpy())\n",
    "                top_diff = float(s_sorted[-1] - s_sorted[-2])\n",
    "\n",
    "            return pd.Series(\n",
    "                {\n",
    "                    f\"climate_risk_country_{risk_type}_score_wmean\": w_mean,\n",
    "                    f\"climate_risk_country_{risk_type}_score_wstd\": w_std,\n",
    "                    f\"climate_risk_country_{risk_type}_concentration_hhi\": hhi,\n",
    "                    f\"climate_risk_country_{risk_type}_prod_exposed_share\": exposed,\n",
    "                    f\"climate_risk_country_{risk_type}_top1_minus_top2\": top_diff,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        extra = (\n",
    "            df.groupby([\"country_name\", \"date_on\"], sort=False)\n",
    "            .apply(_country_metrics)\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        extra_cols = [c for c in extra.columns if c not in [\"country_name\", \"date_on\"]]\n",
    "        country_feats.extend(extra_cols)\n",
    "        df = df.merge(extra, on=[\"country_name\", \"date_on\"], how=\"left\")\n",
    "\n",
    "    # NaN fill\n",
    "    df[country_feats] = df[country_feats].fillna(0)\n",
    "    return df, country_feats\n",
    "\n",
    "\n",
    "df, seasonal_stats, thresholds, feat_cols = build_features(df, seasonal_stats=None, thresholds=None)\n",
    "# Country aggregation (computed on full data)\n",
    "df, country_cols = add_country_agg(df)\n",
    "\n",
    "print(\"engineered:\", len(feat_cols) + len(country_cols), \"| df:\", df.shape)\n",
    "\n",
    "# === Generate AE features ===\n",
    "ae_input_features = pd.read_parquet(AE_INPUT_PATH)[\"feature\"].tolist()\n",
    "missing = [c for c in ae_input_features if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"AE 输入列缺失: {missing[:5]} ... total={len(missing)}\")\n",
    "\n",
    "ae_df = df[ae_input_features].fillna(0.0)\n",
    "scaler = joblib.load(AE_SCALER_PATH)\n",
    "X_scaled = scaler.transform(ae_df)\n",
    "\n",
    "model = AutoEncoder(input_dim=X_scaled.shape[1], latent_dim=3)\n",
    "model.load_state_dict(torch.load(AE_MODEL_PATH, map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    X_hat, Z = model(X_tensor)\n",
    "\n",
    "Z = Z.numpy()\n",
    "\n",
    "# latent\n",
    "for i in range(3):\n",
    "    df[f\"climate_risk_ae_z{i+1}\"] = Z[:, i]\n",
    "\n",
    "# recon error\n",
    "recon_error = ((X_hat - X_tensor) ** 2).mean(dim=1).numpy()\n",
    "df[\"climate_risk_ae_recon_error\"] = recon_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189237e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T01:26:24.569648Z",
     "iopub.status.busy": "2026-01-31T01:26:24.568720Z",
     "iopub.status.idle": "2026-01-31T01:26:24.583347Z",
     "shell.execute_reply": "2026-01-31T01:26:24.582140Z"
    },
    "papermill": {
     "duration": 0.02084,
     "end_time": "2026-01-31T01:26:24.585226",
     "exception": false,
     "start_time": "2026-01-31T01:26:24.564386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_feats: 4\n"
     ]
    }
   ],
   "source": [
    "selected_feats = pd.read_parquet(FEAT_PATH)[\"feature\"].tolist()\n",
    "selected_feats = [c for c in selected_feats if c in df.columns]\n",
    "\n",
    "print(\"selected_feats:\", len(selected_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3b292a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T01:26:24.592965Z",
     "iopub.status.busy": "2026-01-31T01:26:24.592530Z",
     "iopub.status.idle": "2026-01-31T01:26:24.599943Z",
     "shell.execute_reply": "2026-01-31T01:26:24.598521Z"
    },
    "papermill": {
     "duration": 0.013839,
     "end_time": "2026-01-31T01:26:24.602188",
     "exception": false,
     "start_time": "2026-01-31T01:26:24.588349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['climate_risk_ae_z1',\n",
       " 'climate_risk_ae_z2',\n",
       " 'climate_risk_ae_z3',\n",
       " 'climate_risk_ae_recon_error']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0064487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T01:26:24.610569Z",
     "iopub.status.busy": "2026-01-31T01:26:24.609910Z",
     "iopub.status.idle": "2026-01-31T01:26:32.555086Z",
     "shell.execute_reply": "2026-01-31T01:26:32.553858Z"
    },
    "papermill": {
     "duration": 7.952032,
     "end_time": "2026-01-31T01:26:32.557353",
     "exception": false,
     "start_time": "2026-01-31T01:26:24.605321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ids: 219161\n",
      "after align: (219161, 369)\n"
     ]
    }
   ],
   "source": [
    "# === Align Kaggle valid samples (critical) ===\n",
    "# Replicate host notebook's valid_ids logic: based on minimum feature engineering dropna()\n",
    "\n",
    "_tmp = pd.read_csv(RAW_MAIN, low_memory=False)\n",
    "_tmp[\"date_on\"] = pd.to_datetime(_tmp[\"date_on\"], errors=\"coerce\")\n",
    "_tmp = _tmp.merge(share[[\"region_id\", \"percent_country_production\"]], on=\"region_id\", how=\"left\")\n",
    "_tmp[\"percent_country_production\"] = _tmp[\"percent_country_production\"].fillna(1.0)\n",
    "\n",
    "# Base risk scores\n",
    "for risk_type in RISK_CATEGORIES:\n",
    "    low = f\"climate_risk_cnt_locations_{risk_type}_risk_low\"\n",
    "    med = f\"climate_risk_cnt_locations_{risk_type}_risk_medium\"\n",
    "    high = f\"climate_risk_cnt_locations_{risk_type}_risk_high\"\n",
    "    total = _tmp[low] + _tmp[med] + _tmp[high]\n",
    "    score = (_tmp[med] + 2 * _tmp[high]) / (total + 1e-6)\n",
    "    weighted = score * (_tmp[\"percent_country_production\"] / 100.0)\n",
    "    _tmp[f\"climate_risk_{risk_type}_score\"] = score\n",
    "    _tmp[f\"climate_risk_{risk_type}_weighted\"] = weighted\n",
    "\n",
    "# Interactions (和 host notebook 一致)\n",
    "score_cols = [f\"climate_risk_{r}_score\" for r in RISK_CATEGORIES]\n",
    "_tmp[\"climate_risk_temperature_stress\"] = _tmp[[\"climate_risk_heat_stress_score\", \"climate_risk_unseasonably_cold_score\"]].max(axis=1)\n",
    "_tmp[\"climate_risk_precipitation_stress\"] = _tmp[[\"climate_risk_excess_precip_score\", \"climate_risk_drought_score\"]].max(axis=1)\n",
    "_tmp[\"climate_risk_overall_stress\"] = _tmp[score_cols].max(axis=1)\n",
    "_tmp[\"climate_risk_combined_stress\"] = _tmp[score_cols].mean(axis=1)\n",
    "\n",
    "# Sort for time-series ops\n",
    "_tmp = _tmp.sort_values([\"region_id\", \"date_on\"]).copy()\n",
    "\n",
    "# Rolling MA / Max (7/14/30)\n",
    "for window in [7, 14, 30]:\n",
    "    for risk_type in RISK_CATEGORIES:\n",
    "        score_col = f\"climate_risk_{risk_type}_score\"\n",
    "        _tmp[f\"climate_risk_{risk_type}_ma_{window}d\"] = _tmp.groupby(\"region_id\")[score_col].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean()\n",
    "        )\n",
    "        _tmp[f\"climate_risk_{risk_type}_max_{window}d\"] = _tmp.groupby(\"region_id\")[score_col].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).max()\n",
    "        )\n",
    "\n",
    "# Momentum (change / acceleration)\n",
    "for risk_type in RISK_CATEGORIES:\n",
    "    score_col = f\"climate_risk_{risk_type}_score\"\n",
    "    _tmp[f\"climate_risk_{risk_type}_change_1d\"] = _tmp.groupby(\"region_id\")[score_col].diff(1)\n",
    "    _tmp[f\"climate_risk_{risk_type}_change_7d\"] = _tmp.groupby(\"region_id\")[score_col].diff(7)\n",
    "    _tmp[f\"climate_risk_{risk_type}_acceleration\"] = _tmp.groupby(\"region_id\")[f\"climate_risk_{risk_type}_change_1d\"].diff(1)\n",
    "\n",
    "# Country aggregations\n",
    "for risk_type in RISK_CATEGORIES:\n",
    "    score_col = f\"climate_risk_{risk_type}_score\"\n",
    "    weighted_col = f\"climate_risk_{risk_type}_weighted\"\n",
    "    country_agg = _tmp.groupby([\"country_name\", \"date_on\"]).agg({\n",
    "        score_col: [\"mean\", \"max\", \"std\"],\n",
    "        weighted_col: \"sum\",\n",
    "        \"percent_country_production\": \"sum\",\n",
    "    }).round(4)\n",
    "    country_agg.columns = [f\"country_{risk_type}_{'_'.join(col).strip()}\" for col in country_agg.columns]\n",
    "    country_agg = country_agg.reset_index()\n",
    "    _tmp = _tmp.merge(country_agg, on=[\"country_name\", \"date_on\"], how=\"left\")\n",
    "\n",
    "futures_cols = [c for c in _tmp.columns if c.startswith(\"futures_\")]\n",
    "_tmp = _tmp.dropna(subset=futures_cols)\n",
    "valid_ids = _tmp.dropna()[\"ID\"].astype(str).unique()\n",
    "\n",
    "print(\"valid_ids:\", len(valid_ids))\n",
    "\n",
    "if \"ID\" in df.columns:\n",
    "    df[\"ID\"] = df[\"ID\"].astype(str)\n",
    "    df = df[df[\"ID\"].isin(valid_ids)].copy()\n",
    "\n",
    "print(\"after align:\", df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92e785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T01:26:32.566939Z",
     "iopub.status.busy": "2026-01-31T01:26:32.566463Z",
     "iopub.status.idle": "2026-01-31T01:26:39.219408Z",
     "shell.execute_reply": "2026-01-31T01:26:39.218440Z"
    },
    "papermill": {
     "duration": 6.660809,
     "end_time": "2026-01-31T01:26:39.221594",
     "exception": false,
     "start_time": "2026-01-31T01:26:32.560785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission shape: (219161, 25)\n",
      "saved: /kaggle/working/submission.csv\n",
      "nulls: 0\n"
     ]
    }
   ],
   "source": [
    "# Generate submission.csv\n",
    "\n",
    "REQUIRED_COLS = [c for c in [\"ID\", \"date_on\", \"country_name\", \"region_name\"] if c in df.columns]\n",
    "\n",
    "submit_climate_cols = [c for c in selected_feats if c.startswith(\"climate_risk_\")]\n",
    "\n",
    "futures_cols = [c for c in df.columns if c.startswith(\"futures_\")]\n",
    "\n",
    "if submit_climate_cols:\n",
    "    df[submit_climate_cols] = df[submit_climate_cols].fillna(0)\n",
    "\n",
    "submit_cols = REQUIRED_COLS + submit_climate_cols + futures_cols\n",
    "submission = df[submit_cols].copy()\n",
    "\n",
    "out_path = WORK_DIR / \"submission.csv\"\n",
    "submission.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"submission shape:\", submission.shape)\n",
    "print(\"saved:\", out_path)\n",
    "print(\"nulls:\", int(submission.isna().sum().sum()))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15008526,
     "isSourceIdPinned": false,
     "sourceId": 126158,
     "sourceType": "competition"
    },
    {
     "datasetId": 9380110,
     "sourceId": 14683107,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 255.706605,
   "end_time": "2026-01-31T01:26:41.877955",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-31T01:22:26.171350",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
