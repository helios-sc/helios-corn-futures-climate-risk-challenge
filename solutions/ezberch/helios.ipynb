{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b1215a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T15:07:37.648405Z",
     "iopub.status.busy": "2026-01-30T15:07:37.648054Z",
     "iopub.status.idle": "2026-01-30T15:07:40.149732Z",
     "shell.execute_reply": "2026-01-30T15:07:40.148601Z"
    },
    "papermill": {
     "duration": 2.509185,
     "end_time": "2026-01-30T15:07:40.151805",
     "exception": false,
     "start_time": "2026-01-30T15:07:37.642620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ONI data...\n",
      "ONI V3 Created (Leakage Fixed & Syntax Corrected).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "def download_oni_v3():\n",
    "    print(\"Downloading ONI data...\")\n",
    "    url = \"https://www.cpc.ncep.noaa.gov/data/indices/oni.ascii.txt\"\n",
    "    try:\n",
    "        # Parse the fixed-width/whitespace-separated file\n",
    "        df = pd.read_table(url, sep=r'\\s+', engine='python')\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading ONI data: {e}\")\n",
    "        return\n",
    "\n",
    "    # Map Season string to the integer \"Center\" Month\n",
    "    season_map = {\n",
    "        'DJF': 1, 'JFM': 2, 'FMA': 3, 'MAM': 4, \n",
    "        'AMJ': 5, 'MJJ': 6, 'JJA': 7, 'JAS': 8, \n",
    "        'ASO': 9, 'SON': 10, 'OND': 11, 'NDJ': 12\n",
    "    }\n",
    "    \n",
    "    # 1. Rename 'SEAS' to 'month' directly so pd.to_datetime finds it\n",
    "    df['month'] = df['SEAS'].map(season_map)\n",
    "    df = df.rename(columns={'YR': 'year'})\n",
    "    \n",
    "    # 2. Create the \"Center Date\" (e.g., Jan 1st for DJF)\n",
    "    # pd.to_datetime automatically looks for 'year', 'month', 'day' columns\n",
    "    df['center_date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "    \n",
    "    # 3. APPLY PUBLICATION LAG (The Leakage Fix)\n",
    "    # The 'DJF' value (centered on Jan) is calculated after Feb ends.\n",
    "    # It is released by NOAA in early March.\n",
    "    # We add 2 months to the center date to simulate this release delay.\n",
    "    # Jan 1 (Center) -> March 1 (Available to public)\n",
    "    df['date_on'] = df['center_date'] + pd.DateOffset(months=2)\n",
    "    \n",
    "    # Clean up columns\n",
    "    oni = df[['date_on', 'ANOM']].rename(columns={'ANOM': 'climate_risk_ONI_index'})\n",
    "    \n",
    "    # Ensure sorted by date so ffill works forward in time\n",
    "    oni = oni.sort_values('date_on')\n",
    "\n",
    "    # 4. PREVENT INTERPOLATION LEAKAGE\n",
    "    # Use Forward Fill (ffill) only. \n",
    "    # Logic: On Jan 15, we don't know the Feb 1 value yet. \n",
    "    # We only know the last value published (likely from Nov/Dec).\n",
    "    # Linear interpolation would \"cheat\" by drawing a line to the future Feb 1 value.\n",
    "    oni = oni.set_index('date_on').resample('D').ffill().reset_index()\n",
    "    \n",
    "    # Calculate Momentum (safe now because the index is lagged correctly)\n",
    "    oni['climate_risk_ONI_momentum'] = oni['climate_risk_ONI_index'].diff(60)\n",
    "    \n",
    "    # Handle NaNs at the very start of history\n",
    "    oni = oni.fillna(0)\n",
    "    \n",
    "    # Save\n",
    "    oni.to_csv('external_oni.csv', index=False)\n",
    "    print(\"ONI V3 Created (Leakage Fixed & Syntax Corrected).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_oni_v3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f93beb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T15:07:40.159000Z",
     "iopub.status.busy": "2026-01-30T15:07:40.158645Z",
     "iopub.status.idle": "2026-01-30T15:07:41.510537Z",
     "shell.execute_reply": "2026-01-30T15:07:41.509412Z"
    },
    "papermill": {
     "duration": 1.357873,
     "end_time": "2026-01-30T15:07:41.512404",
     "exception": false,
     "start_time": "2026-01-30T15:07:40.154531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading MJO and PDO data from stable mirrors...\n",
      "‚úÖ MJO data downloaded successfully (BOM mirror).\n",
      "‚úÖ PDO data downloaded successfully (NOAA PSL archive).\n",
      "üìä Final file 'external_indices.csv' saved with 28460 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "def download_external_data():\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    print(\"Downloading MJO and PDO data from stable mirrors...\")\n",
    "\n",
    "    # --- 1. MJO (Madden-Julian Oscillation) ---\n",
    "    # Source: Australian Bureau of Meteorology (The primary stable mirror for RMM)\n",
    "    mjo_url = \"http://www.bom.gov.au/climate/mjo/graphics/rmm.74toRealtime.txt\"\n",
    "    try:\n",
    "        response = requests.get(mjo_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # BOM file has a 2-line header. \n",
    "        # Columns: year, month, day, RMM1, RMM2, phase, amplitude, source\n",
    "        mjo_df = pd.read_csv(io.StringIO(response.text), sep=r'\\s+', skiprows=2, header=None,\n",
    "                             names=['year', 'month', 'day', 'RMM1', 'RMM2', 'phase', 'amplitude', 'source'])\n",
    "        \n",
    "        mjo_df['date_on'] = pd.to_datetime(mjo_df[['year', 'month', 'day']])\n",
    "        \n",
    "        # PUBLICATION LAG: RMM is calculated daily with a ~1 day delay.\n",
    "        # Adding 2 days ensures zero data leakage.\n",
    "        mjo_df['date_on'] = mjo_df['date_on'] + pd.DateOffset(days=2)\n",
    "        \n",
    "        mjo_out = mjo_df[['date_on', 'phase', 'amplitude']].copy()\n",
    "        print(\"‚úÖ MJO data downloaded successfully (BOM mirror).\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading MJO: {e}\")\n",
    "        mjo_out = pd.DataFrame()\n",
    "\n",
    "    # --- 2. PDO (Pacific Decadal Oscillation) ---\n",
    "    # Source: NOAA Physical Sciences Laboratory (More stable than the NCEI link)\n",
    "    pdo_url = \"https://psl.noaa.gov/data/correlation/pdo.data\"\n",
    "    try:\n",
    "        response = requests.get(pdo_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # PSL .data files have 1 line of header, then rows: Year Jan Feb ... Dec\n",
    "        lines = response.text.split('\\n')\n",
    "        data_lines = []\n",
    "        for line in lines[1:]: # Skip the \"PDO\" header line\n",
    "            parts = line.split()\n",
    "            if len(parts) == 13: # Only take rows with Year + 12 months\n",
    "                data_lines.append(parts)\n",
    "            if \"99.9\" in line or \"-99.9\" in line: # Stop at footer\n",
    "                break\n",
    "                \n",
    "        pdo_raw = pd.DataFrame(data_lines, dtype=float)\n",
    "        pdo_raw.columns = ['year'] + [str(i) for i in range(1, 13)]\n",
    "        \n",
    "        # Melt to long format\n",
    "        pdo_long = pdo_raw.melt(id_vars='year', var_name='month', value_name='pdo_index')\n",
    "        pdo_long['date_on'] = pd.to_datetime(pdo_long[['year', 'month']].assign(day=1))\n",
    "        \n",
    "        # PUBLICATION LAG: Monthly PDO is released mid-next-month.\n",
    "        # Adding 45 days to the month-start prevents leakage.\n",
    "        pdo_long['date_on'] = pdo_long['date_on'] + pd.DateOffset(days=45)\n",
    "        \n",
    "        pdo_out = pdo_long[['date_on', 'pdo_index']].sort_values('date_on')\n",
    "        # Filter out future/placeholder dates (often 99.9 in these files)\n",
    "        pdo_out = pdo_out[pdo_out['pdo_index'] < 50] \n",
    "        print(\"‚úÖ PDO data downloaded successfully (NOAA PSL archive).\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading PDO: {e}\")\n",
    "        pdo_out = pd.DataFrame()\n",
    "\n",
    "    # --- 3. MERGE & SAVE ---\n",
    "    if not mjo_out.empty and not pdo_out.empty:\n",
    "        combined = pd.merge(mjo_out, pdo_out, on='date_on', how='outer').sort_values('date_on')\n",
    "        \n",
    "        # Forward fill ensures daily data availability for the main competition script\n",
    "        combined = combined.set_index('date_on').resample('D').ffill().reset_index()\n",
    "        combined = combined.fillna(0)\n",
    "        \n",
    "        # Cut off any dates beyond today (to prevent forecasting issues)\n",
    "        today = pd.Timestamp(datetime.date.today())\n",
    "        combined = combined[combined['date_on'] <= today]\n",
    "        \n",
    "        combined.to_csv('external_indices.csv', index=False)\n",
    "        print(f\"üìä Final file 'external_indices.csv' saved with {len(combined)} rows.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_external_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfff4067",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-30T15:07:41.520159Z",
     "iopub.status.busy": "2026-01-30T15:07:41.519833Z",
     "iopub.status.idle": "2026-01-30T19:28:33.828612Z",
     "shell.execute_reply": "2026-01-30T19:28:33.827437Z"
    },
    "papermill": {
     "duration": 15652.326602,
     "end_time": "2026-01-30T19:28:33.841727",
     "exception": false,
     "start_time": "2026-01-30T15:07:41.515125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "RUNNING: With External (Parallel + Precise + Harvest + Lagged ONI)\n",
      "==============================\n",
      "Step 1: Correlation Mining (Parallel Batches)...\n",
      "Processing 12 Raw Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82/82 [2:38:29<00:00, 115.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1.5: Filtering for top 1500 candidates...\n",
      "Step 2: Reconstructing Reduced Pool & Redundancy Matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rebuilding Pool: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82/82 [00:06<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Best Floor: 0.2 -> Score 80.5487\n",
      "\n",
      "==============================================================================================================\n",
      "Metric             | Private (Train)    | Public (Test)      | Full (All)        \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "CFCS Score         | 80.5487            | 80.9395            | 71.1650           \n",
      "Avg Sig Score      | 81.42              | 82.16              | 73.29             \n",
      "Max Corr Score     | 99.82              | 99.96              | 97.73             \n",
      "Sig Count %        | 49.47              | 49.36              | 26.00             \n",
      "==============================================================================================================\n",
      "Saved: submission_with_external.csv\n",
      "\n",
      "==============================\n",
      "RUNNING: No External (Parallel + Precise + Harvest + Lagged ONI)\n",
      "==============================\n",
      "Step 1: Correlation Mining (Parallel Batches)...\n",
      "Processing 12 Raw Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [1:34:20<00:00, 117.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1.5: Filtering for top 1500 candidates...\n",
      "Step 2: Reconstructing Reduced Pool & Redundancy Matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rebuilding Pool: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:07<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Best Floor: 0.2 -> Score 77.9404\n",
      "\n",
      "==============================================================================================================\n",
      "Metric             | Private (Train)    | Public (Test)      | Full (All)        \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "CFCS Score         | 77.9404            | 80.5428            | 69.9727           \n",
      "Avg Sig Score      | 76.85              | 80.94              | 69.67             \n",
      "Max Corr Score     | 99.67              | 99.95              | 97.84             \n",
      "Sig Count %        | 48.08              | 50.44              | 28.94             \n",
      "==============================================================================================================\n",
      "Saved: submission_no_external.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "LB_START_DATE = '2023-12-20' \n",
    "# Windows: Added 7 for ultra-fast reactions, 14 for fast, kept long windows for trends\n",
    "WINDOWS = [7, 14, 30, 60, 120, 240, 365, 450, 500]\n",
    "# Powers: Added 3.0 for extreme tail events\n",
    "POWERS = [0.25, 0.5, 1.0, 2.0, 3.0]\n",
    "REDUNDANCY_CAP = 0.90 \n",
    "TARGET_PHALANX = 80 \n",
    "# Added 0.20 floor to capture weaker but unique signals\n",
    "FLOORS_TO_TEST = [0.20, 0.25, 0.30, 0.35, 0.40]\n",
    "# Increased K to find more candidates for optimizer\n",
    "PRE_SELECT_K = 1500 \n",
    "MAX_WORKERS = 4 \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def reduce_mem_usage(df, verbose=False):\n",
    "    \"\"\"\n",
    "    Downcasts Integers to save memory. \n",
    "    Floats are kept as float64 to maintain exact scoring precision.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object and col_type != '<M8[ns]':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            # SKIPPING float downcasting to preserve precision\n",
    "    if verbose:\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        print(f'Memory usage reduced to {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
    "    return df\n",
    "\n",
    "def load_data():\n",
    "    potential_dirs = ['data', '/kaggle/input/forecasting-the-future-the-helios-corn-climate-challenge']\n",
    "    for d in potential_dirs:\n",
    "        if os.path.exists(d) and os.path.exists(os.path.join(d, 'corn_climate_risk_futures_daily_master.csv')):\n",
    "            df = pd.read_csv(os.path.join(d, 'corn_climate_risk_futures_daily_master.csv'))\n",
    "            df['date_on'] = pd.to_datetime(df['date_on'])\n",
    "            return df, pd.read_csv(os.path.join(d, 'corn_regional_market_share.csv'))\n",
    "    raise FileNotFoundError(\"Data not found.\")\n",
    "\n",
    "def generate_b66_skeleton(df, market_share_df):\n",
    "    orig_climate_cols = [c for c in df.columns if c.startswith('climate_risk_')]\n",
    "    b_df = df.copy()\n",
    "    b_df = b_df.merge(market_share_df[['region_id', 'percent_country_production']], on='region_id', how='left')\n",
    "    b_df['percent_country_production'] = b_df['percent_country_production'].fillna(1.0)\n",
    "    \n",
    "    risk_cats = ['heat_stress', 'unseasonably_cold', 'excess_precip', 'drought']\n",
    "    for r in risk_cats:\n",
    "        l, m, h = [f'climate_risk_cnt_locations_{r}_risk_{x}' for x in ['low', 'medium', 'high']]\n",
    "        if m not in b_df.columns: continue\n",
    "        s_col = f'climate_risk_{r}_score'\n",
    "        b_df[s_col] = (b_df[m] + 2 * b_df[h]) / (b_df[l] + b_df[m] + b_df[h] + 1e-6)\n",
    "        w_col = f'climate_risk_{r}_weighted'\n",
    "        b_df[w_col] = b_df[s_col] * (b_df['percent_country_production'] / 100)\n",
    "\n",
    "    b_df = b_df.sort_values(['region_id', 'date_on'])\n",
    "    \n",
    "    for w in [7, 14, 30]:\n",
    "        for r in risk_cats:\n",
    "            s_col = f'climate_risk_{r}_score'\n",
    "            for stat in ['ma', 'max']:\n",
    "                col_name = f'climate_risk_{r}_{stat}_{w}d'\n",
    "                if stat == 'ma':\n",
    "                    b_df[col_name] = b_df.groupby('region_id')[s_col].rolling(window=w, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "                else:\n",
    "                    b_df[col_name] = b_df.groupby('region_id')[s_col].rolling(window=w, min_periods=1).max().reset_index(level=0, drop=True)\n",
    "\n",
    "    for r in risk_cats:\n",
    "        s_col = f'climate_risk_{r}_score'\n",
    "        for suff in ['change_1d', 'change_7d', 'acceleration']:\n",
    "            col_name = f'climate_risk_{r}_{suff}'\n",
    "            if suff == 'change_1d':\n",
    "                b_df[col_name] = b_df.groupby('region_id')[s_col].diff(1)\n",
    "            elif suff == 'change_7d':\n",
    "                b_df[col_name] = b_df.groupby('region_id')[s_col].diff(7)\n",
    "            else:\n",
    "                b_df[col_name] = b_df.groupby('region_id')[f'climate_risk_{r}_change_1d'].diff(1)\n",
    "\n",
    "    for r in risk_cats:\n",
    "        s_col = f'climate_risk_{r}_score'\n",
    "        w_col = f'climate_risk_{r}_weighted'\n",
    "        country_agg = b_df.groupby(['country_name', 'date_on']).agg({\n",
    "            s_col: ['mean', 'max', 'std'],\n",
    "            w_col: 'sum',\n",
    "            'percent_country_production': 'sum'\n",
    "        })\n",
    "        country_agg.columns = [f'country_{r}_{\"_\".join(col).strip()}' for col in country_agg.columns]\n",
    "        b_df = b_df.merge(country_agg.reset_index(), on=['country_name', 'date_on'], how='left')\n",
    "\n",
    "    filtered_df = b_df.dropna()\n",
    "    cr_baseline = [c for c in filtered_df.columns if c.startswith('climate_risk_') and c not in orig_climate_cols]\n",
    "    return reduce_mem_usage(filtered_df), cr_baseline\n",
    "\n",
    "def generate_base_daily_features(df, share, use_external=False):\n",
    "    \"\"\"Generates the base daily dataframe (aggregated regions, interactions) WITHOUT expanding variants yet.\"\"\"\n",
    "    df_calc = df.copy()\n",
    "    df_calc = df_calc.merge(share[['region_id', 'percent_country_production']], on='region_id', how='left')\n",
    "    df_calc['month'] = df_calc['date_on'].dt.month\n",
    "    \n",
    "    # REVERTED TO STANDARD PHENOLOGY (Non-Circular) - Proven superior\n",
    "    PHENO_MAP = {'US': (7.2, 1.4, 'United States'), 'BR_Sum': (2.5, 3.0, 'Brazil'), \n",
    "                 'BR_Saf': (5.5, 2.5, 'Brazil'), 'AR': (1.5, 2.0, 'Argentina'), 'UA': (7.0, 1.5, 'Ukraine')}\n",
    "    \n",
    "    risk_types = ['heat_stress', 'drought', 'excess_precip', 'unseasonably_cold']\n",
    "    daily = pd.DataFrame({'date_on': df_calc['date_on'].unique()})\n",
    "    \n",
    "    # Critical Sort\n",
    "    daily = daily.sort_values('date_on').reset_index(drop=True)\n",
    "    \n",
    "    # 1. Standard Phenology Aggregation\n",
    "    for pid, (pk, wd, country) in PHENO_MAP.items():\n",
    "        c_df = df_calc[df_calc['country_name'] == country].copy()\n",
    "        # Standard Gaussian (No circular min())\n",
    "        c_df['m_weight'] = np.exp(-((c_df['month'] - pk)**2) / (2 * wd**2))\n",
    "        \n",
    "        for r in risk_types:\n",
    "            cl, cm, ch = [f'climate_risk_cnt_locations_{r}_risk_{x}' for x in ['low', 'medium', 'high']]\n",
    "            if cm not in c_df.columns: continue\n",
    "            score = (c_df[cm] + 2 * c_df[ch]) / (c_df[cl] + c_df[cm] + c_df[ch] + 1e-6)\n",
    "            c_df['v'] = score * (c_df['percent_country_production'] / 100) * c_df['m_weight']\n",
    "            agg = c_df.groupby('date_on')['v'].sum().reset_index().rename(columns={'v': f'{pid}_{r}'})\n",
    "            daily = daily.merge(agg, on='date_on', how='left')\n",
    "            \n",
    "    # 2. US Special Windows (Planting & Harvest)\n",
    "    us_df = df_calc[df_calc['country_name'] == 'United States'].copy()\n",
    "    \n",
    "    # Planting: April/May (Standard Weighting)\n",
    "    us_df['plant_weight'] = np.exp(-((us_df['month'] - 4.5)**2) / (2 * 1.0**2))\n",
    "    # Harvest: October (Standard Weighting)\n",
    "    us_df['harv_weight'] = np.exp(-((us_df['month'] - 10.0)**2) / (2 * 1.0**2))\n",
    "    \n",
    "    for r in ['excess_precip', 'unseasonably_cold']:\n",
    "        if f'climate_risk_cnt_locations_{r}_risk_medium' not in us_df.columns: continue\n",
    "        cl, cm, ch = [f'climate_risk_cnt_locations_{r}_risk_{x}' for x in ['low', 'medium', 'high']]\n",
    "        score = (us_df[cm] + 2 * us_df[ch]) / (us_df[cl] + us_df[cm] + us_df[ch] + 1e-6)\n",
    "        \n",
    "        # Planting Features\n",
    "        us_df['v_p'] = score * (us_df['percent_country_production'] / 100) * us_df['plant_weight']\n",
    "        agg_p = us_df.groupby('date_on')['v_p'].sum().reset_index().rename(columns={'v_p': f'US_Planting_{r}'})\n",
    "        daily = daily.merge(agg_p, on='date_on', how='left')\n",
    "        \n",
    "        # Harvest Features\n",
    "        us_df['v_h'] = score * (us_df['percent_country_production'] / 100) * us_df['harv_weight']\n",
    "        agg_h = us_df.groupby('date_on')['v_h'].sum().reset_index().rename(columns={'v_h': f'US_Harvest_{r}'})\n",
    "        daily = daily.merge(agg_h, on='date_on', how='left')\n",
    "\n",
    "    daily = daily.fillna(0)\n",
    "    \n",
    "    # 3. Compound Logic\n",
    "    for r in risk_types:\n",
    "        cols = [c for c in daily.columns if c.endswith(f'_{r}') and 'Inter' not in c and 'Global' not in c and 'Planting' not in c and 'Harvest' not in c]\n",
    "        if cols: daily[f'Global_{r}'] = daily[cols].sum(axis=1)\n",
    "        \n",
    "        north_cols = [c for c in cols if 'US' in c or 'UA' in c]\n",
    "        south_cols = [c for c in cols if 'BR' in c or 'AR' in c]\n",
    "        if north_cols: daily[f'North_{r}'] = daily[north_cols].sum(axis=1)\n",
    "        if south_cols: daily[f'South_{r}'] = daily[south_cols].sum(axis=1)\n",
    "    \n",
    "    # Interactions\n",
    "    daily['Global_Heat_Drought'] = daily.get('Global_heat_stress',0) * daily.get('Global_drought',0)\n",
    "    daily['US_Heat_Drought'] = daily.get('US_heat_stress',0) * daily.get('US_drought',0)\n",
    "    daily['AR_Heat_Drought'] = daily.get('AR_heat_stress', 0) * daily.get('AR_drought', 0)\n",
    "    \n",
    "    daily['BR_Heat_Drought'] = (daily.get('BR_Sum_heat_stress', 0) + daily.get('BR_Saf_heat_stress', 0)) * \\\n",
    "                               (daily.get('BR_Sum_drought', 0) + daily.get('BR_Saf_drought', 0))\n",
    "    \n",
    "    daily['North_x_South_Failure'] = (daily.get('North_heat_stress',0) + daily.get('North_drought',0)) * \\\n",
    "                                     (daily.get('South_heat_stress',0) + daily.get('South_drought',0))\n",
    "    \n",
    "    # 4. External Data\n",
    "    if use_external:\n",
    "        # Load ONI (Existing)\n",
    "        if os.path.exists('external_oni.csv'):\n",
    "            oni = pd.read_csv('external_oni.csv')\n",
    "            oni['date_on'] = pd.to_datetime(oni['date_on'])\n",
    "            oni = oni.sort_values('date_on')\n",
    "            daily = daily.merge(oni, on='date_on', how='left')\n",
    "        \n",
    "        # Load MJO/PDO (New)\n",
    "        if os.path.exists('external_indices.csv'):\n",
    "            indices = pd.read_csv('external_indices.csv')\n",
    "            indices['date_on'] = pd.to_datetime(indices['date_on'])\n",
    "            daily = daily.merge(indices, on='date_on', how='left')\n",
    "            \n",
    "        daily = daily.ffill().bfill() # Fill any gaps after merging all external sources\n",
    "        \n",
    "        # --- A. ONI (Nino/Nina) ---\n",
    "        if 'climate_risk_ONI_index' in daily.columns:\n",
    "            # La Ni√±a Multiplier (cold phase)\n",
    "            daily['La_Nina_Multiplier'] = daily['climate_risk_ONI_index'].apply(lambda x: abs(x) if x < -0.5 else 0)\n",
    "            # El Ni√±o Multiplier (warm phase)\n",
    "            daily['El_Nino_Multiplier'] = daily['climate_risk_ONI_index'].apply(lambda x: x if x > 0.5 else 0)\n",
    "            \n",
    "            # La Ni√±a Interactions\n",
    "            daily['climate_risk_Global_Stress_x_Nina'] = daily['Global_Heat_Drought'] * daily['La_Nina_Multiplier']\n",
    "            daily['climate_risk_US_Stress_x_Nina'] = daily['US_Heat_Drought'] * daily['La_Nina_Multiplier']\n",
    "            daily['climate_risk_South_Stress_x_Nina'] = (daily.get('South_drought', 0) + daily.get('South_heat_stress', 0)) * daily['La_Nina_Multiplier']\n",
    "            \n",
    "            # El Ni√±o Interactions\n",
    "            daily['climate_risk_Global_Stress_x_Nino'] = daily['Global_Heat_Drought'] * daily['El_Nino_Multiplier']\n",
    "            daily['climate_risk_US_Stress_x_Nino'] = daily['US_Heat_Drought'] * daily['El_Nino_Multiplier']\n",
    "            daily['climate_risk_South_Stress_x_Nino'] = (daily.get('South_drought', 0) + daily.get('South_heat_stress', 0)) * daily['El_Nino_Multiplier']\n",
    "            \n",
    "            # Priming Features\n",
    "            daily['climate_risk_US_Stress_x_Nina_Primed'] = daily['US_Heat_Drought'] * daily['La_Nina_Multiplier'].shift(90).fillna(0)\n",
    "            daily['climate_risk_US_Stress_x_Nina_Primed_120'] = daily['US_Heat_Drought'] * daily['La_Nina_Multiplier'].shift(120).fillna(0)\n",
    "            daily['climate_risk_Global_Stress_x_Nina_Primed'] = daily['Global_Heat_Drought'] * daily['La_Nina_Multiplier'].shift(90).fillna(0)\n",
    "            daily['climate_risk_US_Stress_x_Nino_Primed'] = daily['US_Heat_Drought'] * daily['El_Nino_Multiplier'].shift(90).fillna(0)\n",
    "            \n",
    "            if 'US_Planting_excess_precip' in daily.columns:\n",
    "                daily['climate_risk_US_Wet_Planting_x_Nina'] = daily['US_Planting_excess_precip'] * daily['La_Nina_Multiplier']\n",
    "                daily['climate_risk_US_Wet_Planting_x_Nino'] = daily['US_Planting_excess_precip'] * daily['El_Nino_Multiplier']\n",
    "                \n",
    "            # Momentum/Accel\n",
    "            daily['climate_risk_Nina_Momentum_Gate'] = daily.get('Global_heat_stress',0) * daily['climate_risk_ONI_momentum'].apply(lambda x: abs(x) if x < 0 else 0)\n",
    "            daily['climate_risk_Nino_Momentum_Gate'] = daily.get('Global_heat_stress',0) * daily['climate_risk_ONI_momentum'].apply(lambda x: x if x > 0 else 0)\n",
    "            daily['climate_risk_ONI_acceleration'] = daily['climate_risk_ONI_momentum'].diff(7).fillna(0)\n",
    "            daily['climate_risk_Stress_x_ONI_Accel'] = daily['Global_Heat_Drought'] * daily['climate_risk_ONI_acceleration'].abs()\n",
    "\n",
    "        # --- B. MJO (Madden-Julian Oscillation) ---\n",
    "        # Phase: 1-8. Amplitude: Strength.\n",
    "        if 'phase' in daily.columns and 'amplitude' in daily.columns:\n",
    "            # Amplitude Modulation\n",
    "            daily['MJO_Amp'] = daily['amplitude'].fillna(0)\n",
    "            daily['climate_risk_Global_Stress_x_MJO_Amp'] = daily['Global_Heat_Drought'] * daily['MJO_Amp']\n",
    "            \n",
    "            # Phase Interactions (One-hot encoding implicit via conditions)\n",
    "            # Phases 8, 1, 2: Often wet for Western US / dangerous for some regions\n",
    "            daily['MJO_Phase_812'] = daily['phase'].isin([8, 1, 2]).astype(int)\n",
    "            # Phases 4, 5, 6: Often dry\n",
    "            daily['MJO_Phase_456'] = daily['phase'].isin([4, 5, 6]).astype(int)\n",
    "            \n",
    "            # Interaction with US Precip/Drought\n",
    "            if 'US_excess_precip' in daily.columns:\n",
    "                 daily['climate_risk_US_Wet_x_MJO_812'] = daily['US_excess_precip'] * daily['MJO_Phase_812'] * daily['MJO_Amp']\n",
    "            if 'US_drought' in daily.columns:\n",
    "                 daily['climate_risk_US_Drought_x_MJO_456'] = daily['US_drought'] * daily['MJO_Phase_456'] * daily['MJO_Amp']\n",
    "\n",
    "        # --- C. PDO (Pacific Decadal Oscillation) ---\n",
    "        if 'pdo_index' in daily.columns:\n",
    "            daily['PDO_Index'] = daily['pdo_index'].fillna(0)\n",
    "            # Positive PDO: Warm Pacific Coast\n",
    "            daily['PDO_Positive'] = daily['PDO_Index'].apply(lambda x: x if x > 0 else 0)\n",
    "            daily['PDO_Negative'] = daily['PDO_Index'].apply(lambda x: abs(x) if x < 0 else 0)\n",
    "            \n",
    "            # Constructive Interference (PDO x ONI)\n",
    "            # When PDO and ONI are same sign, effects can be amplified\n",
    "            if 'climate_risk_ONI_index' in daily.columns:\n",
    "                daily['climate_risk_PDO_x_ONI_Resonance'] = daily['PDO_Index'] * daily['climate_risk_ONI_index']\n",
    "                # Amplified Stress when both are 'hot' (El Nino + +PDO)\n",
    "                daily['climate_risk_Global_Stress_x_Nino_PDO'] = daily['Global_Heat_Drought'] * daily['El_Nino_Multiplier'] * daily['PDO_Positive']\n",
    "\n",
    "    # 5. Time Shifts - expanded lag windows\n",
    "    if 'US_heat_stress' in daily.columns:\n",
    "        daily['climate_risk_US_PreStressed_Heat'] = daily['US_heat_stress'] * daily.get('US_drought', pd.Series(0, index=daily.index)).shift(30).fillna(0)\n",
    "        daily['climate_risk_US_PreStressed_Heat_7d'] = daily['US_heat_stress'] * daily.get('US_drought', pd.Series(0, index=daily.index)).shift(7).fillna(0)\n",
    "    if 'Global_heat_stress' in daily.columns:\n",
    "        daily['climate_risk_Global_PreStressed_Heat'] = daily['Global_heat_stress'] * daily.get('Global_drought', pd.Series(0, index=daily.index)).shift(30).fillna(0)\n",
    "        daily['climate_risk_Global_PreStressed_Heat_120d'] = daily['Global_heat_stress'] * daily.get('Global_drought', pd.Series(0, index=daily.index)).shift(120).fillna(0)\n",
    "    if 'South_heat_stress' in daily.columns:\n",
    "        daily['climate_risk_South_PreStressed_Heat'] = daily['South_heat_stress'] * daily.get('South_drought', pd.Series(0, index=daily.index)).shift(30).fillna(0)\n",
    "    \n",
    "    # 6. Cross-hemispheric divergence - NEW\n",
    "    if 'North_heat_stress' in daily.columns and 'South_heat_stress' in daily.columns:\n",
    "        daily['climate_risk_Hemisphere_Divergence'] = abs(daily['North_heat_stress'] - daily['South_heat_stress'])\n",
    "        daily['climate_risk_Cold_Wet_Compound'] = (daily.get('North_unseasonably_cold', 0) + daily.get('US_Planting_excess_precip', 0)) * \\\n",
    "                                                   (daily.get('South_excess_precip', 0) + daily.get('South_unseasonably_cold', 0))\n",
    "        \n",
    "    return reduce_mem_usage(daily)\n",
    "\n",
    "def expand_single_feature(base_series, col_name, date_series):\n",
    "    \"\"\"Generates variants for a SINGLE column. Returns a DataFrame.\"\"\"\n",
    "    expanded = {}\n",
    "    \n",
    "    # Expanded lag windows: added 7, 14, 120 for better temporal coverage\n",
    "    for lag in [0, 7, 14, 30, 60, 90, 120]:\n",
    "        s_base = base_series.shift(lag).fillna(0)\n",
    "        suffix = f\"_L{lag}\" if lag > 0 else \"\"\n",
    "        for pwr in POWERS:\n",
    "            sig = s_base ** pwr\n",
    "            mom = sig.diff(7).fillna(0)\n",
    "            for w in WINDOWS:\n",
    "                expanded[f\"climate_risk_EMA_{col_name}{suffix}_P{pwr}_W{w}\"] = sig.ewm(span=w).mean()\n",
    "                if w >= 60:\n",
    "                    expanded[f\"climate_risk_STD_{col_name}{suffix}_P{pwr}_W{w}\"] = sig.rolling(w).std().fillna(0)\n",
    "                    expanded[f\"climate_risk_TURB_{col_name}{suffix}_P{pwr}_W{w}\"] = mom.rolling(w).std().fillna(0)\n",
    "    \n",
    "    return pd.DataFrame(expanded)\n",
    "\n",
    "def get_corrs_batch(df_features, df_targets, df_metadata):\n",
    "    \"\"\"Calculates correlations for a batch of features against targets.\"\"\"\n",
    "    local_df = pd.concat([df_features, df_targets, df_metadata], axis=1)\n",
    "    f_cols = df_targets.columns.tolist()\n",
    "    c_cols = df_features.columns.tolist()\n",
    "    \n",
    "    results = {c: [] for c in c_cols}\n",
    "    groups = local_df.groupby(['crop_name', 'country_name', 'date_on_month'])\n",
    "    \n",
    "    for _, group in groups:\n",
    "        if len(group) < 5: continue\n",
    "        \n",
    "        cols_to_use = c_cols + f_cols\n",
    "        num = group[cols_to_use].select_dtypes(include=np.number).astype(np.float64)\n",
    "        \n",
    "        v_std = num.std()\n",
    "        valid_cols = v_std[v_std > 1e-5].index\n",
    "        \n",
    "        valid_targets = [f for f in f_cols if f in valid_cols]\n",
    "        valid_cands = [c for c in c_cols if c in valid_cols]\n",
    "        \n",
    "        if not valid_targets or not valid_cands:\n",
    "            continue\n",
    "            \n",
    "        corr_mat = num[valid_cands + valid_targets].corr()\n",
    "        \n",
    "        for c in valid_cands:\n",
    "            res = corr_mat.loc[c, valid_targets].values\n",
    "            results[c].append(res)\n",
    "            \n",
    "    final_corrs = {}\n",
    "    for c in c_cols:\n",
    "        if results[c]:\n",
    "            final_corrs[c] = np.concatenate(results[c])\n",
    "        else:\n",
    "            final_corrs[c] = np.array([], dtype=np.float64)\n",
    "            \n",
    "    return final_corrs\n",
    "\n",
    "def process_batch_task(base_col, base_data, train_dates, train_targets, train_meta):\n",
    "    \"\"\"Worker function for parallel processing.\"\"\"\n",
    "    # 1. Expand\n",
    "    expanded_df = expand_single_feature(base_data[base_col], base_col, base_data['date_on'])\n",
    "    \n",
    "    # 2. Align with Train\n",
    "    batch_with_date = pd.concat([base_data[['date_on']], expanded_df], axis=1)\n",
    "    aligned_batch = train_dates.merge(batch_with_date, on='date_on', how='left')\n",
    "    \n",
    "    feat_batch = aligned_batch.drop(columns=['date_on']).fillna(0)\n",
    "    \n",
    "    # 3. Calculate Correlations\n",
    "    corrs = get_corrs_batch(feat_batch, train_targets, train_meta)\n",
    "    return corrs\n",
    "\n",
    "def calculate_cfcs(all_corrs_flat):\n",
    "    if len(all_corrs_flat) == 0: return 0, 0, 0, 0\n",
    "    flat = np.floor(np.array(all_corrs_flat, dtype=np.float64) * 100000) / 100000\n",
    "    abs_flat = np.abs(flat)\n",
    "    sig = abs_flat[abs_flat >= 0.5]\n",
    "    avg_s = np.mean(sig) if len(sig) > 0 else 0\n",
    "    max_c = np.max(abs_flat)\n",
    "    pct = len(sig) / len(flat)\n",
    "    score = (50 * min(1.0, avg_s)) + (30 * min(1.0, max_c)) + (20 * pct)\n",
    "    return score, avg_s, max_c, pct\n",
    "\n",
    "def run_optimizer_round(density_floor, candidates, densities, cache, raw_cols, name_to_idx, red_matrix, king):\n",
    "    selected = [king]\n",
    "    sniper_pool = [c for c in candidates if c != king and densities.get(c, 0) >= density_floor]\n",
    "    \n",
    "    current_flat = np.concatenate([cache[f] for f in raw_cols + selected if len(cache[f]) > 0])\n",
    "    current_score = calculate_cfcs(current_flat)[0]\n",
    "    \n",
    "    # Forward Selection\n",
    "    for i in range(TARGET_PHALANX - 1):\n",
    "        best_cand, best_new_score = None, -1\n",
    "        selected_indices = [name_to_idx[s] for s in selected]\n",
    "        \n",
    "        for c in sniper_pool:\n",
    "            c_idx = name_to_idx[c]\n",
    "            is_redundant = False\n",
    "            for s_idx in selected_indices:\n",
    "                if abs(red_matrix[c_idx, s_idx]) > REDUNDANCY_CAP:\n",
    "                    is_redundant = True; break\n",
    "            if is_redundant: continue\n",
    "            \n",
    "            trial_flat = np.concatenate([current_flat, cache[c]])\n",
    "            score = calculate_cfcs(trial_flat)[0]\n",
    "            if score > best_new_score: best_new_score, best_cand = score, c\n",
    "            \n",
    "        if best_cand:\n",
    "            selected.append(best_cand)\n",
    "            sniper_pool.remove(best_cand)\n",
    "            current_flat = np.concatenate([current_flat, cache[best_cand]])\n",
    "            current_score = best_new_score\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Backward Elimination\n",
    "    final_list = raw_cols + selected\n",
    "    improved = True\n",
    "    while improved:\n",
    "        improved = False\n",
    "        if len(final_list) < 20: break \n",
    "        for feat in [f for f in final_list if f not in raw_cols and f != king]:\n",
    "            temp_list = [f for f in final_list if f != feat]\n",
    "            temp_flat = np.concatenate([cache[f] for f in temp_list if len(cache[f]) > 0])\n",
    "            score = calculate_cfcs(temp_flat)[0]\n",
    "            if score > current_score + 1e-6:\n",
    "                current_score = score\n",
    "                final_list = temp_list\n",
    "                current_flat = temp_flat\n",
    "                improved = True\n",
    "                break\n",
    "    return current_score, [f for f in final_list if f not in raw_cols]\n",
    "\n",
    "def run_pipeline(use_external=False, label=\"Experiment\"):\n",
    "    print(f\"\\n{'='*30}\\nRUNNING: {label} (Parallel + Precise + Harvest + Lagged ONI)\\n{'='*30}\")\n",
    "    \n",
    "    df_raw, share = load_data()\n",
    "    skeleton, baseline_cr_cols = generate_b66_skeleton(df_raw, share)\n",
    "    skeleton = reduce_mem_usage(skeleton)\n",
    "    \n",
    "    base_daily = generate_base_daily_features(df_raw, share, use_external=use_external)\n",
    "    \n",
    "    f_cols = [c for c in skeleton.columns if c.startswith('futures_')]\n",
    "    target_df = skeleton[['date_on'] + f_cols].copy()\n",
    "    meta_df = skeleton[['date_on', 'crop_name', 'country_name', 'date_on_month']].copy()\n",
    "    \n",
    "    raw_cols = [c for c in df_raw.columns if c.startswith('climate_risk_cnt_locations_')]\n",
    "    base_candidates = [c for c in base_daily.columns if c != 'date_on']\n",
    "    \n",
    "    print(\"Step 1: Correlation Mining (Parallel Batches)...\")\n",
    "    cache = {} \n",
    "    \n",
    "    priv_mask = skeleton['date_on'] < LB_START_DATE\n",
    "    train_skel = skeleton.loc[priv_mask].reset_index(drop=True)\n",
    "    train_targets = target_df.loc[priv_mask, f_cols].reset_index(drop=True)\n",
    "    train_meta = meta_df.loc[priv_mask, ['crop_name', 'country_name', 'date_on_month']].reset_index(drop=True)\n",
    "    train_dates = skeleton.loc[priv_mask, ['date_on']].reset_index(drop=True)\n",
    "\n",
    "    # A. Raw Cols\n",
    "    print(f\"Processing {len(raw_cols)} Raw Features...\")\n",
    "    raw_corrs = get_corrs_batch(train_skel[raw_cols], train_targets, train_meta)\n",
    "    cache.update(raw_corrs)\n",
    "    \n",
    "    # B. Expanded Features (Parallel)\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {}\n",
    "        for base_col in base_candidates:\n",
    "            f = executor.submit(process_batch_task, base_col, base_daily, train_dates, train_targets, train_meta)\n",
    "            futures[f] = base_col\n",
    "            \n",
    "        for f in tqdm(as_completed(futures), total=len(base_candidates), desc=\"Batches\"):\n",
    "            res = f.result()\n",
    "            cache.update(res)\n",
    "\n",
    "    print(f\"Step 1.5: Filtering for top {PRE_SELECT_K} candidates...\")\n",
    "    candidates = list(cache.keys())\n",
    "    scores_dict = {c: np.max(np.abs(cache[c])) if len(cache[c]) > 0 else 0 for c in candidates}\n",
    "    sorted_cands = sorted([c for c in candidates if c not in raw_cols], key=lambda x: scores_dict[x], reverse=True)\n",
    "    active_candidates = sorted_cands[:PRE_SELECT_K]\n",
    "    \n",
    "    best_max, king = -1, None\n",
    "    densities = {}\n",
    "    for c in active_candidates:\n",
    "        arr = cache[c]\n",
    "        densities[c] = len(arr[np.abs(arr) >= 0.5]) / len(arr)\n",
    "        m = np.max(np.abs(arr))\n",
    "        if m > best_max: best_max, king = m, c\n",
    "\n",
    "    # --- STEP 2: RECONSTRUCT POOL ---\n",
    "    print(\"Step 2: Reconstructing Reduced Pool & Redundancy Matrix...\")\n",
    "    reconstructed_features = []\n",
    "    \n",
    "    for base_col in tqdm(base_candidates, desc=\"Rebuilding Pool\"):\n",
    "        relevant_cols = [ac for ac in active_candidates if base_col in ac]\n",
    "        if not relevant_cols: continue\n",
    "        \n",
    "        batch_expanded = expand_single_feature(base_daily[base_col], base_col, base_daily['date_on'])\n",
    "        cols_to_keep = [c for c in batch_expanded.columns if c in active_candidates]\n",
    "        \n",
    "        if cols_to_keep:\n",
    "            batch_with_date = pd.concat([base_daily[['date_on']], batch_expanded[cols_to_keep]], axis=1)\n",
    "            reconstructed_features.append(batch_with_date)\n",
    "\n",
    "    if reconstructed_features:\n",
    "        pool_df = pd.DataFrame({'date_on': base_daily['date_on']})\n",
    "        for pdf in reconstructed_features:\n",
    "            pool_df = pool_df.merge(pdf, on='date_on', how='left')\n",
    "    else:\n",
    "        pool_df = pd.DataFrame({'date_on': base_daily['date_on']})\n",
    "\n",
    "    pool_subset_cols = [c for c in pool_df.columns if c in active_candidates]\n",
    "    if not pool_subset_cols:\n",
    "        redundancy_matrix = np.eye(len(active_candidates))\n",
    "    else:\n",
    "        red_aligned = skeleton[['date_on']].merge(pool_df, on='date_on', how='left')\n",
    "        redundancy_matrix = np.corrcoef(red_aligned[active_candidates].fillna(0).values.T)\n",
    "        del red_aligned\n",
    "        gc.collect()\n",
    "        \n",
    "    name_to_idx = {name: i for i, name in enumerate(active_candidates)}\n",
    "    \n",
    "    best_overall_score, best_features, best_floor = -1, [], -1\n",
    "    for floor in FLOORS_TO_TEST:\n",
    "        score, feats = run_optimizer_round(floor, active_candidates, densities, cache, raw_cols, name_to_idx, redundancy_matrix, king)\n",
    "        if score > best_overall_score: best_overall_score, best_features, best_floor = score, feats, floor\n",
    "\n",
    "    print(f\"üèÜ Best Floor: {best_floor} -> Score {best_overall_score:.4f}\")\n",
    "    \n",
    "    skeleton_clean = skeleton.drop(columns=[c for c in baseline_cr_cols if c in skeleton.columns], errors='ignore')\n",
    "    final_sub = skeleton_clean.merge(pool_df[['date_on'] + best_features], on='date_on', how='left')\n",
    "    \n",
    "    final_cr_cols = [c for c in final_sub.columns if c.startswith('climate_risk_')]\n",
    "    final_sub[final_cr_cols] = final_sub[final_cr_cols].fillna(0)\n",
    "    \n",
    "    def score_ds(df_sub, feats):\n",
    "        if len(df_sub) == 0: return (0,0,0,0)\n",
    "        t_sub = df_sub[f_cols]\n",
    "        m_sub = df_sub[['crop_name', 'country_name', 'date_on_month']]\n",
    "        f_sub = df_sub[feats]\n",
    "        res_dict = get_corrs_batch(f_sub, t_sub, m_sub)\n",
    "        flat = np.concatenate(list(res_dict.values())) if res_dict else []\n",
    "        return calculate_cfcs(flat)\n",
    "    \n",
    "    res_p = score_ds(final_sub[final_sub['date_on'] < LB_START_DATE], final_cr_cols)\n",
    "    res_b = score_ds(final_sub[final_sub['date_on'] >= LB_START_DATE], final_cr_cols)\n",
    "    res_f = score_ds(final_sub, final_cr_cols)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*110 + f\"\\n{'Metric':<18} | {'Private (Train)':<18} | {'Public (Test)':<18} | {'Full (All)':<18}\\n\" + \"-\" * 110)\n",
    "    print(f\"{'CFCS Score':<18} | {res_p[0]:<18.4f} | {res_b[0]:<18.4f} | {res_f[0]:<18.4f}\")\n",
    "    print(f\"{'Avg Sig Score':<18} | {res_p[1]*100:<18.2f} | {res_b[1]*100:<18.2f} | {res_f[1]*100:<18.2f}\")\n",
    "    print(f\"{'Max Corr Score':<18} | {res_p[2]*100:<18.2f} | {res_b[2]*100:<18.2f} | {res_f[2]*100:<18.2f}\")\n",
    "    print(f\"{'Sig Count %':<18} | {res_p[3]*100:<18.2f} | {res_b[3]*100:<18.2f} | {res_f[3]*100:<18.2f}\\n\" + \"=\"*110)\n",
    "    \n",
    "    filename = f\"submission_{label.lower().replace(' ', '_')}.csv\"\n",
    "    final_sub.to_csv(filename, index=False)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # RUN EXTERNAL FIRST as requested for faster feedback\n",
    "    run_pipeline(use_external=True, label=\"With External\")\n",
    "    run_pipeline(use_external=False, label=\"No External\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2deb7",
   "metadata": {
    "papermill": {
     "duration": 0.020341,
     "end_time": "2026-01-30T19:28:33.879350",
     "exception": false,
     "start_time": "2026-01-30T19:28:33.859009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecfbf55",
   "metadata": {
    "papermill": {
     "duration": 0.018732,
     "end_time": "2026-01-30T19:28:33.914783",
     "exception": false,
     "start_time": "2026-01-30T19:28:33.896051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15008526,
     "sourceId": 126158,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15661.021351,
   "end_time": "2026-01-30T19:28:34.759844",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-30T15:07:33.738493",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
