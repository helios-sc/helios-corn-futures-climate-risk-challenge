{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36300f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T09:35:45.462292Z",
     "iopub.status.busy": "2026-01-29T09:35:45.461992Z",
     "iopub.status.idle": "2026-01-29T09:36:05.559430Z",
     "shell.execute_reply": "2026-01-29T09:36:05.558341Z"
    },
    "papermill": {
     "duration": 20.105313,
     "end_time": "2026-01-29T09:36:05.561416",
     "exception": false,
     "start_time": "2026-01-29T09:35:45.456103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Config ---\n",
    "CONFIG_KEY = os.environ.get(\"CONFIG_KEY\", \"ZAF_m02_private_robust\")\n",
    "\n",
    "CONFIGS = {\n",
    "    \"ZAF_m02_private_robust\": dict(\n",
    "        country=\"South Africa\", month=2, signal=\"wet_dry_diff\", agg=\"ma\", w=269, shift=46, transform=\"signlog1p\"\n",
    "    ),\n",
    "    \"IND_m12_public_robust_tradeoff\": dict(\n",
    "        country=\"India\", month=12, signal=\"wet_high_wmean\", agg=\"ma\", w=1082, shift=60, transform=\"square\"\n",
    "    ),\n",
    "}\n",
    "if CONFIG_KEY not in CONFIGS:\n",
    "    raise ValueError(f\"Unknown CONFIG_KEY={CONFIG_KEY}. Options: {list(CONFIGS.keys())}\")\n",
    "cfg = CONFIGS[CONFIG_KEY]\n",
    "\n",
    "# --- Paths ---\n",
    "COMP_DIR = \"/kaggle/input/forecasting-the-future-the-helios-corn-climate-challenge\"\n",
    "MAIN_CSV = f\"{COMP_DIR}/corn_climate_risk_futures_daily_master.csv\"\n",
    "SHARE_CSV = f\"{COMP_DIR}/corn_regional_market_share.csv\"\n",
    "OUT_PATH = \"/kaggle/working/submission.csv\"\n",
    "\n",
    "# --- Minimal helpers ---\n",
    "def shift_array(x: np.ndarray, shift: int) -> np.ndarray:\n",
    "    x = x.astype(np.float64, copy=False)\n",
    "    n = x.shape[0]\n",
    "    out = np.full((n,), np.nan, dtype=np.float64)\n",
    "    if shift == 0:\n",
    "        out[:] = x\n",
    "        return out\n",
    "    if shift > 0:\n",
    "        out[shift:] = x[: n - shift]\n",
    "    else:\n",
    "        s = -shift\n",
    "        out[: n - s] = x[s:]\n",
    "    return out\n",
    "\n",
    "def rolling_mean_min1(x: np.ndarray, w: int) -> np.ndarray:\n",
    "    x = x.astype(np.float64, copy=False)\n",
    "    n = x.shape[0]\n",
    "    if w <= 1:\n",
    "        return x.copy()\n",
    "    out = np.full((n,), np.nan, dtype=np.float64)\n",
    "    xx = np.where(np.isnan(x), 0.0, x)\n",
    "    cs = np.zeros(n + 1, dtype=np.float64)\n",
    "    cs[1:] = np.cumsum(xx)\n",
    "    cnt = np.zeros(n + 1, dtype=np.int32)\n",
    "    cnt[1:] = np.cumsum(~np.isnan(x))\n",
    "    for i in range(n):\n",
    "        j0 = max(0, i - w + 1)\n",
    "        s = cs[i + 1] - cs[j0]\n",
    "        c = cnt[i + 1] - cnt[j0]\n",
    "        if c > 0:\n",
    "            out[i] = s / c\n",
    "    return out\n",
    "\n",
    "def apply_transform(x: np.ndarray, name: str) -> np.ndarray:\n",
    "    x = x.astype(np.float64, copy=False)\n",
    "    if name == \"square\":\n",
    "        return np.sign(x) * (x * x)\n",
    "    if name == \"signlog1p\":\n",
    "        return np.sign(x) * np.log1p(np.abs(x))\n",
    "    raise ValueError(f\"Unknown transform: {name}\")\n",
    "\n",
    "def make_feature_name(cfg: dict) -> str:\n",
    "    c3 = cfg[\"country\"][:3].lower()\n",
    "    mon = f\"m{int(cfg['month']):02d}\"\n",
    "    sig = cfg[\"signal\"]\n",
    "    agg = cfg[\"agg\"].replace(\".\", \"p\")\n",
    "    w = int(cfg[\"w\"])\n",
    "    sh = int(cfg[\"shift\"])\n",
    "    if sh < 0:\n",
    "        shs = f\"lead{-sh}\"\n",
    "    elif sh > 0:\n",
    "        shs = f\"lag{sh}\"\n",
    "    else:\n",
    "        shs = \"shift0\"\n",
    "    tr = cfg[\"transform\"].replace(\".\", \"p\")\n",
    "    name = f\"climate_risk_{c3}_{mon}_{sig}_{agg}_w{w}_{shs}_{tr}\"\n",
    "    return name.replace(\"-\", \"m\").replace(\" \", \"_\")\n",
    "\n",
    "FEATURE_COL = make_feature_name(cfg)\n",
    "\n",
    "# --- Load data ---\n",
    "df = pd.read_csv(MAIN_CSV)\n",
    "df[\"date_on\"] = pd.to_datetime(df[\"date_on\"], errors=\"coerce\")\n",
    "market_share_df = pd.read_csv(SHARE_CSV)\n",
    "\n",
    "# --- Reproduce sample submission engineering to get the exact dropna() row-set ---\n",
    "merged_daily_df = df.copy()\n",
    "merged_daily_df[\"day_of_year\"] = merged_daily_df[\"date_on\"].dt.dayofyear\n",
    "merged_daily_df[\"quarter\"] = merged_daily_df[\"date_on\"].dt.quarter\n",
    "\n",
    "merged_daily_df = merged_daily_df.merge(\n",
    "    market_share_df[[\"region_id\", \"percent_country_production\"]],\n",
    "    on=\"region_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "merged_daily_df[\"percent_country_production\"] = merged_daily_df[\"percent_country_production\"].fillna(1.0)\n",
    "\n",
    "risk_categories = [\"heat_stress\", \"unseasonably_cold\", \"excess_precip\", \"drought\"]\n",
    "for risk_type in risk_categories:\n",
    "    low_col = f\"climate_risk_cnt_locations_{risk_type}_risk_low\"\n",
    "    med_col = f\"climate_risk_cnt_locations_{risk_type}_risk_medium\"\n",
    "    high_col = f\"climate_risk_cnt_locations_{risk_type}_risk_high\"\n",
    "\n",
    "    total_locations = merged_daily_df[low_col] + merged_daily_df[med_col] + merged_daily_df[high_col]\n",
    "    risk_score = (merged_daily_df[med_col] + 2.0 * merged_daily_df[high_col]) / (total_locations + 1e-6)\n",
    "    weighted_risk = risk_score * (merged_daily_df[\"percent_country_production\"] / 100.0)\n",
    "\n",
    "    merged_daily_df[f\"climate_risk_{risk_type}_score\"] = risk_score\n",
    "    merged_daily_df[f\"climate_risk_{risk_type}_weighted\"] = weighted_risk\n",
    "\n",
    "temperature_risks = [\"heat_stress\", \"unseasonably_cold\"]\n",
    "precipitation_risks = [\"excess_precip\", \"drought\"]\n",
    "temp_scores = [f\"climate_risk_{r}_score\" for r in temperature_risks]\n",
    "precip_scores = [f\"climate_risk_{r}_score\" for r in precipitation_risks]\n",
    "all_scores = [f\"climate_risk_{r}_score\" for r in risk_categories]\n",
    "\n",
    "merged_daily_df[\"climate_risk_temperature_stress\"] = merged_daily_df[temp_scores].max(axis=1)\n",
    "merged_daily_df[\"climate_risk_precipitation_stress\"] = merged_daily_df[precip_scores].max(axis=1)\n",
    "merged_daily_df[\"climate_risk_overall_stress\"] = merged_daily_df[all_scores].max(axis=1)\n",
    "merged_daily_df[\"climate_risk_combined_stress\"] = merged_daily_df[all_scores].mean(axis=1)\n",
    "\n",
    "merged_daily_df = merged_daily_df.sort_values([\"region_id\", \"date_on\"])\n",
    "\n",
    "windows = [7, 14, 30]\n",
    "for window in windows:\n",
    "    for risk_type in risk_categories:\n",
    "        score_col = f\"climate_risk_{risk_type}_score\"\n",
    "        merged_daily_df[f\"climate_risk_{risk_type}_ma_{window}d\"] = (\n",
    "            merged_daily_df.groupby(\"region_id\")[score_col]\n",
    "            .rolling(window=window, min_periods=1)\n",
    "            .mean()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        merged_daily_df[f\"climate_risk_{risk_type}_max_{window}d\"] = (\n",
    "            merged_daily_df.groupby(\"region_id\")[score_col]\n",
    "            .rolling(window=window, min_periods=1)\n",
    "            .max()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "for risk_type in risk_categories:\n",
    "    score_col = f\"climate_risk_{risk_type}_score\"\n",
    "    merged_daily_df[f\"climate_risk_{risk_type}_change_1d\"] = merged_daily_df.groupby(\"region_id\")[score_col].diff(1)\n",
    "    merged_daily_df[f\"climate_risk_{risk_type}_change_7d\"] = merged_daily_df.groupby(\"region_id\")[score_col].diff(7)\n",
    "    merged_daily_df[f\"climate_risk_{risk_type}_acceleration\"] = (\n",
    "        merged_daily_df.groupby(\"region_id\")[f\"climate_risk_{risk_type}_change_1d\"].diff(1)\n",
    "    )\n",
    "\n",
    "for risk_type in risk_categories:\n",
    "    score_col = f\"climate_risk_{risk_type}_score\"\n",
    "    weighted_col = f\"climate_risk_{risk_type}_weighted\"\n",
    "\n",
    "    country_agg = (\n",
    "        merged_daily_df.groupby([\"country_name\", \"date_on\"]).agg(\n",
    "            {\n",
    "                score_col: [\"mean\", \"max\", \"std\"],\n",
    "                weighted_col: \"sum\",\n",
    "                \"percent_country_production\": \"sum\",\n",
    "            }\n",
    "        ).round(4)\n",
    "    )\n",
    "    country_agg.columns = [f\"country_{risk_type}_{'_'.join(col).strip()}\" for col in country_agg.columns]\n",
    "    country_agg = country_agg.reset_index()\n",
    "    merged_daily_df = merged_daily_df.merge(country_agg, on=[\"country_name\", \"date_on\"], how=\"left\")\n",
    "\n",
    "base = merged_daily_df.dropna().copy()\n",
    "\n",
    "# --- Feature (only signals needed by the 2 configs) ---\n",
    "df_share = market_share_df[[\"region_id\", \"percent_country_production\"]].copy()\n",
    "regions = df[[\"region_id\", \"country_name\"]].drop_duplicates().merge(df_share, on=\"region_id\", how=\"left\")\n",
    "w_raw = regions[\"percent_country_production\"].astype(np.float64).fillna(1.0).to_numpy()\n",
    "w_raw = np.where(w_raw <= 0, 1.0, w_raw)\n",
    "regions[\"w_raw\"] = w_raw\n",
    "regions[\"prod_w\"] = regions[\"w_raw\"] / regions.groupby(\"country_name\")[\"w_raw\"].transform(\"sum\")\n",
    "w_map = regions.set_index(\"region_id\")[\"prod_w\"]\n",
    "\n",
    "C_WET_L = \"climate_risk_cnt_locations_excess_precip_risk_low\"\n",
    "C_WET_M = \"climate_risk_cnt_locations_excess_precip_risk_medium\"\n",
    "C_WET_H = \"climate_risk_cnt_locations_excess_precip_risk_high\"\n",
    "C_DRY_L = \"climate_risk_cnt_locations_drought_risk_low\"\n",
    "C_DRY_M = \"climate_risk_cnt_locations_drought_risk_medium\"\n",
    "C_DRY_H = \"climate_risk_cnt_locations_drought_risk_high\"\n",
    "\n",
    "df_feat = df[\n",
    "    [\n",
    "        \"date_on\",\n",
    "        \"country_name\",\n",
    "        \"region_id\",\n",
    "        C_WET_L,\n",
    "        C_WET_M,\n",
    "        C_WET_H,\n",
    "        C_DRY_L,\n",
    "        C_DRY_M,\n",
    "        C_DRY_H,\n",
    "    ]\n",
    "].copy()\n",
    "df_feat[\"prod_w\"] = df_feat[\"region_id\"].map(w_map).fillna(0.0).astype(np.float64)\n",
    "\n",
    "eps = 1e-6\n",
    "def sev_and_high(low, med, high):\n",
    "    tot = low + med + high\n",
    "    sev = (med + 2.0 * high) / (tot + eps)\n",
    "    hi = high / (tot + eps)\n",
    "    return sev, hi\n",
    "\n",
    "wet_sev, wet_high = sev_and_high(\n",
    "    df_feat[C_WET_L].to_numpy(np.float64),\n",
    "    df_feat[C_WET_M].to_numpy(np.float64),\n",
    "    df_feat[C_WET_H].to_numpy(np.float64),\n",
    ")\n",
    "dry_sev, _ = sev_and_high(\n",
    "    df_feat[C_DRY_L].to_numpy(np.float64),\n",
    "    df_feat[C_DRY_M].to_numpy(np.float64),\n",
    "    df_feat[C_DRY_H].to_numpy(np.float64),\n",
    ")\n",
    "\n",
    "tmp = df_feat[[\"country_name\", \"date_on\", \"prod_w\"]].copy()\n",
    "tmp[\"wet_sev\"] = wet_sev\n",
    "tmp[\"dry_sev\"] = dry_sev\n",
    "tmp[\"wet_high\"] = wet_high\n",
    "\n",
    "tmp[\"w_wet_sev\"] = tmp[\"prod_w\"] * tmp[\"wet_sev\"]\n",
    "tmp[\"w_dry_sev\"] = tmp[\"prod_w\"] * tmp[\"dry_sev\"]\n",
    "tmp[\"w_wet_high\"] = tmp[\"prod_w\"] * tmp[\"wet_high\"]\n",
    "\n",
    "cd = (\n",
    "    tmp.groupby([\"country_name\", \"date_on\"], sort=False)[[\"prod_w\", \"w_wet_sev\", \"w_dry_sev\", \"w_wet_high\"]]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "den = cd[\"prod_w\"].to_numpy(np.float64)\n",
    "cd[\"wet_sev_wmean\"] = cd[\"w_wet_sev\"].to_numpy(np.float64) / den\n",
    "cd[\"dry_sev_wmean\"] = cd[\"w_dry_sev\"].to_numpy(np.float64) / den\n",
    "cd[\"wet_high_wmean\"] = cd[\"w_wet_high\"].to_numpy(np.float64) / den\n",
    "cd[\"wet_dry_diff\"] = cd[\"wet_sev_wmean\"] - cd[\"dry_sev_wmean\"]\n",
    "\n",
    "country = cfg[\"country\"]\n",
    "month = int(cfg[\"month\"])\n",
    "signal = cfg[\"signal\"]\n",
    "w = int(cfg[\"w\"])\n",
    "shift = int(cfg[\"shift\"])\n",
    "transform = cfg[\"transform\"]\n",
    "\n",
    "if cfg[\"agg\"] != \"ma\":\n",
    "    raise ValueError(f\"Unsupported agg for these configs: {cfg['agg']}\")\n",
    "\n",
    "cd_c = cd.loc[cd[\"country_name\"] == country].sort_values(\"date_on\").reset_index(drop=True)\n",
    "if cd_c.empty:\n",
    "    raise RuntimeError(f\"No rows found for country={country}\")\n",
    "if signal not in cd_c.columns:\n",
    "    raise RuntimeError(f\"Signal '{signal}' not found. Available: {list(cd_c.columns)}\")\n",
    "\n",
    "x_full = cd_c[signal].to_numpy(np.float64)\n",
    "x_shift = shift_array(x_full, shift)\n",
    "x_agg = rolling_mean_min1(x_shift, w)\n",
    "x_final = apply_transform(x_agg, transform)\n",
    "\n",
    "s_feat = pd.Series(x_final, index=cd_c[\"date_on\"]).replace([np.inf, -np.inf], np.nan)\n",
    "s_feat = s_feat.ffill().bfill().fillna(0.0)\n",
    "\n",
    "base[FEATURE_COL] = 0.0\n",
    "mask_country = base[\"country_name\"] == country\n",
    "base.loc[mask_country, FEATURE_COL] = (\n",
    "    base.loc[mask_country, \"date_on\"].map(s_feat).fillna(0.0).astype(np.float64)\n",
    ")\n",
    "\n",
    "mask_gate = mask_country & (base[\"date_on\"].dt.month == month)\n",
    "base.loc[~mask_gate, FEATURE_COL] = 0.0\n",
    "\n",
    "# --- Submission: keep futures_* and only this one climate_risk_* feature ---\n",
    "futures_cols = [c for c in base.columns if c.startswith(\"futures_\")]\n",
    "if not futures_cols:\n",
    "    raise RuntimeError(\"No futures_* columns found in base; evaluator will fail.\")\n",
    "\n",
    "meta_cols = [c for c in base.columns if not c.startswith(\"climate_risk_\") and not c.startswith(\"futures_\")]\n",
    "out = base[meta_cols + futures_cols + [FEATURE_COL]].copy()\n",
    "\n",
    "out[\"date_on\"] = pd.to_datetime(out[\"date_on\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "clim_cols = [c for c in out.columns if c.startswith(\"climate_risk_\")]\n",
    "if clim_cols != [FEATURE_COL]:\n",
    "    raise RuntimeError(f\"Unexpected climate columns present: {clim_cols}\")\n",
    "if out.isna().any().any():\n",
    "    raise RuntimeError(\"Submission still contains NaNs. Something went wrong (should not happen with base.dropna()).\")\n",
    "\n",
    "out.to_csv(OUT_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f7c07",
   "metadata": {
    "papermill": {
     "duration": 0.00315,
     "end_time": "2026-01-29T09:36:05.567570",
     "exception": false,
     "start_time": "2026-01-29T09:36:05.564420",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The above submission was created by running the following command on a local system (it takes a long time), then selecting from the recommendations by picking ZAF_m02_private_robust and IND_m12_public_robust_tradeoff (as per the notebook above) in order to avoid data leakage because they had nonnegative shifts:\n",
    "<pre>\n",
    "# To prevent BLAS thread oversubscription with joblib processes\n",
    "$env:OMP_NUM_THREADS=\"1\"\n",
    "$env:MKL_NUM_THREADS=\"1\"\n",
    "$env:OPENBLAS_NUM_THREADS=\"1\"\n",
    "$env:NUMEXPR_NUM_THREADS=\"1\"\n",
    "\n",
    "python -u .\\helios_script.py `\n",
    "  --main_csv \".\\forecasting-the-future-the-helios-corn-climate-challenge\\corn_climate_risk_futures_daily_master.csv\" `\n",
    "  --share_csv \".\\forecasting-the-future-the-helios-corn-climate-challenge\\corn_regional_market_share.csv\" `\n",
    "  --out_dir \".\\sweep_out_full_allgroups_public_and_robust\" `\n",
    "  --n_jobs 24 `\n",
    "  --time_budget_hours 40 `\n",
    "  --resume `\n",
    "  --rebuild_cache `\n",
    "  --val_years_list \"2,3,4,5\" `\n",
    "  --min_rows 60 `\n",
    "  --stage0_keep_per_country_month 18 `\n",
    "  --deep_top_groups 0 `\n",
    "  --deep_min_proxy 0 `\n",
    "  --shift_min -60 `\n",
    "  --shift_max 60 `\n",
    "  --coarse_shifts \"0,-60,-30,30,60\" `\n",
    "  --w_min 2 `\n",
    "  --w_max 2500 `\n",
    "  --aggs \"ma,max,ewm,std,streakq85,streakthr0.5\" `\n",
    "  --transforms \"identity,square,signlog1p\" `\n",
    "  --top_windows_keep 60 `\n",
    "  --top_pairs_keep 80 `\n",
    "  --refine_w_radius 80 `\n",
    "  --refine_shift_radius 45 `\n",
    "  --save_top 500 `\n",
    "  --public_vy 2 `\n",
    "  --save_top_public 200\n",
    "</pre>\n",
    "\n",
    "And this is the helios_script.py that was ran locally:\n",
    "<pre>\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "What this script does\n",
    "---------------------\n",
    "1) Reconstructs the sample_submission-style engineered dataframe and uses `.dropna()` to\n",
    "   reproduce the Kaggle evaluation row-set.\n",
    "\n",
    "2) Builds country-day climate signals using the exact weighting scheme from the sample submission\n",
    "(production-share normalization by country, with missing shares filled as 1.0).\n",
    "\n",
    "3) Scores *single-feature gated submissions*:\n",
    "   - One climate_risk_* feature at a time\n",
    "   - Non-zero only inside one (country, month) bucket\n",
    "   - Broadcast across all region rows for that country-date\n",
    "   This prevents CFCS dilution.\n",
    "\n",
    "4) Runs a 2-stage sweep:\n",
    "   Stage 0 (baseline): scan ALL (country, month, signal) quickly to rank promising groups.\n",
    "   Stage 1 (deep): refine only the top groups with pruned search + local refinements.\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "(out_dir)\n",
    "- cache/ base_cache_*.npz, cd_cache_*.npz\n",
    "- baseline_scan_all.csv\n",
    "- grids/ grid_<COUNTRY>_mMM_<SIGNAL>.csv.gz\n",
    "- grid_all_candidates.csv\n",
    "- recommendations.txt\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "# ---- Threading hygiene (avoid oversubscription when using many processes) ----\n",
    "import os as _os\n",
    "_os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "_os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "_os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "_os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "import argparse\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from joblib import Parallel, delayed\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"joblib is required. Install via: pip install joblib\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Logging\n",
    "# =============================================================================\n",
    "\n",
    "def now_str() -> str:\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "def log(msg: str) -> None:\n",
    "    print(f\"[{now_str()}] {msg}\", flush=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CFCS scoring (matches sample submission logic)\n",
    "# =============================================================================\n",
    "\n",
    "def _pearson_corr_nan(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    y = np.asarray(y, dtype=np.float64)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    if m.sum() < 3:\n",
    "        return float(\"nan\")\n",
    "    xv = x[m]; yv = y[m]\n",
    "    sx = xv.std(ddof=1); sy = yv.std(ddof=1)\n",
    "    if sx <= 0 or sy <= 0:\n",
    "        return float(\"nan\")\n",
    "    return float(np.corrcoef(xv, yv)[0, 1])\n",
    "\n",
    "def corr_vector(z: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Correlation between z (n,) and each column of Y (n,k), with NaN handling.\n",
    "    \"\"\"\n",
    "    z = np.asarray(z, dtype=np.float64)\n",
    "    Y = np.asarray(Y, dtype=np.float64)\n",
    "\n",
    "    # Fast path if fully finite\n",
    "    if np.isfinite(z).all() and np.isfinite(Y).all():\n",
    "        z0 = z - z.mean()\n",
    "        denom_z = np.sqrt(np.dot(z0, z0))\n",
    "        if denom_z <= 0:\n",
    "            return np.full((Y.shape[1],), np.nan, dtype=np.float64)\n",
    "        Y0 = Y - Y.mean(axis=0, keepdims=True)\n",
    "        denom_y = np.sqrt((Y0 * Y0).sum(axis=0))\n",
    "        denom = denom_z * denom_y\n",
    "        out = (z0[:, None] * Y0).sum(axis=0) / np.where(denom > 0, denom, np.nan)\n",
    "        return out.astype(np.float64)\n",
    "\n",
    "    out = np.full((Y.shape[1],), np.nan, dtype=np.float64)\n",
    "    for j in range(Y.shape[1]):\n",
    "        out[j] = _pearson_corr_nan(z, Y[:, j])\n",
    "    return out\n",
    "\n",
    "def cfcs_from_corrs(corrs_1d: np.ndarray) -> Tuple[float, Dict[str, Any]]:\n",
    "    s = pd.Series(np.asarray(corrs_1d, dtype=np.float64)).dropna()\n",
    "    if s.empty:\n",
    "        meta = dict(\n",
    "            cfcs_score=0.0,\n",
    "            avg_significant_correlation=0.0,\n",
    "            max_abs_correlation=0.0,\n",
    "            significant_correlations_pct=0.0,\n",
    "            total_correlations=0,\n",
    "            significant_correlations=0,\n",
    "        )\n",
    "        return 0.0, meta\n",
    "\n",
    "    # Kaggle rounds correlations before thresholding\n",
    "    s = s.round(5)\n",
    "    abs_corrs = s.abs()\n",
    "    sig = abs_corrs[abs_corrs >= 0.5]\n",
    "    sig_count = int(sig.shape[0])\n",
    "    total = int(abs_corrs.shape[0])\n",
    "\n",
    "    if sig_count > 0:\n",
    "        avg_sig = float(sig.mean())\n",
    "        avg_sig_score = min(100.0, avg_sig * 100.0)\n",
    "    else:\n",
    "        avg_sig = 0.0\n",
    "        avg_sig_score = 0.0\n",
    "\n",
    "    max_abs = float(abs_corrs.max())\n",
    "    max_score = min(100.0, max_abs * 100.0)\n",
    "\n",
    "    sig_pct = (sig_count / total) * 100.0 if total else 0.0\n",
    "    cfcs = (0.5 * avg_sig_score) + (0.3 * max_score) + (0.2 * sig_pct)\n",
    "\n",
    "    meta = dict(\n",
    "        cfcs_score=float(round(cfcs, 2)),\n",
    "        avg_significant_correlation=float(round(avg_sig, 4)),\n",
    "        max_abs_correlation=float(round(max_abs, 4)),\n",
    "        significant_correlations_pct=float(round(sig_pct, 2)),\n",
    "        total_correlations=total,\n",
    "        significant_correlations=sig_count,\n",
    "    )\n",
    "    return float(round(cfcs, 2)), meta\n",
    "\n",
    "def cfcs_score(z: np.ndarray, Y: np.ndarray) -> Tuple[float, Dict[str, Any]]:\n",
    "    return cfcs_from_corrs(corr_vector(z, Y))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Rolling + transforms \n",
    "# =============================================================================\n",
    "\n",
    "def shift_array(x: np.ndarray, shift: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    pandas-like shift:\n",
    "      out[t] = x[t - shift]\n",
    "    shift > 0 => lag (uses past)\n",
    "    shift < 0 => lead (uses future)\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    n = x.shape[0]\n",
    "    out = np.full((n,), np.nan, dtype=np.float64)\n",
    "    if shift == 0:\n",
    "        out[:] = x\n",
    "        return out\n",
    "    if shift > 0:\n",
    "        out[shift:] = x[: n - shift]\n",
    "    else:\n",
    "        s = -shift\n",
    "        out[: n - s] = x[s:]\n",
    "    return out\n",
    "\n",
    "def ffill_bfill_0(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Replicate pandas: s.ffill().bfill().fillna(0) on a 1D float array.\"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64).copy()\n",
    "    n = x.shape[0]\n",
    "    prev = np.nan\n",
    "    for i in range(n):\n",
    "        if not np.isfinite(x[i]):\n",
    "            x[i] = prev\n",
    "        else:\n",
    "            prev = x[i]\n",
    "    nxt = np.nan\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        if not np.isfinite(x[i]):\n",
    "            x[i] = nxt\n",
    "        else:\n",
    "            nxt = x[i]\n",
    "    x[~np.isfinite(x)] = 0.0\n",
    "    return x\n",
    "\n",
    "def rolling_mean_min1(x: np.ndarray, w: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    n = x.shape[0]\n",
    "    if w <= 1:\n",
    "        return x.copy()\n",
    "    out = np.full((n,), np.nan, dtype=np.float64)\n",
    "    xx = np.where(np.isfinite(x), x, 0.0)\n",
    "    cs = np.zeros(n + 1, dtype=np.float64)\n",
    "    cs[1:] = np.cumsum(xx)\n",
    "    cnt = np.zeros(n + 1, dtype=np.int32)\n",
    "    cnt[1:] = np.cumsum(np.isfinite(x).astype(np.int32))\n",
    "    for i in range(n):\n",
    "        j0 = max(0, i - w + 1)\n",
    "        s = cs[i + 1] - cs[j0]\n",
    "        c = cnt[i + 1] - cnt[j0]\n",
    "        if c > 0:\n",
    "            out[i] = s / c\n",
    "    return out\n",
    "\n",
    "def rolling_std_min1(x: np.ndarray, w: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    n = x.shape[0]\n",
    "    if w <= 1:\n",
    "        return np.zeros_like(x, dtype=np.float64)\n",
    "    out = np.full((n,), np.nan, dtype=np.float64)\n",
    "    xx = np.where(np.isfinite(x), x, 0.0)\n",
    "    cs = np.zeros(n + 1, dtype=np.float64)\n",
    "    cs2 = np.zeros(n + 1, dtype=np.float64)\n",
    "    cs[1:] = np.cumsum(xx)\n",
    "    cs2[1:] = np.cumsum(xx * xx)\n",
    "    cnt = np.zeros(n + 1, dtype=np.int32)\n",
    "    cnt[1:] = np.cumsum(np.isfinite(x).astype(np.int32))\n",
    "    for i in range(n):\n",
    "        j0 = max(0, i - w + 1)\n",
    "        s = cs[i + 1] - cs[j0]\n",
    "        s2 = cs2[i + 1] - cs2[j0]\n",
    "        c = cnt[i + 1] - cnt[j0]\n",
    "        if c > 1:\n",
    "            mean = s / c\n",
    "            var = max(0.0, (s2 / c) - (mean * mean))\n",
    "            out[i] = math.sqrt(var)\n",
    "        elif c == 1:\n",
    "            out[i] = 0.0\n",
    "    return out\n",
    "\n",
    "def rolling_max_min1(x: np.ndarray, w: int) -> np.ndarray:\n",
    "    from collections import deque\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    n = x.shape[0]\n",
    "    if w <= 1:\n",
    "        return x.copy()\n",
    "    out = np.full((n,), np.nan, dtype=np.float64)\n",
    "    dq: \"deque[int]\" = deque()\n",
    "    for i in range(n):\n",
    "        j0 = i - w + 1\n",
    "        while dq and dq[0] < j0:\n",
    "            dq.popleft()\n",
    "        xi = x[i]\n",
    "        if np.isfinite(xi):\n",
    "            while dq and x[dq[-1]] <= xi:\n",
    "                dq.pop()\n",
    "            dq.append(i)\n",
    "        if dq:\n",
    "            out[i] = x[dq[0]]\n",
    "    return out\n",
    "\n",
    "def ewm_mean(x: np.ndarray, span: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    n = x.shape[0]\n",
    "    if span <= 1:\n",
    "        return x.copy()\n",
    "    out = np.full((n,), np.nan, dtype=np.float64)\n",
    "    alpha = 2.0 / (span + 1.0)\n",
    "    prev = np.nan\n",
    "    for i in range(n):\n",
    "        xi = x[i]\n",
    "        if not np.isfinite(xi):\n",
    "            out[i] = prev\n",
    "            continue\n",
    "        if not np.isfinite(prev):\n",
    "            prev = xi\n",
    "        else:\n",
    "            prev = (1.0 - alpha) * prev + alpha * xi\n",
    "        out[i] = prev\n",
    "    return out\n",
    "\n",
    "def streak_fraction(x: np.ndarray, w: int, thr: float) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    n = x.shape[0]\n",
    "    if w <= 1:\n",
    "        b = (x >= thr).astype(np.float64)\n",
    "        b[~np.isfinite(x)] = np.nan\n",
    "        return b\n",
    "    out = np.full((n,), np.nan, dtype=np.float64)\n",
    "    b = (x >= thr).astype(np.float64)\n",
    "    b[~np.isfinite(x)] = np.nan\n",
    "    b0 = np.where(np.isfinite(b), b, 0.0)\n",
    "    cs = np.zeros(n + 1, dtype=np.float64)\n",
    "    cs[1:] = np.cumsum(b0)\n",
    "    cnt = np.zeros(n + 1, dtype=np.int32)\n",
    "    cnt[1:] = np.cumsum(np.isfinite(b).astype(np.int32))\n",
    "    for i in range(n):\n",
    "        j0 = max(0, i - w + 1)\n",
    "        s = cs[i + 1] - cs[j0]\n",
    "        c = cnt[i + 1] - cnt[j0]\n",
    "        if c > 0:\n",
    "            out[i] = s / c\n",
    "    return out\n",
    "\n",
    "def runlen_current(x: np.ndarray, thr: float) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    n = x.shape[0]\n",
    "    out = np.zeros((n,), dtype=np.float64)\n",
    "    cur = 0\n",
    "    for i in range(n):\n",
    "        xi = x[i]\n",
    "        if (not np.isfinite(xi)) or (xi < thr):\n",
    "            cur = 0\n",
    "        else:\n",
    "            cur += 1\n",
    "        out[i] = float(cur)\n",
    "    return out\n",
    "\n",
    "def apply_transform(x: np.ndarray, name: str) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    name = (name or \"identity\").lower()\n",
    "    if name == \"identity\":\n",
    "        return x\n",
    "    if name == \"square\":\n",
    "        # signed square (matches submission.py)\n",
    "        return np.sign(x) * (x * x)\n",
    "    if name == \"signlog1p\":\n",
    "        return np.sign(x) * np.log1p(np.abs(x))\n",
    "    if name == \"abs\":\n",
    "        return np.abs(x)\n",
    "    if name == \"tanh\":\n",
    "        return np.tanh(x)\n",
    "    raise ValueError(f\"Unknown transform: {name}\")\n",
    "\n",
    "def compute_agg_no_shift(x_full: np.ndarray, agg: str, w: int, *, q_for_streakq: float = 0.85) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute aggregator on unshifted timeline; shift output later (translation invariance).\n",
    "    \"\"\"\n",
    "    agg_l = (agg or \"ma\").lower().strip()\n",
    "    w = int(max(1, w))\n",
    "    if agg_l in (\"identity\", \"raw\"):\n",
    "        return np.asarray(x_full, dtype=np.float64).copy()\n",
    "    if agg_l == \"ma\":\n",
    "        return rolling_mean_min1(x_full, w)\n",
    "    if agg_l == \"std\":\n",
    "        return rolling_std_min1(x_full, w)\n",
    "    if agg_l == \"max\":\n",
    "        return rolling_max_min1(x_full, w)\n",
    "    if agg_l == \"ewm\":\n",
    "        return ewm_mean(x_full, span=max(2, w))\n",
    "    if agg_l.startswith(\"streakq\"):\n",
    "        q = q_for_streakq\n",
    "        tail = agg_l.replace(\"streakq\", \"\")\n",
    "        if tail:\n",
    "            try:\n",
    "                q = float(tail) / 100.0\n",
    "            except Exception:\n",
    "                q = q_for_streakq\n",
    "        q = float(np.clip(q, 0.5, 0.99))\n",
    "        xf = np.asarray(x_full, dtype=np.float64)\n",
    "        m = np.isfinite(xf)\n",
    "        thr = float(np.nanquantile(xf[m], q)) if m.any() else 0.0\n",
    "        if not np.isfinite(thr):\n",
    "            thr = 0.0\n",
    "        return streak_fraction(x_full, w, thr)\n",
    "    if agg_l.startswith(\"streakthr\"):\n",
    "        tail = agg_l.replace(\"streakthr\", \"\")\n",
    "        thr = float(tail) if tail else 0.5\n",
    "        return streak_fraction(x_full, w, thr)\n",
    "    if agg_l.startswith(\"runlenth\"):\n",
    "        tail = agg_l.replace(\"runlenth\", \"\")\n",
    "        thr = float(tail) if tail else 0.5\n",
    "        return runlen_current(x_full, thr)\n",
    "    raise ValueError(f\"Unknown agg: {agg}\")\n",
    "\n",
    "def compute_feature_series(\n",
    "    x_full: np.ndarray,\n",
    "    *,\n",
    "    agg: str,\n",
    "    w: int,\n",
    "    shift: int,\n",
    "    transform: str,\n",
    "    q_for_streakq: float,\n",
    ") -> np.ndarray:\n",
    "    base = compute_agg_no_shift(x_full, agg=agg, w=w, q_for_streakq=q_for_streakq)\n",
    "    z = apply_transform(shift_array(base, int(shift)), transform)\n",
    "    return ffill_bfill_0(z)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Row-set alignment\n",
    "# =============================================================================\n",
    "\n",
    "RISK_CATEGORIES = [\"heat_stress\", \"unseasonably_cold\", \"excess_precip\", \"drought\"]\n",
    "\n",
    "def build_kaggle_rowset_base(\n",
    "    main_df: pd.DataFrame,\n",
    "    share_df: pd.DataFrame,\n",
    "    *,\n",
    "    rolling_windows: Sequence[int] = (7, 14, 30),\n",
    ") -> pd.DataFrame:\n",
    "    df = main_df.copy()\n",
    "    df[\"date_on\"] = pd.to_datetime(df[\"date_on\"], errors=\"coerce\")\n",
    "\n",
    "    merged_daily_df = df.copy()\n",
    "    merged_daily_df[\"day_of_year\"] = merged_daily_df[\"date_on\"].dt.dayofyear\n",
    "    merged_daily_df[\"quarter\"] = merged_daily_df[\"date_on\"].dt.quarter\n",
    "\n",
    "    share_cols = share_df[[\"region_id\", \"percent_country_production\"]].copy()\n",
    "    merged_daily_df = merged_daily_df.merge(share_cols, on=\"region_id\", how=\"left\")\n",
    "    merged_daily_df[\"percent_country_production\"] = merged_daily_df[\"percent_country_production\"].fillna(1.0)\n",
    "\n",
    "    for risk_type in RISK_CATEGORIES:\n",
    "        low_col  = f\"climate_risk_cnt_locations_{risk_type}_risk_low\"\n",
    "        med_col  = f\"climate_risk_cnt_locations_{risk_type}_risk_medium\"\n",
    "        high_col = f\"climate_risk_cnt_locations_{risk_type}_risk_high\"\n",
    "        total_locations = merged_daily_df[low_col] + merged_daily_df[med_col] + merged_daily_df[high_col]\n",
    "        risk_score = (merged_daily_df[med_col] + 2.0 * merged_daily_df[high_col]) / (total_locations + 1e-6)\n",
    "        weighted_risk = risk_score * (merged_daily_df[\"percent_country_production\"] / 100.0)\n",
    "        merged_daily_df[f\"climate_risk_{risk_type}_score\"] = risk_score\n",
    "        merged_daily_df[f\"climate_risk_{risk_type}_weighted\"] = weighted_risk\n",
    "\n",
    "    temperature_risks = [\"heat_stress\", \"unseasonably_cold\"]\n",
    "    precipitation_risks = [\"excess_precip\", \"drought\"]\n",
    "    temp_scores = [f\"climate_risk_{r}_score\" for r in temperature_risks]\n",
    "    precip_scores = [f\"climate_risk_{r}_score\" for r in precipitation_risks]\n",
    "    all_scores = [f\"climate_risk_{r}_score\" for r in RISK_CATEGORIES]\n",
    "\n",
    "    merged_daily_df[\"climate_risk_temperature_stress\"] = merged_daily_df[temp_scores].max(axis=1)\n",
    "    merged_daily_df[\"climate_risk_precipitation_stress\"] = merged_daily_df[precip_scores].max(axis=1)\n",
    "    merged_daily_df[\"climate_risk_overall_stress\"] = merged_daily_df[all_scores].max(axis=1)\n",
    "    merged_daily_df[\"climate_risk_combined_stress\"] = merged_daily_df[all_scores].mean(axis=1)\n",
    "\n",
    "    merged_daily_df = merged_daily_df.sort_values([\"region_id\", \"date_on\"])\n",
    "\n",
    "    for window in rolling_windows:\n",
    "        for risk_type in RISK_CATEGORIES:\n",
    "            score_col = f\"climate_risk_{risk_type}_score\"\n",
    "            merged_daily_df[f\"climate_risk_{risk_type}_ma_{window}d\"] = (\n",
    "                merged_daily_df.groupby(\"region_id\")[score_col]\n",
    "                .rolling(window=window, min_periods=1)\n",
    "                .mean()\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "            merged_daily_df[f\"climate_risk_{risk_type}_max_{window}d\"] = (\n",
    "                merged_daily_df.groupby(\"region_id\")[score_col]\n",
    "                .rolling(window=window, min_periods=1)\n",
    "                .max()\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "\n",
    "    for risk_type in RISK_CATEGORIES:\n",
    "        score_col = f\"climate_risk_{risk_type}_score\"\n",
    "        merged_daily_df[f\"climate_risk_{risk_type}_change_1d\"] = merged_daily_df.groupby(\"region_id\")[score_col].diff(1)\n",
    "        merged_daily_df[f\"climate_risk_{risk_type}_change_7d\"] = merged_daily_df.groupby(\"region_id\")[score_col].diff(7)\n",
    "        merged_daily_df[f\"climate_risk_{risk_type}_acceleration\"] = (\n",
    "            merged_daily_df.groupby(\"region_id\")[f\"climate_risk_{risk_type}_change_1d\"].diff(1)\n",
    "        )\n",
    "\n",
    "    for risk_type in RISK_CATEGORIES:\n",
    "        score_col = f\"climate_risk_{risk_type}_score\"\n",
    "        weighted_col = f\"climate_risk_{risk_type}_weighted\"\n",
    "        country_agg = (\n",
    "            merged_daily_df.groupby([\"country_name\", \"date_on\"]).agg({\n",
    "                score_col: [\"mean\", \"max\", \"std\"],\n",
    "                weighted_col: \"sum\",\n",
    "                \"percent_country_production\": \"sum\",\n",
    "            }).round(4)\n",
    "        )\n",
    "        country_agg.columns = [f\"country_{risk_type}_{'_'.join(col).strip()}\" for col in country_agg.columns]\n",
    "        country_agg = country_agg.reset_index()\n",
    "        merged_daily_df = merged_daily_df.merge(country_agg, on=[\"country_name\", \"date_on\"], how=\"left\")\n",
    "\n",
    "    return merged_daily_df.dropna().copy()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Country-day signals \n",
    "# =============================================================================\n",
    "\n",
    "RISK_MAP = {\n",
    "    \"heat\": \"heat_stress\",\n",
    "    \"cold\": \"unseasonably_cold\",\n",
    "    \"wet\":  \"excess_precip\",\n",
    "    \"dry\":  \"drought\",\n",
    "}\n",
    "\n",
    "def build_country_day_signals(\n",
    "    main_df: pd.DataFrame,\n",
    "    share_df: pd.DataFrame,\n",
    "    *,\n",
    "    country_whitelist: Optional[Sequence[str]] = None,\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    df = main_df.copy()\n",
    "    df[\"date_on\"] = pd.to_datetime(df[\"date_on\"], errors=\"coerce\")\n",
    "\n",
    "    # region weights (submission.py style)\n",
    "    regions = df[[\"region_id\", \"country_name\"]].drop_duplicates().copy()\n",
    "    regions = regions.merge(share_df[[\"region_id\", \"percent_country_production\"]], on=\"region_id\", how=\"left\")\n",
    "    w_raw = regions[\"percent_country_production\"].astype(np.float64).fillna(1.0).to_numpy()\n",
    "    w_raw = np.where(w_raw <= 0, 1.0, w_raw)\n",
    "    regions[\"w_raw\"] = w_raw\n",
    "    regions[\"prod_w\"] = regions[\"w_raw\"] / regions.groupby(\"country_name\")[\"w_raw\"].transform(\"sum\")\n",
    "    if country_whitelist is not None:\n",
    "        regions = regions[regions[\"country_name\"].isin(list(country_whitelist))].copy()\n",
    "    w_map = regions.set_index(\"region_id\")[\"prod_w\"]\n",
    "\n",
    "    df[\"prod_w\"] = df[\"region_id\"].map(w_map).fillna(0.0).astype(np.float64)\n",
    "\n",
    "    eps = 1e-6\n",
    "    tmp = df[[\"country_name\", \"date_on\", \"prod_w\"]].copy()\n",
    "\n",
    "    signal_cols: List[str] = []\n",
    "    for short, kind in RISK_MAP.items():\n",
    "        low = f\"climate_risk_cnt_locations_{kind}_risk_low\"\n",
    "        med = f\"climate_risk_cnt_locations_{kind}_risk_medium\"\n",
    "        high = f\"climate_risk_cnt_locations_{kind}_risk_high\"\n",
    "        tot = (df[low] + df[med] + df[high]).astype(np.float64)\n",
    "\n",
    "        sev = (df[med].astype(np.float64) + 2.0 * df[high].astype(np.float64)) / (tot + eps)\n",
    "        hi  = df[high].astype(np.float64) / (tot + eps)\n",
    "        wapr = (df[med].astype(np.float64) + df[high].astype(np.float64)) / (tot + eps)\n",
    "\n",
    "        tmp[f\"w_{short}_sev\"] = tmp[\"prod_w\"] * sev\n",
    "        tmp[f\"w_{short}_high\"] = tmp[\"prod_w\"] * hi\n",
    "        tmp[f\"w_{short}_wapr\"] = tmp[\"prod_w\"] * wapr\n",
    "\n",
    "    sum_cols = [\"prod_w\"] + [c for c in tmp.columns if c.startswith(\"w_\")]\n",
    "    cd = tmp.groupby([\"country_name\", \"date_on\"], sort=False)[sum_cols].sum().reset_index()\n",
    "    den = cd[\"prod_w\"].to_numpy(np.float64) + 1e-12\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"country_name\": cd[\"country_name\"].values,\n",
    "        \"date_on\": cd[\"date_on\"].values,\n",
    "    })\n",
    "\n",
    "    for short in RISK_MAP.keys():\n",
    "        out[f\"{short}_sev_wmean\"]  = (cd[f\"w_{short}_sev\"].to_numpy(np.float64)  / den).astype(np.float32)\n",
    "        out[f\"{short}_high_wmean\"] = (cd[f\"w_{short}_high\"].to_numpy(np.float64) / den).astype(np.float32)\n",
    "        out[f\"{short}_wapr_wmean\"] = (cd[f\"w_{short}_wapr\"].to_numpy(np.float64) / den).astype(np.float32)\n",
    "        signal_cols.extend([f\"{short}_sev_wmean\", f\"{short}_high_wmean\", f\"{short}_wapr_wmean\"])\n",
    "\n",
    "    # Interactions (include what your earlier sweeps found useful)\n",
    "    out[\"heat_dry_prod\"] = (out[\"heat_sev_wmean\"] * out[\"dry_sev_wmean\"]).astype(np.float32)\n",
    "    out[\"wet_dry_diff\"]  = (out[\"wet_sev_wmean\"]  - out[\"dry_sev_wmean\"]).astype(np.float32)\n",
    "    out[\"temp_stress_max\"] = out[[\"heat_sev_wmean\", \"cold_sev_wmean\"]].max(axis=1).astype(np.float32)\n",
    "    out[\"precip_stress_max\"] = out[[\"wet_sev_wmean\", \"dry_sev_wmean\"]].max(axis=1).astype(np.float32)\n",
    "    out[\"overall_stress_max\"] = out[[\"heat_sev_wmean\",\"cold_sev_wmean\",\"wet_sev_wmean\",\"dry_sev_wmean\"]].max(axis=1).astype(np.float32)\n",
    "    out[\"overall_stress_mean\"] = out[[\"heat_sev_wmean\",\"cold_sev_wmean\",\"wet_sev_wmean\",\"dry_sev_wmean\"]].mean(axis=1).astype(np.float32)\n",
    "    signal_cols.extend([\"heat_dry_prod\",\"wet_dry_diff\",\"temp_stress_max\",\"precip_stress_max\",\"overall_stress_max\",\"overall_stress_mean\"])\n",
    "\n",
    "    out[\"date_on\"] = pd.to_datetime(out[\"date_on\"])\n",
    "    out[\"year\"] = out[\"date_on\"].dt.year.astype(np.int16)\n",
    "    out[\"month\"] = out[\"date_on\"].dt.month.astype(np.int8)\n",
    "    out = out.sort_values([\"country_name\", \"date_on\"]).reset_index(drop=True)\n",
    "    return out, signal_cols\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Helper: window grid (coarse)\n",
    "# =============================================================================\n",
    "\n",
    "def make_window_grid(w_min: int, w_max: int) -> List[int]:\n",
    "    w_min = int(max(1, w_min))\n",
    "    w_max = int(max(w_min, w_max))\n",
    "    ws: List[int] = []\n",
    "\n",
    "    def add_range(a: int, b: int, step: int) -> None:\n",
    "        for v in range(a, b + 1, step):\n",
    "            ws.append(v)\n",
    "\n",
    "    add_range(w_min, min(w_max, 120), 1)\n",
    "    if w_max > 120:\n",
    "        add_range(121, min(w_max, 400), 2)\n",
    "    if w_max > 400:\n",
    "        add_range(405, min(w_max, 800), 5)\n",
    "    if w_max > 800:\n",
    "        add_range(810, min(w_max, 1500), 10)\n",
    "    if w_max > 1500:\n",
    "        add_range(1525, min(w_max, 2500), 25)\n",
    "    if w_max > 2500:\n",
    "        add_range(2600, w_max, 50)\n",
    "\n",
    "    specials = [7, 14, 21, 30, 45, 60, 90, 98, 102, 112, 120, 140, 168, 180, 200, 224, 240, 252,\n",
    "                280, 300, 330, 365, 400, 450, 500, 540, 600, 730, 900, 1095, 1200, 1461, 1500, 1800, 2000, 2500]\n",
    "    for v in specials:\n",
    "        if w_min <= v <= w_max:\n",
    "            ws.append(int(v))\n",
    "\n",
    "    return sorted(set(ws))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Time split helpers\n",
    "# =============================================================================\n",
    "\n",
    "def last_n_years_subset(years: np.ndarray, n: int) -> Optional[np.ndarray]:\n",
    "    years = np.asarray(years, dtype=np.int16)\n",
    "    uniq = np.array(sorted(set(int(y) for y in years if np.isfinite(y))), dtype=np.int16)\n",
    "    if uniq.size < n:\n",
    "        return None\n",
    "    last = set(uniq[-n:].tolist())\n",
    "    return np.where(np.isin(years, list(last)))[0]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Cache format (npz): to keep worker args small\n",
    "# =============================================================================\n",
    "\n",
    "def date_to_int_days(dts: np.ndarray) -> np.ndarray:\n",
    "    dd = dts.astype(\"datetime64[D]\")\n",
    "    return dd.astype(np.int32)\n",
    "\n",
    "def load_or_build_caches(\n",
    "    *,\n",
    "    main_csv: str,\n",
    "    share_csv: str,\n",
    "    out_dir: str,\n",
    "    rebuild: bool,\n",
    "    kaggle_rowset: bool,\n",
    ") -> Tuple[Path, Path, Dict[str, Any]]:\n",
    "    out = Path(out_dir)\n",
    "    cache_dir = out / \"cache\"\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    base_npz = cache_dir / (\"base_cache_kaggle.npz\" if kaggle_rowset else \"base_cache_raw.npz\")\n",
    "    cd_npz = cache_dir / (\"cd_cache_kaggle.npz\" if kaggle_rowset else \"cd_cache_raw.npz\")\n",
    "    meta_json = cache_dir / (\"cache_meta_kaggle.json\" if kaggle_rowset else \"cache_meta_raw.json\")\n",
    "\n",
    "    if (not rebuild) and base_npz.exists() and cd_npz.exists() and meta_json.exists():\n",
    "        meta = json.loads(meta_json.read_text(encoding=\"utf-8\"))\n",
    "        return base_npz, cd_npz, meta\n",
    "\n",
    "    log(\"Loading CSVs to build cache...\")\n",
    "    df_main = pd.read_csv(main_csv)\n",
    "    df_share = pd.read_csv(share_csv)\n",
    "\n",
    "    futures_cols = [c for c in df_main.columns if c.startswith(\"futures_\")]\n",
    "    if not futures_cols:\n",
    "        raise RuntimeError(\"No futures_* columns found in main_csv.\")\n",
    "\n",
    "    if kaggle_rowset:\n",
    "        log(\"Building Kaggle-aligned base row-set (sample_submission-style + dropna)...\")\n",
    "        base = build_kaggle_rowset_base(df_main, df_share, rolling_windows=(7,14,30))\n",
    "        log(f\"Base rows after dropna: {len(base):,}\")\n",
    "    else:\n",
    "        log(\"Using RAW row-set (no sample_submission dropna).\")\n",
    "        base = df_main.copy()\n",
    "        base[\"date_on\"] = pd.to_datetime(base[\"date_on\"], errors=\"coerce\")\n",
    "        base = base.dropna(subset=[\"date_on\", \"country_name\"]).copy()\n",
    "\n",
    "    # Keep only columns needed for scoring\n",
    "    keep_cols = [\"date_on\", \"country_name\"] + futures_cols\n",
    "    base = base[keep_cols].copy()\n",
    "    base[\"date_on\"] = pd.to_datetime(base[\"date_on\"], errors=\"coerce\")\n",
    "    base = base.dropna(subset=[\"date_on\"]).copy()\n",
    "\n",
    "    countries = sorted(base[\"country_name\"].astype(str).unique().tolist())\n",
    "    cat = pd.Categorical(base[\"country_name\"].astype(str), categories=countries)\n",
    "    base_country_code = cat.codes.astype(np.int16)\n",
    "\n",
    "    base_date_int = date_to_int_days(base[\"date_on\"].to_numpy())\n",
    "    dt_series = pd.to_datetime(base[\"date_on\"])\n",
    "    base_month = dt_series.dt.month.astype(np.int8).to_numpy()\n",
    "    base_year = dt_series.dt.year.astype(np.int16).to_numpy()\n",
    "\n",
    "    Y = base[futures_cols].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "    log(\"Building country-day signals (submission.py weighting)...\")\n",
    "    cd_df, signal_cols = build_country_day_signals(df_main, df_share, country_whitelist=countries)\n",
    "\n",
    "    cd_cat = pd.Categorical(cd_df[\"country_name\"].astype(str), categories=countries)\n",
    "    cd_country_code = cd_cat.codes.astype(np.int16)\n",
    "    cd_date_int = date_to_int_days(cd_df[\"date_on\"].to_numpy())\n",
    "    cd_year = cd_df[\"year\"].to_numpy(dtype=np.int16, copy=False)\n",
    "    cd_month = cd_df[\"month\"].to_numpy(dtype=np.int8, copy=False)\n",
    "    Xsig = cd_df[signal_cols].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "    log(f\"Saving base cache -> {base_npz}\")\n",
    "    np.savez(\n",
    "        base_npz,\n",
    "        base_date_int=base_date_int.astype(np.int32),\n",
    "        base_country_code=base_country_code.astype(np.int16),\n",
    "        base_month=base_month.astype(np.int8),\n",
    "        base_year=base_year.astype(np.int16),\n",
    "        futures_cols=np.array(futures_cols, dtype=object),\n",
    "        Y=Y.astype(np.float32),\n",
    "        countries=np.array(countries, dtype=object),\n",
    "    )\n",
    "\n",
    "    log(f\"Saving country-day cache -> {cd_npz}\")\n",
    "    np.savez(\n",
    "        cd_npz,\n",
    "        cd_country_code=cd_country_code.astype(np.int16),\n",
    "        cd_date_int=cd_date_int.astype(np.int32),\n",
    "        cd_year=cd_year.astype(np.int16),\n",
    "        cd_month=cd_month.astype(np.int8),\n",
    "        signal_cols=np.array(signal_cols, dtype=object),\n",
    "        Xsig=Xsig.astype(np.float32),\n",
    "    )\n",
    "\n",
    "    meta = dict(\n",
    "        built_utc=time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "        kaggle_rowset=bool(kaggle_rowset),\n",
    "        n_base_rows=int(base.shape[0]),\n",
    "        n_countries=int(len(countries)),\n",
    "        n_futures_cols=int(len(futures_cols)),\n",
    "        n_cd_rows=int(cd_df.shape[0]),\n",
    "        n_signal_cols=int(len(signal_cols)),\n",
    "    )\n",
    "    meta_json.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "    return base_npz, cd_npz, meta\n",
    "\n",
    "\n",
    "def _load_npz(path: Path) -> Dict[str, Any]:\n",
    "    with np.load(path, allow_pickle=True) as z:\n",
    "        return {k: z[k] for k in z.files}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Fast date mapping helpers (precompute pos+ok per bucket)\n",
    "# =============================================================================\n",
    "\n",
    "def make_date_mapper(cd_dates: np.ndarray, bucket_dates: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    cd_dates = np.asarray(cd_dates, dtype=np.int32)\n",
    "    bucket_dates = np.asarray(bucket_dates, dtype=np.int32)\n",
    "    pos = np.searchsorted(cd_dates, bucket_dates)\n",
    "    ok = (pos >= 0) & (pos < cd_dates.size)\n",
    "    pos_clip = pos.clip(0, cd_dates.size - 1)\n",
    "    ok = ok & (cd_dates[pos_clip] == bucket_dates)\n",
    "    return pos.astype(np.int32), ok\n",
    "\n",
    "def map_with_mapper(z_full: np.ndarray, pos: np.ndarray, ok: np.ndarray) -> np.ndarray:\n",
    "    out = np.zeros(pos.shape[0], dtype=np.float64)\n",
    "    if ok.any():\n",
    "        out[ok] = z_full[pos[ok]]\n",
    "    return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Stage 0 baseline scan\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class BaselineHit:\n",
    "    country: str\n",
    "    month: int\n",
    "    signal: str\n",
    "    agg: str\n",
    "    transform: str\n",
    "    w: int\n",
    "    shift: int\n",
    "    proxy_cfcs: float\n",
    "    robust_min_cfcs: float\n",
    "    per_vy: Dict[int, float]\n",
    "    n_rows: int\n",
    "    n_years: int\n",
    "\n",
    "def score_on_bucket(\n",
    "    *,\n",
    "    z_full: np.ndarray,\n",
    "    mapper_pos: np.ndarray,\n",
    "    mapper_ok: np.ndarray,\n",
    "    bucket_years: np.ndarray,\n",
    "    Y_bucket: np.ndarray,\n",
    "    val_years_list: Sequence[int],\n",
    ") -> Tuple[float, float, Dict[int, float]]:\n",
    "    z = map_with_mapper(z_full, mapper_pos, mapper_ok)\n",
    "    proxy, _ = cfcs_score(z, Y_bucket)\n",
    "    per: Dict[int, float] = {}\n",
    "    vals = []\n",
    "    for vy in val_years_list:\n",
    "        idx = last_n_years_subset(bucket_years, int(vy))\n",
    "        if idx is None or idx.size < 30:\n",
    "            per[int(vy)] = float(\"nan\")\n",
    "            continue\n",
    "        s, _ = cfcs_score(z[idx], Y_bucket[idx])\n",
    "        per[int(vy)] = float(s)\n",
    "        if math.isfinite(s):\n",
    "            vals.append(float(s))\n",
    "    robust = float(min(vals)) if vals else 0.0\n",
    "    return float(proxy), float(robust), per\n",
    "\n",
    "def stage0_scan_country(\n",
    "    *,\n",
    "    base_npz: Path,\n",
    "    cd_npz: Path,\n",
    "    country_code: int,\n",
    "    stage0_windows: Sequence[int],\n",
    "    stage0_aggs: Sequence[str],\n",
    "    stage0_transforms: Sequence[str],\n",
    "    val_years_list: Sequence[int],\n",
    "    min_rows: int,\n",
    "    q_for_streakq: float,\n",
    "    time_budget_end: float,\n",
    ") -> List[BaselineHit]:\n",
    "    \"\"\"\n",
    "    For one country:\n",
    "      - precompute month buckets and their date mappers\n",
    "      - for each signal, precompute agg series for each (agg,w) ONCE\n",
    "      - score transforms quickly\n",
    "      - return best config per (month, signal)\n",
    "    \"\"\"\n",
    "    if time.time() > time_budget_end:\n",
    "        return []\n",
    "\n",
    "    base = _load_npz(base_npz)\n",
    "    cd = _load_npz(cd_npz)\n",
    "\n",
    "    countries = base[\"countries\"].tolist()\n",
    "    signal_cols = cd[\"signal_cols\"].tolist()\n",
    "\n",
    "    bc = base[\"base_country_code\"].astype(np.int16)\n",
    "    mask_country = (bc == int(country_code))\n",
    "    if not mask_country.any():\n",
    "        return []\n",
    "\n",
    "    base_dates_all = base[\"base_date_int\"].astype(np.int32)\n",
    "    base_month_all = base[\"base_month\"].astype(np.int8)\n",
    "    base_year_all = base[\"base_year\"].astype(np.int16)\n",
    "    Y_all = base[\"Y\"].astype(np.float32)\n",
    "\n",
    "    idx_country = np.where(mask_country)[0]\n",
    "    months_present = sorted(set(int(m) for m in base_month_all[idx_country].tolist()))\n",
    "\n",
    "    # Country-day slice\n",
    "    cdc = cd[\"cd_country_code\"].astype(np.int16)\n",
    "    idx_cd_country = np.where(cdc == int(country_code))[0]\n",
    "    if idx_cd_country.size == 0:\n",
    "        return []\n",
    "    cd_dates = cd[\"cd_date_int\"].astype(np.int32)[idx_cd_country]\n",
    "    order_cd = np.argsort(cd_dates, kind=\"mergesort\")\n",
    "    idx_cd_country = idx_cd_country[order_cd]\n",
    "    cd_dates = cd_dates[order_cd]\n",
    "    Xsig = cd[\"Xsig\"].astype(np.float32)[idx_cd_country]  # [n_days, n_signals]\n",
    "\n",
    "    # Precompute bucket data per month: indices, mapper, years, Y\n",
    "    buckets: Dict[int, Dict[str, Any]] = {}\n",
    "    for month in months_present:\n",
    "        idx_m = idx_country[base_month_all[idx_country] == int(month)]\n",
    "        if idx_m.size < int(min_rows):\n",
    "            continue\n",
    "        bucket_dates = base_dates_all[idx_m]\n",
    "        bucket_years = base_year_all[idx_m]\n",
    "        Y_bucket = Y_all[idx_m].astype(np.float64)\n",
    "        pos, ok = make_date_mapper(cd_dates, bucket_dates)\n",
    "        buckets[int(month)] = dict(\n",
    "            bucket_years=bucket_years,\n",
    "            Y_bucket=Y_bucket,\n",
    "            pos=pos,\n",
    "            ok=ok,\n",
    "            n_rows=int(idx_m.size),\n",
    "            n_years=len(set(int(y) for y in bucket_years.tolist())),\n",
    "        )\n",
    "    if not buckets:\n",
    "        return []\n",
    "\n",
    "    country_name = str(countries[int(country_code)])\n",
    "\n",
    "    hits: List[BaselineHit] = []\n",
    "    # For each signal and month: find best\n",
    "    for si, sig in enumerate(signal_cols):\n",
    "        if time.time() > time_budget_end:\n",
    "            break\n",
    "        x_full = Xsig[:, si].astype(np.float64, copy=False)\n",
    "\n",
    "        # cache agg series for this signal\n",
    "        agg_cache: Dict[Tuple[str, int], np.ndarray] = {}\n",
    "        def get_agg(agg: str, w: int) -> np.ndarray:\n",
    "            key = (agg, int(w))\n",
    "            if key in agg_cache:\n",
    "                return agg_cache[key]\n",
    "            s = compute_agg_no_shift(x_full, agg=agg, w=int(w), q_for_streakq=q_for_streakq)\n",
    "            agg_cache[key] = s.astype(np.float64, copy=False)\n",
    "            return agg_cache[key]\n",
    "\n",
    "        for month, b in buckets.items():\n",
    "            if time.time() > time_budget_end:\n",
    "                break\n",
    "\n",
    "            best: Optional[BaselineHit] = None\n",
    "            for agg in stage0_aggs:\n",
    "                # precompute agg series per window once\n",
    "                for w in stage0_windows:\n",
    "                    base_series = get_agg(agg, int(w))\n",
    "                    # shift is fixed 0 in stage0\n",
    "                    for transform in stage0_transforms:\n",
    "                        z_full = apply_transform(base_series, transform)\n",
    "                        z_full = ffill_bfill_0(z_full)\n",
    "                        proxy, robust, per = score_on_bucket(\n",
    "                            z_full=z_full,\n",
    "                            mapper_pos=b[\"pos\"],\n",
    "                            mapper_ok=b[\"ok\"],\n",
    "                            bucket_years=b[\"bucket_years\"],\n",
    "                            Y_bucket=b[\"Y_bucket\"],\n",
    "                            val_years_list=val_years_list,\n",
    "                        )\n",
    "                        cand = BaselineHit(\n",
    "                            country=country_name,\n",
    "                            month=int(month),\n",
    "                            signal=str(sig),\n",
    "                            agg=str(agg),\n",
    "                            transform=str(transform),\n",
    "                            w=int(w),\n",
    "                            shift=0,\n",
    "                            proxy_cfcs=float(proxy),\n",
    "                            robust_min_cfcs=float(robust),\n",
    "                            per_vy=per,\n",
    "                            n_rows=int(b[\"n_rows\"]),\n",
    "                            n_years=int(b[\"n_years\"]),\n",
    "                        )\n",
    "                        if (best is None) or (cand.proxy_cfcs > best.proxy_cfcs):\n",
    "                            best = cand\n",
    "            if best is not None:\n",
    "                hits.append(best)\n",
    "\n",
    "    return hits\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Deep sweep for one (country, month, signal)\n",
    "# =============================================================================\n",
    "\n",
    "def deep_sweep_group(\n",
    "    *,\n",
    "    base_npz: Path,\n",
    "    cd_npz: Path,\n",
    "    country_name: str,\n",
    "    month: int,\n",
    "    signal: str,\n",
    "    out_dir: Path,\n",
    "    val_years_list: Sequence[int],\n",
    "    min_rows: int,\n",
    "    w_min: int,\n",
    "    w_max: int,\n",
    "    shift_min: int,\n",
    "    shift_max: int,\n",
    "    coarse_shifts: Sequence[int],\n",
    "    aggs: Sequence[str],\n",
    "    transforms: Sequence[str],\n",
    "    top_windows_keep: int,\n",
    "    top_pairs_keep: int,\n",
    "    refine_w_radius: int,\n",
    "    refine_shift_radius: int,\n",
    "    q_for_streakq: float,\n",
    "    save_top: int,\n",
    "    save_top_public: int,\n",
    "    public_vy: int,\n",
    "    time_budget_end: float,\n",
    "    resume: bool,\n",
    ") -> Optional[str]:\n",
    "    if time.time() > time_budget_end:\n",
    "        return None\n",
    "\n",
    "    safe_country = country_name.replace(\" \", \"_\")\n",
    "    out_csv = out_dir / f\"grid_{safe_country}_m{int(month):02d}_{signal}.csv.gz\"\n",
    "    if resume and out_csv.exists():\n",
    "        return str(out_csv)\n",
    "\n",
    "    base = _load_npz(base_npz)\n",
    "    cd = _load_npz(cd_npz)\n",
    "\n",
    "    countries = base[\"countries\"].tolist()\n",
    "    try:\n",
    "        country_code = int(countries.index(country_name))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "    signal_cols = cd[\"signal_cols\"].tolist()\n",
    "    if signal not in signal_cols:\n",
    "        return None\n",
    "    sig_idx = int(signal_cols.index(signal))\n",
    "\n",
    "    bc = base[\"base_country_code\"].astype(np.int16)\n",
    "    bm = base[\"base_month\"].astype(np.int8)\n",
    "    idx = np.where((bc == country_code) & (bm == int(month)))[0]\n",
    "    if idx.size < int(min_rows):\n",
    "        return None\n",
    "\n",
    "    bucket_dates = base[\"base_date_int\"].astype(np.int32)[idx]\n",
    "    bucket_years = base[\"base_year\"].astype(np.int16)[idx]\n",
    "    Y_bucket = base[\"Y\"].astype(np.float32)[idx].astype(np.float64)\n",
    "\n",
    "    cdc = cd[\"cd_country_code\"].astype(np.int16)\n",
    "    idx_cd = np.where(cdc == country_code)[0]\n",
    "    if idx_cd.size == 0:\n",
    "        return None\n",
    "    cd_dates = cd[\"cd_date_int\"].astype(np.int32)[idx_cd]\n",
    "    order_cd = np.argsort(cd_dates, kind=\"mergesort\")\n",
    "    idx_cd = idx_cd[order_cd]\n",
    "    cd_dates = cd_dates[order_cd]\n",
    "    x_full = cd[\"Xsig\"].astype(np.float32)[idx_cd, sig_idx].astype(np.float64, copy=False)\n",
    "\n",
    "    mapper_pos, mapper_ok = make_date_mapper(cd_dates, bucket_dates)\n",
    "\n",
    "    w_min = int(max(1, w_min))\n",
    "    w_max = int(max(w_min, w_max))\n",
    "    w_grid = [w for w in make_window_grid(w_min, w_max) if w_min <= w <= w_max]\n",
    "\n",
    "    coarse = sorted(set(int(s) for s in coarse_shifts if int(shift_min) <= int(s) <= int(shift_max)))\n",
    "    if 0 not in coarse and int(shift_min) <= 0 <= int(shift_max):\n",
    "        coarse = [0] + coarse\n",
    "\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    agg_cache: Dict[Tuple[str, int], np.ndarray] = {}\n",
    "    def get_agg_series(agg: str, w: int) -> np.ndarray:\n",
    "        key = (agg, int(w))\n",
    "        if key in agg_cache:\n",
    "            return agg_cache[key]\n",
    "        s = compute_agg_no_shift(x_full, agg=agg, w=int(w), q_for_streakq=q_for_streakq)\n",
    "        agg_cache[key] = s.astype(np.float64, copy=False)\n",
    "        return agg_cache[key]\n",
    "\n",
    "    def score_one(agg: str, transform: str, w: int, shift: int) -> Tuple[float, float, Dict[int, float]]:\n",
    "        base_series = get_agg_series(agg, w)\n",
    "        z_full = apply_transform(shift_array(base_series, int(shift)), transform)\n",
    "        z_full = ffill_bfill_0(z_full)\n",
    "        proxy, robust, per = score_on_bucket(\n",
    "            z_full=z_full,\n",
    "            mapper_pos=mapper_pos,\n",
    "            mapper_ok=mapper_ok,\n",
    "            bucket_years=bucket_years,\n",
    "            Y_bucket=Y_bucket,\n",
    "            val_years_list=val_years_list,\n",
    "        )\n",
    "        return proxy, robust, per\n",
    "\n",
    "    # Stage A: shift=0, rank windows per (agg,transform)\n",
    "    seeds: List[Tuple[float, str, str, int, int]] = []\n",
    "    for agg in aggs:\n",
    "        for transform in transforms:\n",
    "            if time.time() > time_budget_end:\n",
    "                break\n",
    "            scored_w: List[Tuple[float, int]] = []\n",
    "            for w in w_grid:\n",
    "                if time.time() > time_budget_end:\n",
    "                    break\n",
    "                proxy, _, _ = score_one(agg, transform, int(w), 0)\n",
    "                scored_w.append((float(proxy), int(w)))\n",
    "            if not scored_w:\n",
    "                continue\n",
    "            scored_w.sort(reverse=True, key=lambda t: t[0])\n",
    "            keep_ws = [w for _, w in scored_w[: min(int(top_windows_keep), len(scored_w))]]\n",
    "\n",
    "            # Stage B: for each kept window, score coarse shifts; keep top 3 shifts\n",
    "            for w in keep_ws:\n",
    "                scored_s: List[Tuple[float, int]] = []\n",
    "                for s in coarse:\n",
    "                    if time.time() > time_budget_end:\n",
    "                        break\n",
    "                    proxy, _, _ = score_one(agg, transform, int(w), int(s))\n",
    "                    scored_s.append((float(proxy), int(s)))\n",
    "                scored_s.sort(reverse=True, key=lambda t: t[0])\n",
    "                for proxy, s in scored_s[:3]:\n",
    "                    seeds.append((float(proxy), agg, transform, int(w), int(s)))\n",
    "\n",
    "    if not seeds:\n",
    "        return None\n",
    "    seeds.sort(reverse=True, key=lambda t: t[0])\n",
    "    seeds = seeds[: min(int(top_pairs_keep), len(seeds))]\n",
    "\n",
    "    # Refinement per seed: 1D shift refine, 1D window refine, small shift refine\n",
    "    for _, agg, transform, w0, s0 in seeds:\n",
    "        if time.time() > time_budget_end:\n",
    "            break\n",
    "\n",
    "        # shift refine (w fixed)\n",
    "        s_lo = max(int(shift_min), int(s0) - int(refine_shift_radius))\n",
    "        s_hi = min(int(shift_max), int(s0) + int(refine_shift_radius))\n",
    "        best_s = int(s0)\n",
    "        best_proxy = -1e9\n",
    "        for s in range(s_lo, s_hi + 1):\n",
    "            proxy, _, _ = score_one(agg, transform, int(w0), int(s))\n",
    "            if proxy > best_proxy:\n",
    "                best_proxy = float(proxy)\n",
    "                best_s = int(s)\n",
    "\n",
    "        # window refine (shift fixed)\n",
    "        w_lo = max(int(w_min), int(w0) - int(refine_w_radius))\n",
    "        w_hi = min(int(w_max), int(w0) + int(refine_w_radius))\n",
    "        best_w = int(w0)\n",
    "        best_proxy2 = -1e9\n",
    "        for w in range(w_lo, w_hi + 1):\n",
    "            proxy, _, _ = score_one(agg, transform, int(w), int(best_s))\n",
    "            if proxy > best_proxy2:\n",
    "                best_proxy2 = float(proxy)\n",
    "                best_w = int(w)\n",
    "\n",
    "        # small shift refine (w fixed)\n",
    "        sr = max(5, int(refine_shift_radius // 3))\n",
    "        s_lo2 = max(int(shift_min), int(best_s) - sr)\n",
    "        s_hi2 = min(int(shift_max), int(best_s) + sr)\n",
    "        best_s2 = int(best_s)\n",
    "        best_proxy3 = -1e9\n",
    "        for s in range(s_lo2, s_hi2 + 1):\n",
    "            proxy, _, _ = score_one(agg, transform, int(best_w), int(s))\n",
    "            if proxy > best_proxy3:\n",
    "                best_proxy3 = float(proxy)\n",
    "                best_s2 = int(s)\n",
    "\n",
    "        proxy, robust, per = score_one(agg, transform, int(best_w), int(best_s2))\n",
    "        row: Dict[str, Any] = dict(\n",
    "            country=country_name,\n",
    "            month=int(month),\n",
    "            signal=str(signal),\n",
    "            agg=str(agg),\n",
    "            transform=str(transform),\n",
    "            w=int(best_w),\n",
    "            shift=int(best_s2),\n",
    "            proxy_cfcs=float(proxy),\n",
    "            robust_min_cfcs=float(robust),\n",
    "            n_rows=int(idx.size),\n",
    "        )\n",
    "        for vy, sc in per.items():\n",
    "            row[f\"vy{int(vy)}_cfcs\"] = float(sc) if math.isfinite(sc) else float(\"nan\")\n",
    "        rows.append(row)\n",
    "\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    keycols = [\"country\",\"month\",\"signal\",\"agg\",\"transform\",\"w\",\"shift\"]\n",
    "    df_all = pd.DataFrame(rows).drop_duplicates(subset=keycols)\n",
    "\n",
    "    # Always keep robust-first rows\n",
    "    df_rob = df_all.sort_values([\"robust_min_cfcs\", \"proxy_cfcs\"], ascending=False).head(int(save_top))\n",
    "\n",
    "    # Also keep public-proxy rows (so they don't get truncated away)\n",
    "    pub_col = f\"vy{int(public_vy)}_cfcs\"\n",
    "    if int(save_top_public) > 0 and pub_col in df_all.columns:\n",
    "        df_pub = df_all.sort_values([pub_col, \"proxy_cfcs\"], ascending=False).head(int(save_top_public))\n",
    "        df_out = pd.concat([df_rob, df_pub], ignore_index=True).drop_duplicates(subset=keycols)\n",
    "    else:\n",
    "        df_out = df_rob\n",
    "\n",
    "    # Final ordering for the saved grid file (robust-first)\n",
    "    df_out = df_out.sort_values([\"robust_min_cfcs\", \"proxy_cfcs\"], ascending=False)\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with gzip.open(out_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "        df_out.to_csv(f, index=False)\n",
    "\n",
    "    return str(out_csv)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main orchestration\n",
    "# =============================================================================\n",
    "\n",
    "def parse_int_list(s: str) -> List[int]:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    return [int(x.strip()) for x in s.split(\",\") if x.strip()]\n",
    "\n",
    "def parse_str_list(s: str) -> List[str]:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    return [x.strip() for x in s.split(\",\") if x.strip()]\n",
    "\n",
    "def main() -> None:\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--main_csv\", required=True)\n",
    "    ap.add_argument(\"--share_csv\", required=True)\n",
    "    ap.add_argument(\"--out_dir\", required=True)\n",
    "\n",
    "    ap.add_argument(\"--n_jobs\", type=int, default=24)\n",
    "    ap.add_argument(\"--time_budget_hours\", type=float, default=12.0)\n",
    "    ap.add_argument(\"--resume\", action=\"store_true\")\n",
    "    ap.add_argument(\"--rebuild_cache\", action=\"store_true\")\n",
    "\n",
    "    ap.add_argument(\"--no_kaggle_rowset\", action=\"store_true\",\n",
    "                    help=\"Disable Kaggle row-set reconstruction + dropna (debug only).\")\n",
    "\n",
    "    ap.add_argument(\"--val_years_list\", type=str, default=\"2,3,4,5\")\n",
    "    ap.add_argument(\"--min_rows\", type=int, default=60)\n",
    "\n",
    "    # Stage 0 controls\n",
    "    ap.add_argument(\"--stage0_windows\", type=str, default=\"7,14,21,30,45,60,90,98,102,112,120,140,168,180,200,224,240,252,280,300,330,365,400,450,540,730,1095\")\n",
    "    ap.add_argument(\"--stage0_aggs\", type=str, default=\"ma\")\n",
    "    ap.add_argument(\"--stage0_transforms\", type=str, default=\"identity,square,signlog1p\")\n",
    "    ap.add_argument(\"--stage0_keep_per_country_month\", type=int, default=3)\n",
    "\n",
    "    # Deep selection\n",
    "    ap.add_argument(\"--deep_top_groups\", type=int, default=400,\n",
    "                    help=\"Deep sweep top-N (country,month,signal) groups by baseline proxy. 0 => deep all.\")\n",
    "    ap.add_argument(\"--deep_min_proxy\", type=float, default=70.0)\n",
    "\n",
    "    # Deep sweep search space\n",
    "    ap.add_argument(\"--w_min\", type=int, default=2)\n",
    "    ap.add_argument(\"--w_max\", type=int, default=2500)\n",
    "    ap.add_argument(\"--shift_min\", type=int, default=-365)\n",
    "    ap.add_argument(\"--shift_max\", type=int, default=365)\n",
    "    ap.add_argument(\"--coarse_shifts\", type=str, default=\"-365,-240,-180,-150,-120,-90,-60,-30,0,30,60,90,120,150,180,240,365\")\n",
    "    ap.add_argument(\"--aggs\", type=str, default=\"ma,max,ewm,std,streakq85,streakthr0.5,runlenth0.5\")\n",
    "    ap.add_argument(\"--transforms\", type=str, default=\"identity,square,signlog1p\")\n",
    "    ap.add_argument(\"--top_windows_keep\", type=int, default=60)\n",
    "    ap.add_argument(\"--top_pairs_keep\", type=int, default=80)\n",
    "    ap.add_argument(\"--refine_w_radius\", type=int, default=80)\n",
    "    ap.add_argument(\"--refine_shift_radius\", type=int, default=45)\n",
    "    ap.add_argument(\"--save_top\", type=int, default=500)\n",
    "    ap.add_argument(\"--streak_q\", type=float, default=0.85)\n",
    "    ap.add_argument(\"--public_vy\", type=int, default=2,\n",
    "                help=\"Which vyN_cfcs column to treat as the public-LB proxy (e.g. 2 => vy2_cfcs).\")\n",
    "    ap.add_argument(\"--save_top_public\", type=int, default=200,\n",
    "                help=\"Per-group: also keep this many rows ranked by vy{public_vy}_cfcs so public-strong rows don't get truncated.\")\n",
    "\n",
    "\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    out_dir = Path(args.out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    time_budget_end = time.time() + float(args.time_budget_hours) * 3600.0\n",
    "    val_years_list = parse_int_list(args.val_years_list)\n",
    "    stage0_windows = parse_int_list(args.stage0_windows)\n",
    "    stage0_aggs = parse_str_list(args.stage0_aggs)\n",
    "    stage0_transforms = parse_str_list(args.stage0_transforms)\n",
    "    coarse_shifts = parse_int_list(args.coarse_shifts)\n",
    "    aggs = parse_str_list(args.aggs)\n",
    "    transforms = parse_str_list(args.transforms)\n",
    "\n",
    "    kaggle_rowset = not bool(args.no_kaggle_rowset)\n",
    "\n",
    "    log(\"Preparing caches...\")\n",
    "    base_npz, cd_npz, meta = load_or_build_caches(\n",
    "        main_csv=args.main_csv,\n",
    "        share_csv=args.share_csv,\n",
    "        out_dir=str(out_dir),\n",
    "        rebuild=bool(args.rebuild_cache),\n",
    "        kaggle_rowset=bool(kaggle_rowset),\n",
    "    )\n",
    "    log(f\"Cache meta: {meta}\")\n",
    "\n",
    "    # ---------------- Stage 0 ----------------\n",
    "    baseline_path = out_dir / \"baseline_scan_all.csv\"\n",
    "    if args.resume and baseline_path.exists():\n",
    "        log(f\"Resume: loading existing baseline scan: {baseline_path}\")\n",
    "        baseline_all = pd.read_csv(baseline_path)\n",
    "    else:\n",
    "        log(\"Stage 0: baseline scan across ALL countries...\")\n",
    "        base = _load_npz(base_npz)\n",
    "        n_countries = int(len(base[\"countries\"].tolist()))\n",
    "        country_codes = list(range(n_countries))\n",
    "\n",
    "        hits_nested: List[List[BaselineHit]] = Parallel(n_jobs=int(args.n_jobs), backend=\"loky\", batch_size=1)(\n",
    "            delayed(stage0_scan_country)(\n",
    "                base_npz=base_npz,\n",
    "                cd_npz=cd_npz,\n",
    "                country_code=cc,\n",
    "                stage0_windows=stage0_windows,\n",
    "                stage0_aggs=stage0_aggs,\n",
    "                stage0_transforms=stage0_transforms,\n",
    "                val_years_list=val_years_list,\n",
    "                min_rows=int(args.min_rows),\n",
    "                q_for_streakq=float(args.streak_q),\n",
    "                time_budget_end=time_budget_end,\n",
    "            )\n",
    "            for cc in country_codes\n",
    "        )\n",
    "\n",
    "        hits = [h for sub in hits_nested for h in sub]\n",
    "        if not hits:\n",
    "            raise SystemExit(\"Stage 0 produced no hits (check min_rows / cache / data).\")\n",
    "\n",
    "        rows = []\n",
    "        for h in hits:\n",
    "            r = dict(\n",
    "                country=h.country,\n",
    "                month=int(h.month),\n",
    "                signal=h.signal,\n",
    "                agg=h.agg,\n",
    "                transform=h.transform,\n",
    "                w=int(h.w),\n",
    "                shift=int(h.shift),\n",
    "                proxy_cfcs=float(h.proxy_cfcs),\n",
    "                robust_min_cfcs=float(h.robust_min_cfcs),\n",
    "                n_rows=int(h.n_rows),\n",
    "                n_years=int(h.n_years),\n",
    "            )\n",
    "            for vy, sc in h.per_vy.items():\n",
    "                r[f\"vy{int(vy)}_cfcs\"] = float(sc) if math.isfinite(sc) else float(\"nan\")\n",
    "            rows.append(r)\n",
    "\n",
    "        baseline_all = pd.DataFrame(rows)\n",
    "        baseline_all = baseline_all.sort_values([\"proxy_cfcs\",\"robust_min_cfcs\"], ascending=False).reset_index(drop=True)\n",
    "        baseline_all.to_csv(baseline_path, index=False)\n",
    "        log(f\"Wrote: {baseline_path}\")\n",
    "\n",
    "    # Keep only top signals per (country, month)\n",
    "    baseline_all = baseline_all.sort_values([\"country\",\"month\",\"proxy_cfcs\"], ascending=[True, True, False]).copy()\n",
    "    baseline_top = (\n",
    "        baseline_all.groupby([\"country\",\"month\"], as_index=False, sort=True)\n",
    "        .head(int(args.stage0_keep_per_country_month))\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    # Select deep groups\n",
    "    baseline_top = baseline_top.sort_values([\"proxy_cfcs\",\"robust_min_cfcs\"], ascending=False).reset_index(drop=True)\n",
    "    baseline_top = baseline_top[baseline_top[\"proxy_cfcs\"] >= float(args.deep_min_proxy)].copy()\n",
    "\n",
    "    if int(args.deep_top_groups) > 0:\n",
    "        deep_groups = baseline_top.head(int(args.deep_top_groups)).copy()\n",
    "    else:\n",
    "        deep_groups = baseline_top.copy()\n",
    "\n",
    "    if deep_groups.empty:\n",
    "        raise SystemExit(\"No deep groups after filtering. Lower --deep_min_proxy or increase --stage0_keep_per_country_month.\")\n",
    "\n",
    "    deep_list = list(deep_groups[[\"country\",\"month\",\"signal\"]].itertuples(index=False, name=None))\n",
    "    log(f\"Deep sweep groups: {len(deep_list)} (deep_top_groups={args.deep_top_groups}, deep_min_proxy={args.deep_min_proxy})\")\n",
    "\n",
    "    coarse_windows_count = len(make_window_grid(int(args.w_min), int(args.w_max)))\n",
    "    log(f\"Deep sweep search summary: aggs={len(aggs)} transforms={len(transforms)} coarse_windows{coarse_windows_count} coarse_shifts={len(coarse_shifts)}\")\n",
    "    if int(args.deep_top_groups) == 0:\n",
    "        log(\"WARNING: deep_top_groups=0 means 'deep all'. This can take a VERY long time.\")\n",
    "\n",
    "    # ---------------- Stage 1 ----------------\n",
    "    grids_dir = out_dir / \"grids\"\n",
    "    grids_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _deep_one(tup: Tuple[str, int, str]) -> Optional[str]:\n",
    "        c, m, s = tup\n",
    "        return deep_sweep_group(\n",
    "            base_npz=base_npz,\n",
    "            cd_npz=cd_npz,\n",
    "            country_name=str(c),\n",
    "            month=int(m),\n",
    "            signal=str(s),\n",
    "            out_dir=grids_dir,\n",
    "            val_years_list=val_years_list,\n",
    "            min_rows=int(args.min_rows),\n",
    "            w_min=int(args.w_min),\n",
    "            w_max=int(args.w_max),\n",
    "            shift_min=int(args.shift_min),\n",
    "            shift_max=int(args.shift_max),\n",
    "            coarse_shifts=coarse_shifts,\n",
    "            aggs=aggs,\n",
    "            transforms=transforms,\n",
    "            top_windows_keep=int(args.top_windows_keep),\n",
    "            top_pairs_keep=int(args.top_pairs_keep),\n",
    "            refine_w_radius=int(args.refine_w_radius),\n",
    "            refine_shift_radius=int(args.refine_shift_radius),\n",
    "            q_for_streakq=float(args.streak_q),\n",
    "            save_top=int(args.save_top),\n",
    "            time_budget_end=time_budget_end,\n",
    "            resume=bool(args.resume),\n",
    "            public_vy=int(args.public_vy),\n",
    "            save_top_public=int(args.save_top_public),\n",
    "        )\n",
    "\n",
    "    log(\"Stage 1: starting deep sweep...\")\n",
    "    deep_paths = Parallel(n_jobs=int(args.n_jobs), backend=\"loky\", batch_size=1)(\n",
    "        delayed(_deep_one)(t) for t in deep_list\n",
    "    )\n",
    "    deep_paths = [p for p in deep_paths if p is not None]\n",
    "    log(f\"Deep sweep produced {len(deep_paths)} grid files.\")\n",
    "\n",
    "    # Consolidate\n",
    "    log(\"Consolidating grid files...\")\n",
    "    frames = []\n",
    "    for p in sorted(grids_dir.glob(\"grid_*.csv.gz\")):\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            df[\"grid_file\"] = str(p.name)\n",
    "            frames.append(df)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not frames:\n",
    "        raise SystemExit(\"No grid files found to consolidate.\")\n",
    "\n",
    "    grid_all = pd.concat(frames, ignore_index=True)\n",
    "    grid_all_path = out_dir / \"grid_all_candidates.csv\"\n",
    "    grid_all.to_csv(grid_all_path, index=False)\n",
    "    log(f\"Wrote: {grid_all_path} | rows={len(grid_all):,}\")\n",
    "\n",
    "    # Recommendations\n",
    "    grid_all2 = grid_all.copy()\n",
    "    grid_all2[\"abs_shift\"] = grid_all2[\"shift\"].abs()\n",
    "\n",
    "    robust_pick = grid_all2.sort_values([\"robust_min_cfcs\",\"proxy_cfcs\",\"abs_shift\"], ascending=[False, False, True]).head(25)\n",
    "\n",
    "    best_rob = float(grid_all2[\"robust_min_cfcs\"].max())\n",
    "    aggr_pool = grid_all2[grid_all2[\"robust_min_cfcs\"] >= (best_rob - 1.0)].copy()\n",
    "    aggr_pick = aggr_pool.sort_values([\"proxy_cfcs\",\"robust_min_cfcs\",\"abs_shift\"], ascending=[False, False, True]).head(25)\n",
    "\n",
    "    rec_path = out_dir / \"recommendations.txt\"\n",
    "    lines = []\n",
    "    lines.append(\"TOP-25 ROBUST (max robust_min_cfcs, tie proxy, prefer smaller |shift|)\\n\")\n",
    "    lines.append(robust_pick[[\"country\",\"month\",\"signal\",\"agg\",\"transform\",\"w\",\"shift\",\"robust_min_cfcs\",\"proxy_cfcs\"]].to_string(index=False))\n",
    "    lines.append(\"\\n\\nTOP-25 AGGRESSIVE (max proxy_cfcs among robust>=best-1)\\n\")\n",
    "    lines.append(aggr_pick[[\"country\",\"month\",\"signal\",\"agg\",\"transform\",\"w\",\"shift\",\"robust_min_cfcs\",\"proxy_cfcs\"]].to_string(index=False))\n",
    "    lines.append(\"\\n\\nNOTES\\n\")\n",
    "    lines.append(\" - This script scores ONE gated feature per row (single-feature submissions).\\n\")\n",
    "    lines.append(\" - If you want Kaggle-faithful CFCS, keep only one climate_risk_* column in your submission.\\n\")\n",
    "    lines.append(\" - shift < 0 = lead (uses future climate relative to futures date); shift > 0 = lag.\\n\")\n",
    "    rec_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    log(f\"Wrote: {rec_path}\")\n",
    "\n",
    "    pub_col = f\"vy{int(args.public_vy)}_cfcs\"\n",
    "    if pub_col in grid_all.columns:\n",
    "        g = grid_all.copy()\n",
    "        g[\"abs_shift\"] = g[\"shift\"].abs()\n",
    "\n",
    "        # pick public-proxy: prioritize vyN, then proxy, then robustness, then prefer shift closer to 0\n",
    "        g = g.sort_values([pub_col, \"proxy_cfcs\", \"robust_min_cfcs\", \"abs_shift\"],\n",
    "                        ascending=[False, False, False, True])\n",
    "\n",
    "        pub_pick = g.drop_duplicates(subset=[\"country\",\"month\",\"signal\",\"agg\",\"transform\",\"w\",\"shift\"]).head(25)\n",
    "\n",
    "        outp = out_dir / \"recommendations_public.txt\"\n",
    "        lines = []\n",
    "        lines.append(f\"Top PUBLIC-PROXY picks (ranked by {pub_col})\")\n",
    "        lines.append(\"\")\n",
    "        for _, r in pub_pick.iterrows():\n",
    "            lines.append(\n",
    "                f\"{r['country']}\\tm{int(r['month'])}\\t{r['signal']}\\t{r['agg']}\\t{r['transform']}\\t\"\n",
    "                f\"w={int(r['w'])}\\tshift={int(r['shift'])}\\t\"\n",
    "                f\"{pub_col}={float(r[pub_col]):.2f}\\trobust_min={float(r['robust_min_cfcs']):.2f}\\tproxy={float(r['proxy_cfcs']):.2f}\"\n",
    "            )\n",
    "        outp.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "        log(f\"Wrote: {outp}\")\n",
    "\n",
    "    log(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15008526,
     "sourceId": 126158,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 23.714556,
   "end_time": "2026-01-29T09:36:06.089080",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-29T09:35:42.374524",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
