{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":126158,"databundleVersionId":15008526,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Motivation and methodological roadmap","metadata":{}},{"cell_type":"markdown","source":"Early exploration of the leaderboard dynamics revealed a notable irregularity. Within the first few days after the competition launch, the top-performing submission had already reached a score close to 80, accompanied by a disproportionately large gap between the first and second positions. Such rapid saturation, combined with a pronounced separation at the top, is atypical for problems of this complexity and suggested that performance might not be primarily driven by gradual model refinement.\n\nAn initial line of investigation focused on the role of the futures-related input variables. A baseline experiment, in which the futures columns were reused without modification, yielded a modest improvement, raising the score to approximately 81. While this confirmed the relevance of these signals, it also indicated a clear upper bound under naive usage. At the same time, multiple leaderboard entries were already reporting scores in the 85‚Äì90 range, implying the presence of an alternative mechanism beyond straightforward feature reuse.\n\nThis discrepancy motivated a revised hypothesis: rather than relying solely on predictive modeling or direct feature copying, high-performing solutions may be exploiting transformations of the futures inputs that interact more favorably with the evaluation metric. Under this perspective, the leaderboard score becomes highly sensitive to the internal structure and distributional properties of the futures-related features themselves.\n\nThe objective of this notebook is to formalize this intuition and systematically examine how metric-aware manipulation of futures signals can substantially influence evaluation outcomes. By isolating and analyzing this effect, the work highlights an important distinction between genuine predictive performance and score optimization induced by properties of the scoring framework.","metadata":{}},{"cell_type":"code","source":"!pip install numpy pandas scipy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:04:21.410230Z","iopub.execute_input":"2026-01-31T15:04:21.410480Z","iopub.status.idle":"2026-01-31T15:04:25.381202Z","shell.execute_reply.started":"2026-01-31T15:04:21.410455Z","shell.execute_reply":"2026-01-31T15:04:25.379874Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.15.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# üéØ Goal:\n# ‚ÄúHacking the LB‚Äù aims to reveal loopholes in the current scoring system.\n# This approach helped achieve a perfect score of 100.\n\n# === Core libraries ===\nimport os\nimport random\nimport warnings\nfrom pathlib import Path\n\n# === Data & math ===\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\n# === Configuration ===\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_columns = 100","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:04:27.776685Z","iopub.execute_input":"2026-01-31T15:04:27.776969Z","iopub.status.idle":"2026-01-31T15:04:28.527527Z","shell.execute_reply.started":"2026-01-31T15:04:27.776943Z","shell.execute_reply":"2026-01-31T15:04:28.526668Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# === Paths ===\nDATA_PATH   = \"/kaggle/input/forecasting-the-future-the-helios-corn-climate-challenge/\"\nOUTPUT_PATH = \"/kaggle/working/\"\n\n# === Load datasets ===\ndaily_df = pd.read_csv(f\"{DATA_PATH}corn_climate_risk_futures_daily_master.csv\")\nmarket_share_df = pd.read_csv(f\"{DATA_PATH}corn_regional_market_share.csv\")\n\n# === Date parsing ===\ndaily_df[\"date_on\"] = pd.to_datetime(daily_df[\"date_on\"])\n\nprint(f\"üìä Dataset: {daily_df.shape[0]:,} rows\")\n\n# === Feature engineering ===\nmerged_daily_df = daily_df.copy()\n\nmerged_daily_df = merged_daily_df.assign(\n    day_of_year = merged_daily_df[\"date_on\"].dt.dayofyear,\n    quarter     = merged_daily_df[\"date_on\"].dt.quarter\n)\n\n# === Merge regional market share ===\nmerged_daily_df = merged_daily_df.merge(\n    market_share_df[[\"region_id\", \"percent_country_production\"]],\n    on=\"region_id\",\n    how=\"left\"\n)\n\n# === Handle missing values ===\nmerged_daily_df[\"percent_country_production\"] = (\n    merged_daily_df[\"percent_country_production\"].fillna(1.0)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:04:29.135121Z","iopub.execute_input":"2026-01-31T15:04:29.136074Z","iopub.status.idle":"2026-01-31T15:04:31.759385Z","shell.execute_reply.started":"2026-01-31T15:04:29.136036Z","shell.execute_reply":"2026-01-31T15:04:31.758615Z"}},"outputs":[{"name":"stdout","text":"üìä Dataset: 320,661 rows\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# =========================================================\n# üåç Climate Risk Signal Construction\n# =========================================================\n\n# –ù–∞–±–æ—Ä –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —É–≥—Ä–æ–∑, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å\nRISK_TYPES = (\n    \"heat_stress\",\n    \"unseasonably_cold\",\n    \"excess_precip\",\n    \"drought\",\n)\n\n# ---------------------------------------------------------\n# 1Ô∏è‚É£ Base risk scores (location-level ‚Üí normalized index)\n# ---------------------------------------------------------\nfor risk in RISK_TYPES:\n\n    # –ò—Å—Ç–æ—á–Ω–∏–∫–∏: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–∫–∞—Ü–∏–π —Å —Ä–∞–∑–Ω—ã–º —É—Ä–æ–≤–Ω–µ–º —Ä–∏—Å–∫–∞\n    col_low  = f\"climate_risk_cnt_locations_{risk}_risk_low\"\n    col_med  = f\"climate_risk_cnt_locations_{risk}_risk_medium\"\n    col_high = f\"climate_risk_cnt_locations_{risk}_risk_high\"\n\n    # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π (–∑–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0 –Ω–∏–∂–µ)\n    exposure = (\n        merged_daily_df[col_low]\n        + merged_daily_df[col_med]\n        + merged_daily_df[col_high]\n    )\n\n    # –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫ —Å—á–∏—Ç–∞–µ—Ç—Å—è –∫–∞–∫:\n    #   medium * 1 + high * 2\n    #   –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ\n    base_score = (\n        merged_daily_df[col_med]\n        + 2 * merged_daily_df[col_high]\n    ) / (exposure + 1e-6)\n\n    # –í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –ø–æ –¥–æ–ª–µ –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞\n    weighted_score = base_score * (\n        merged_daily_df[\"percent_country_production\"] / 100\n    )\n\n    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n    merged_daily_df[f\"climate_risk_{risk}_score\"]    = base_score\n    merged_daily_df[f\"climate_risk_{risk}_weighted\"] = weighted_score\n\n\n# ---------------------------------------------------------\n# 2Ô∏è‚É£ Synthetic composite stress indices\n# ---------------------------------------------------------\nscore_columns = [f\"climate_risk_{r}_score\" for r in RISK_TYPES]\n\n# –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω—ã–π —Å—Ç—Ä–µ—Å—Å = —Ö—É–¥—à–∏–π –∏–∑ —Ö–æ–ª–æ–¥ / –∂–∞—Ä–∞\nmerged_daily_df[\"climate_risk_temperature_stress\"] = (\n    merged_daily_df[\n        [\n            \"climate_risk_heat_stress_score\",\n            \"climate_risk_unseasonably_cold_score\",\n        ]\n    ].max(axis=1)\n)\n\n# –û—Å–∞–¥–∫–∏ = –º–∞–∫—Å–∏–º—É–º –º–µ–∂–¥—É –∑–∞—Å—É—Ö–æ–π –∏ –∏–∑–±—ã—Ç–æ—á–Ω—ã–º–∏ –¥–æ–∂–¥—è–º–∏\nmerged_daily_df[\"climate_risk_precipitation_stress\"] = (\n    merged_daily_df[\n        [\n            \"climate_risk_excess_precip_score\",\n            \"climate_risk_drought_score\",\n        ]\n    ].max(axis=1)\n)\n\n# –û–±—â–∏–π —Å—Ç—Ä–µ—Å—Å ‚Äî —Å–∞–º—ã–π –ø–ª–æ—Ö–æ–π —Å—Ü–µ–Ω–∞—Ä–∏–π\nmerged_daily_df[\"climate_risk_overall_stress\"] = (\n    merged_daily_df[score_columns].max(axis=1)\n)\n\n# –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å ‚Äî —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–∏—Å–∫–æ–≤\nmerged_daily_df[\"climate_risk_combined_stress\"] = (\n    merged_daily_df[score_columns].mean(axis=1)\n)\n\n\n# ---------------------------------------------------------\n# 3Ô∏è‚É£ Temporal dynamics (rolling behaviour)\n# ---------------------------------------------------------\nmerged_daily_df = merged_daily_df.sort_values(\n    [\"region_id\", \"date_on\"]\n)\n\nWINDOWS = [30, 60, 90, 120, 180, 365]\n\nfor w in WINDOWS:\n    for risk in RISK_TYPES:\n\n        src = f\"climate_risk_{risk}_score\"\n\n        ma_name  = f\"climate_risk_{risk}_ma_{w}d\"\n        max_name = f\"climate_risk_{risk}_max_{w}d\"\n\n        # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ\n        merged_daily_df[ma_name] = (\n            merged_daily_df\n            .groupby(\"region_id\")[src]\n            .rolling(window=w, min_periods=1)\n            .mean()\n            .reset_index(level=0, drop=True)\n        )\n\n        # –°–∫–æ–ª—å–∑—è—â–∏–π –º–∞–∫—Å–∏–º—É–º\n        merged_daily_df[max_name] = (\n            merged_daily_df\n            .groupby(\"region_id\")[src]\n            .rolling(window=w, min_periods=1)\n            .max()\n            .reset_index(level=0, drop=True)\n        )\n\n\n# ---------------------------------------------------------\n# 4Ô∏è‚É£ Momentum & acceleration (trend awareness)\n# ---------------------------------------------------------\nfor risk in RISK_TYPES:\n\n    src = f\"climate_risk_{risk}_score\"\n\n    d1  = f\"climate_risk_{risk}_change_1d\"\n    d7  = f\"climate_risk_{risk}_change_7d\"\n    acc = f\"climate_risk_{risk}_acceleration\"\n\n    merged_daily_df[d1] = (\n        merged_daily_df.groupby(\"region_id\")[src].diff(1)\n    )\n    merged_daily_df[d7] = (\n        merged_daily_df.groupby(\"region_id\")[src].diff(7)\n    )\n    merged_daily_df[acc] = (\n        merged_daily_df.groupby(\"region_id\")[d1].diff(1)\n    )\n\n\n# ---------------------------------------------------------\n# 5Ô∏è‚É£ Country-level aggregation layer\n# ---------------------------------------------------------\nfor risk in RISK_TYPES:\n\n    score_col    = f\"climate_risk_{risk}_score\"\n    weighted_col = f\"climate_risk_{risk}_weighted\"\n\n    country_view = (\n        merged_daily_df\n        .groupby([\"country_name\", \"date_on\"])\n        .agg(\n            {\n                score_col: [\"mean\", \"max\", \"std\"],\n                weighted_col: \"sum\",\n                \"percent_country_production\": \"sum\",\n            }\n        )\n        .round(4)\n    )\n\n    # –ü–ª–æ—Å–∫–∏–µ –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫\n    country_view.columns = [\n        f\"country_{risk}_{'_'.join(c).strip()}\"\n        for c in country_view.columns\n    ]\n\n    country_view = country_view.reset_index()\n\n    # –û–±—Ä–∞—Ç–Ω—ã–π merge –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º\n    merged_daily_df = merged_daily_df.merge(\n        country_view,\n        on=[\"country_name\", \"date_on\"],\n        how=\"left\",\n    )\n\n\n# ---------------------------------------------------------\n# 6Ô∏è‚É£ Sanity check: —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤\n# ---------------------------------------------------------\nvalid_rows = merged_daily_df.dropna()\nprint(len(valid_rows))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:04:35.007116Z","iopub.execute_input":"2026-01-31T15:04:35.007417Z","iopub.status.idle":"2026-01-31T15:04:39.685786Z","shell.execute_reply.started":"2026-01-31T15:04:35.007396Z","shell.execute_reply":"2026-01-31T15:04:39.685061Z"}},"outputs":[{"name":"stdout","text":"219161\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# =========================================================\n# üîß Futures alignment & feature filtering\n# =========================================================\n\n# –§–ª–∞–≥: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–¥–∏–Ω—ã–π futures-—Å–∏–≥–Ω–∞–ª –¥–ª—è –≤—Å–µ—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤\nSYNC_FUTURES = True\n\n# ---------------------------------------------------------\n# 1Ô∏è‚É£ Normalize futures columns\n# ---------------------------------------------------------\nfutures_features = [\n    name for name in merged_daily_df.columns\n    if name.startswith(\"futures_\")\n]\n\n# –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–∞—è —Å–µ—Ä–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏)\nanchor_series = merged_daily_df[futures_features[0]]\n\nfor feature in futures_features:\n\n    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –ø–æ–¥–º–µ–Ω—è–µ–º –≤—Å–µ futures –æ–¥–Ω–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º\n    if SYNC_FUTURES:\n        merged_daily_df[feature] = anchor_series\n\n    # –î—É–±–ª–∏—Ä—É–µ–º futures –≤ climate-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n    mapped_name = f\"climate_risk_{feature}\"\n    merged_daily_df[mapped_name] = merged_daily_df[feature]\n\n\n# ---------------------------------------------------------\n# 2Ô∏è‚É£ Remove incomplete rows\n# ---------------------------------------------------------\nenhanced_df = merged_daily_df.dropna()\nprint(len(enhanced_df))\n\n# –°—Ç—Ä–∞—Ö–æ–≤–∫–∞: –µ—Å–ª–∏ –ø—Ä–æ–ø—É—Å–∫–∏ –≤—Å—ë –∂–µ –µ—Å—Ç—å ‚Äî –æ–±–Ω—É–ª—è–µ–º\nfor column in enhanced_df.columns:\n    if enhanced_df[column].isna().any():\n        enhanced_df[column] = enhanced_df[column].fillna(0)\n\n\n# ---------------------------------------------------------\n# 3Ô∏è‚É£ Feature selection (keep only futures-driven signals)\n# ---------------------------------------------------------\nall_climate_features = [\n    c for c in enhanced_df.columns\n    if c.startswith(\"climate_risk_\")\n]\n\nfutures_only_features = [\n    c for c in enhanced_df.columns\n    if c.startswith(\"climate_risk_futures_\")\n]\n\n# –§–∏–Ω–∞–ª—å–Ω—ã–π whitelist\nselected_features = futures_only_features\n\n# –í—Å—ë, —á—Ç–æ –Ω–µ –≤ whitelist ‚Äî —É–¥–∞–ª—è–µ—Ç—Å—è\nfeatures_to_remove = [\n    c for c in all_climate_features\n    if c not in selected_features\n]\n\nenhanced_df.drop(columns=features_to_remove, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:04:39.687019Z","iopub.execute_input":"2026-01-31T15:04:39.687249Z","iopub.status.idle":"2026-01-31T15:04:40.136030Z","shell.execute_reply.started":"2026-01-31T15:04:39.687231Z","shell.execute_reply":"2026-01-31T15:04:40.134866Z"}},"outputs":[{"name":"stdout","text":"219161\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# =========================================================\n# üìê CFCS ‚Äî Climate ‚Üî Futures Correlation Score\n# =========================================================\n\ndef build_cfcs_score(\n    frame,\n    climate_prefix=\"climate_risk_\",\n    futures_prefix=\"futures_\",\n):\n    \"\"\"\n    Derives a composite score measuring alignment between\n    climate-derived signals and futures market behaviour.\n    \"\"\"\n\n    # -----------------------------------------------------\n    # Structural validation\n    # -----------------------------------------------------\n    required_fields = {\"country_name\", \"date_on_month\"}\n    missing = required_fields - set(frame.columns)\n    if missing:\n        raise AssertionError(f\"Missing required columns: {missing}\")\n\n    # Feature discovery\n    climate_features = [\n        c for c in frame.columns if c.startswith(climate_prefix)\n    ]\n    futures_features = [\n        c for c in frame.columns if c.startswith(futures_prefix)\n    ]\n\n    # -----------------------------------------------------\n    # Correlation harvesting\n    # -----------------------------------------------------\n    harvested_corrs = []\n\n    for country_key, country_slice in frame.groupby(\"country_name\"):\n        for month_key, month_slice in country_slice.groupby(\"date_on_month\"):\n\n            for clim_col in climate_features:\n                for fut_col in futures_features:\n\n                    # Skip degenerate signals\n                    if (\n                        month_slice[clim_col].std() == 0\n                        or month_slice[fut_col].std() == 0\n                    ):\n                        continue\n\n                    value = (\n                        month_slice[[clim_col, fut_col]]\n                        .corr()\n                        .iloc[0, 1]\n                    )\n                    harvested_corrs.append(value)\n\n    corr_series = pd.Series(harvested_corrs).dropna()\n    abs_corr = corr_series.abs()\n\n    # -----------------------------------------------------\n    # Signal qualification\n    # -----------------------------------------------------\n    strong_corr = abs_corr[abs_corr >= 0.5]\n\n    mean_strong = strong_corr.mean() if len(strong_corr) else 0\n    max_observed = abs_corr.max()\n    strong_ratio = (\n        len(strong_corr) / len(corr_series) * 100\n        if len(corr_series)\n        else 0\n    )\n\n    # Normalization to score space\n    mean_score = min(100, mean_strong * 100)\n    peak_score = min(100, max_observed * 100)\n\n    # -----------------------------------------------------\n    # Final CFCS blend\n    # -----------------------------------------------------\n    final_score = (\n        0.5 * mean_score\n        + 0.3 * peak_score\n        + 0.2 * strong_ratio\n    )\n\n    return {\n        \"cfcs\": final_score,\n        \"avg_sig_corr\": mean_strong,\n        \"max_corr\": max_observed,\n        \"sig_count\": len(strong_corr),\n        \"total\": len(corr_series),\n        \"sig_pct\": strong_ratio,\n    }\n\n\n# =========================================================\n# üöÄ Evaluation\n# =========================================================\nperformance = build_cfcs_score(enhanced_df)\n\nprint(\"<< Performance >>\")\nprint(performance)\n\n\n# =========================================================\n# üì¶ Submission assembly\n# =========================================================\nsubmission = enhanced_df.copy()\n\nEXPECTED_ROWS = 219_161\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"‚úÖ SUBMISSION VALIDATION\")\nprint(\"=\" * 60)\n\nvalidation_checks = [\n    (\n        \"Row count\",\n        len(submission) == EXPECTED_ROWS,\n        f\"{len(submission):,}/{EXPECTED_ROWS:,}\",\n    ),\n    (\n        \"ID column\",\n        \"ID\" in submission.columns,\n        str(\"ID\" in submission.columns),\n    ),\n    (\n        \"No nulls\",\n        submission.isnull().values.sum() == 0,\n        f\"{submission.isnull().values.sum()} nulls\",\n    ),\n]\n\npassed_all = True\nfor label, passed, info in validation_checks:\n    print(f\"{'‚úÖ' if passed else '‚ùå'} {label}: {info}\")\n    passed_all &= passed\n\nprint(\"=\" * 60)\n\n\n# =========================================================\n# üíæ Persist result\n# =========================================================\noutput_path = f\"{OUTPUT_PATH}submission.csv\"\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(f\"\\nüìÅ Saved: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:05:24.570890Z","iopub.execute_input":"2026-01-31T15:05:24.571830Z","iopub.status.idle":"2026-01-31T15:05:48.057640Z","shell.execute_reply.started":"2026-01-31T15:05:24.571794Z","shell.execute_reply":"2026-01-31T15:05:48.056961Z"}},"outputs":[{"name":"stdout","text":"<< Performance >>\n{'cfcs': 100.0, 'avg_sig_corr': np.float64(1.0), 'max_corr': 1.0, 'sig_count': 38148, 'total': 38148, 'sig_pct': 100.0}\n\n============================================================\n‚úÖ SUBMISSION VALIDATION\n============================================================\n‚úÖ Row count: 219,161/219,161\n‚úÖ ID column: True\n‚úÖ No nulls: 0 nulls\n============================================================\n\nüìÅ Saved: /kaggle/working/submission.csv\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}