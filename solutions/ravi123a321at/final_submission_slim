{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SLIM Feature Engineering Pipeline (40 Features)\n",
        "\n",
        "**Pruned for Private Leaderboard Robustness**\n",
        "\n",
        "## Feature Categories:\n",
        "1. Baseline: 8\n",
        "2. Phenology: 8\n",
        "3. Market Interaction: 5\n",
        "4. Complex: 4\n",
        "5. Advanced: 5\n",
        "6. Rare: 2\n",
        "7. Competition: 4\n",
        "8. External Weather: 3\n",
        "9. Soil Moisture: 1\n",
        "\n",
        "**Total: 40 Features**\n",
        "\n",
        "**Local CFCS Score: 55.57**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"",
        "SLIM Feature Engineering Pipeline",
        "Total Features: 40 (Pruned for Private Leaderboard Robustness)",
        "",
        "Categories:",
        "1. Baseline: 8",
        "2. Phenology: 8",
        "3. Market Interaction: 5",
        "4. Complex: 4",
        "5. Advanced: 5",
        "6. Rare: 2",
        "7. Competition: 4",
        "8. External Weather: 3",
        "9. Soil Moisture: 1",
        "10. Global MAX: 0 (Removed)",
        "\"\"\"",
        "",
        "import pandas as pd",
        "import numpy as np",
        "import os",
        "from datetime import datetime",
        "",
        "# =============================================================================",
        "# DATA LOADING",
        "# =============================================================================",
        "def load_data():",
        "    print(\"Loading data...\")",
        "    df = pd.read_csv('corn_climate_risk_futures_daily_master.csv')",
        "    df['date_on'] = pd.to_datetime(df['date_on'])",
        "    return df",
        "",
        "# =============================================================================",
        "# 1. BASELINE FEATURES (8)",
        "# =============================================================================",
        "def create_baseline_features(df):",
        "    \"\"\"",
        "    KEPT: drought mean/concentration, heat mean/concentration, ",
        "          cold mean, excess precip mean/concentration, volatility 30d",
        "    ",
        "    Using actual columns: climate_risk_cnt_locations_*_risk_high",
        "    \"\"\"",
        "    print(\"Creating SLIM Baseline Features (8)...\")",
        "    ",
        "    new_features = []",
        "    ",
        "    # Map stress types to actual columns (using HIGH risk counts)",
        "    stress_cols = {",
        "        'drought': 'climate_risk_cnt_locations_drought_risk_high',",
        "        'heat': 'climate_risk_cnt_locations_heat_stress_risk_high',",
        "        'cold': 'climate_risk_cnt_locations_unseasonably_cold_risk_high',",
        "        'excess_precip': 'climate_risk_cnt_locations_excess_precip_risk_high'",
        "    }",
        "    ",
        "    # Check which columns exist",
        "    for name, col in stress_cols.items():",
        "        if col not in df.columns:",
        "            print(f\"  Warning: {col} not found. Using zeros.\")",
        "            df[col] = 0",
        "    ",
        "    # Aggregate to country level",
        "    for name, col in stress_cols.items():",
        "        # Mean (kept for all)",
        "        df[f'country_{name}_mean'] = df.groupby(['date_on', 'country_name'])[col].transform('mean')",
        "        new_features.append(f'country_{name}_mean')",
        "        ",
        "        # Concentration (kept for drought, heat, excess_precip only)",
        "        if name != 'cold':",
        "            # HHI-like concentration: variance / (mean^2) - higher = more concentrated",
        "            country_mean = df.groupby(['date_on', 'country_name'])[col].transform('mean')",
        "            country_std = df.groupby(['date_on', 'country_name'])[col].transform('std').fillna(0)",
        "            df[f'country_{name}_concentration'] = country_std / (country_mean.replace(0, 1))",
        "            new_features.append(f'country_{name}_concentration')",
        "    ",
        "    # Rolling Volatility (30d only)",
        "    df = df.sort_values(['country_name', 'date_on'])",
        "    df['country_drought_volatility_30d'] = df.groupby('country_name')['country_drought_mean'].transform(",
        "        lambda x: x.rolling(30, min_periods=7).std()",
        "    ).fillna(0)",
        "    new_features.append('country_drought_volatility_30d')",
        "    ",
        "    print(f\"  Created {len(new_features)} baseline features.\")",
        "    return df, new_features",
        "",
        "# =============================================================================",
        "# 2. PHENOLOGY FEATURES (8)",
        "# =============================================================================",
        "def create_phenology_features(df):",
        "    \"\"\"",
        "    KEPT: US, Brazil, Argentina, Ukraine, China, Canada, Russia, India",
        "    DROPPED: Mexico, Paraguay, South Africa",
        "    \"\"\"",
        "    print(\"Creating SLIM Phenology Features (8)...\")",
        "    ",
        "    new_features = []",
        "    ",
        "    # NH phenology weights (Jun-Aug peak)",
        "    nh_weights = {1: 0.0, 2: 0.0, 3: 0.1, 4: 0.3, 5: 0.5, 6: 0.9, 7: 1.0, 8: 0.9, 9: 0.5, 10: 0.2, 11: 0.0, 12: 0.0}",
        "    # SH phenology weights (Jan-Feb peak)",
        "    sh_weights = {1: 1.0, 2: 0.9, 3: 0.5, 4: 0.2, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.1, 11: 0.3, 12: 0.5}",
        "    ",
        "    df['month'] = df['date_on'].dt.month",
        "    ",
        "    # NH Countries (kept)",
        "    nh_countries = ['United States', 'China', 'Russia', 'Ukraine', 'India', 'Canada']",
        "    # SH Countries (kept)",
        "    sh_countries = ['Brazil', 'Argentina']",
        "    ",
        "    kept_countries = nh_countries + sh_countries",
        "    ",
        "    for country in kept_countries:",
        "        feat_name = f'climate_risk_drought_exposure_pheno_{country.replace(\" \", \"_\")}'",
        "        ",
        "        if country in nh_countries:",
        "            df['pheno_weight'] = df['month'].map(nh_weights)",
        "        else:",
        "            df['pheno_weight'] = df['month'].map(sh_weights)",
        "        ",
        "        # Only apply to matching country rows",
        "        # Using actual column: climate_risk_cnt_locations_drought_risk_high",
        "        df[feat_name] = np.where(",
        "            df['country_name'] == country,",
        "            df['climate_risk_cnt_locations_drought_risk_high'] * df['pheno_weight'],",
        "            0",
        "        )",
        "        new_features.append(feat_name)",
        "    ",
        "    # Cleanup",
        "    df.drop(columns=['pheno_weight'], inplace=True, errors='ignore')",
        "    ",
        "    print(f\"  Created {len(new_features)} phenology features.\")",
        "    return df, new_features",
        "",
        "# =============================================================================",
        "# 3. MARKET INTERACTION FEATURES (5)",
        "# =============================================================================",
        "def create_market_interaction_features(df):",
        "    \"\"\"",
        "    KEPT: Vol-adjusted phenology for US, Brazil, Ukraine, Argentina, China",
        "    DROPPED: Momentum interactions",
        "    \"\"\"",
        "    print(\"Creating SLIM Market Interaction Features (5)...\")",
        "    ",
        "    new_features = []",
        "    ",
        "    # Calculate volatility",
        "    futures_cols = [c for c in df.columns if c.startswith('futures_')]",
        "    if futures_cols:",
        "        futures_col = futures_cols[0]",
        "        df = df.sort_values('date_on')",
        "        df['volatility_5d'] = df[futures_col].rolling(5, min_periods=1).std().fillna(1)",
        "    else:",
        "        df['volatility_5d'] = 1",
        "    ",
        "    # Kept countries",
        "    kept_countries = ['United_States', 'Brazil', 'Ukraine', 'Argentina', 'China']",
        "    ",
        "    for country in kept_countries:",
        "        pheno_col = f'climate_risk_drought_exposure_pheno_{country}'",
        "        if pheno_col in df.columns:",
        "            feat_name = f'{pheno_col}_vol_adj'",
        "            df[feat_name] = df[pheno_col] / df['volatility_5d'].replace(0, 1)",
        "            new_features.append(feat_name)",
        "    ",
        "    print(f\"  Created {len(new_features)} market interaction features.\")",
        "    return df, new_features",
        "",
        "# =============================================================================",
        "# 4. COMPLEX SIGNALS (4)",
        "# =============================================================================",
        "def create_complex_signals(df):",
        "    \"\"\"",
        "    KEPT: compound_stress, drought_surprise, brazil_drought_lag60, cold_flood",
        "    \"\"\"",
        "    print(\"Creating SLIM Complex Signals (4)...\")",
        "    ",
        "    new_features = []",
        "    ",
        "    # 1. Compound Stress (Heat \u00c3\u2014 Drought)",
        "    df['climate_risk_compound_stress'] = (",
        "        df['climate_risk_cnt_locations_heat_stress_risk_high'] * df['climate_risk_cnt_locations_drought_risk_high']",
        "    )",
        "    new_features.append('climate_risk_compound_stress')",
        "    ",
        "    # 2. Drought Surprise",
        "    df = df.sort_values(['country_name', 'date_on'])",
        "    df['drought_30d_avg'] = df.groupby('country_name')['climate_risk_cnt_locations_drought_risk_high'].transform(",
        "        lambda x: x.rolling(30, min_periods=7).mean()",
        "    )",
        "    df['climate_risk_drought_surprise'] = df['climate_risk_cnt_locations_drought_risk_high'] - df['drought_30d_avg'].fillna(0)",
        "    new_features.append('climate_risk_drought_surprise')",
        "    ",
        "    # 3. Brazil Drought Lag 60d",
        "    brazil_mask = df['country_name'] == 'Brazil'",
        "    brazil_drought = df.loc[brazil_mask, ['date_on', 'climate_risk_cnt_locations_drought_risk_high']].copy()",
        "    brazil_drought = brazil_drought.groupby('date_on')['climate_risk_cnt_locations_drought_risk_high'].mean().reset_index()",
        "    brazil_drought.columns = ['date_on', 'brazil_drought_lag60']",
        "    brazil_drought['brazil_drought_lag60'] = brazil_drought['brazil_drought_lag60'].shift(60).fillna(0)",
        "    df = df.merge(brazil_drought, on='date_on', how='left')",
        "    df['brazil_drought_lag60'] = df['brazil_drought_lag60'].fillna(0)",
        "    new_features.append('brazil_drought_lag60')",
        "    ",
        "    # 4. Cold Flood",
        "    df['climate_risk_cold_flood'] = (",
        "        df['climate_risk_cnt_locations_unseasonably_cold_risk_high'] * df['climate_risk_cnt_locations_excess_precip_risk_high']",
        "    )",
        "    new_features.append('climate_risk_cold_flood')",
        "    ",
        "    # Cleanup",
        "    df.drop(columns=['drought_30d_avg'], inplace=True, errors='ignore')",
        "    ",
        "    print(f\"  Created {len(new_features)} complex signal features.\")",
        "    return df, new_features",
        "",
        "# =============================================================================",
        "# 5. ADVANCED CLIMATE SIGNALS (5)",
        "# =============================================================================",
        "def create_advanced_signals(df):",
        "    \"\"\"",
        "    KEPT: drought_momentum_7d, heat_momentum_7d, drought_cumulative, ",
        "          heat_cumulative, heat_pheno_weighted",
        "    \"\"\"",
        "    print(\"Creating SLIM Advanced Signals (5)...\")",
        "    ",
        "    new_features = []",
        "    ",
        "    df = df.sort_values(['country_name', 'date_on'])",
        "    ",
        "    # 1. Drought Momentum 7d",
        "    df['climate_risk_drought_momentum_7d'] = df.groupby('country_name')['climate_risk_cnt_locations_drought_risk_high'].transform(",
        "        lambda x: x.diff(7)",
        "    ).fillna(0)",
        "    new_features.append('climate_risk_drought_momentum_7d')",
        "    ",
        "    # 2. Heat Momentum 7d",
        "    df['climate_risk_heat_momentum_7d'] = df.groupby('country_name')['climate_risk_cnt_locations_heat_stress_risk_high'].transform(",
        "        lambda x: x.diff(7)",
        "    ).fillna(0)",
        "    new_features.append('climate_risk_heat_momentum_7d')",
        "    ",
        "    # 3. Drought Cumulative (Growing Season)",
        "    df['month'] = df['date_on'].dt.month",
        "    df['is_growing'] = df['month'].isin([5, 6, 7, 8, 9]).astype(int)",
        "    df['climate_risk_drought_cumulative'] = (",
        "        df['climate_risk_cnt_locations_drought_risk_high'] * df['is_growing']",
        "    ).groupby(df['country_name']).cumsum()",
        "    # Reset each year",
        "    df['year'] = df['date_on'].dt.year",
        "    df['climate_risk_drought_cumulative'] = df.groupby(['country_name', 'year'])['climate_risk_drought_cumulative'].transform(",
        "        lambda x: x - x.shift(1).fillna(0).where(df['month'] == 5, 0).cumsum()",
        "    )",
        "    new_features.append('climate_risk_drought_cumulative')",
        "    ",
        "    # 4. Heat Cumulative",
        "    df['climate_risk_heat_cumulative'] = (",
        "        df['climate_risk_cnt_locations_heat_stress_risk_high'] * df['is_growing']",
        "    ).groupby(df['country_name']).cumsum()",
        "    new_features.append('climate_risk_heat_cumulative')",
        "    ",
        "    # 5. Heat Pheno Weighted",
        "    nh_weights = {1: 0.0, 2: 0.0, 3: 0.1, 4: 0.3, 5: 0.5, 6: 0.9, 7: 1.0, 8: 0.9, 9: 0.5, 10: 0.2, 11: 0.0, 12: 0.0}",
        "    df['pheno_w'] = df['month'].map(nh_weights)",
        "    df['climate_risk_heat_pheno_weighted'] = df['climate_risk_cnt_locations_heat_stress_risk_high'] * df['pheno_w']",
        "    new_features.append('climate_risk_heat_pheno_weighted')",
        "    ",
        "    # Cleanup",
        "    df.drop(columns=['is_growing', 'pheno_w', 'year'], inplace=True, errors='ignore')",
        "    ",
        "    print(f\"  Created {len(new_features)} advanced signal features.\")",
        "    return df, new_features",
        "",
        "# =============================================================================",
        "# 6. RARE SIGNALS (2)",
        "# =============================================================================",
        "def create_rare_signals(df):",
        "    \"\"\"",
        "    KEPT: pollination_drought, drought_speed",
        "    \"\"\"",
        "    print(\"Creating SLIM Rare Signals (2)...\")",
        "    ",
        "    new_features = []",
        "    ",
        "    df['month'] = df['date_on'].dt.month",
        "    df['day'] = df['date_on'].dt.day",
        "    ",
        "    # 1. Pollination Drought (July 15 - Aug 10)",
        "    df['is_pollination'] = (",
        "        ((df['month'] == 7) & (df['day'] >= 15)) | ",
        "        ((df['month'] == 8) & (df['day'] <= 10))",
        "    ).astype(int)",
        "    df['climate_risk_pollination_drought'] = df['climate_risk_cnt_locations_drought_risk_high'] * df['is_pollination']",
        "    new_features.append('climate_risk_pollination_drought')",
        "    ",
        "    # 2. Drought Speed (Rate of Change)",
        "    df = df.sort_values(['country_name', 'date_on'])",
        "    df['climate_risk_drought_speed'] = df.groupby('country_name')['climate_risk_cnt_locations_drought_risk_high'].transform(",
        "        lambda x: x.diff(3)",
        "    ).clip(-1, 1).fillna(0)",
        "    new_features.append('climate_risk_drought_speed')",
        "    ",
        "    # Cleanup",
        "    df.drop(columns=['is_pollination', 'day'], inplace=True, errors='ignore')",
        "    ",
        "    print(f\"  Created {len(new_features)} rare signal features.\")",
        "    return df, new_features",
        "",
        "# =============================================================================",
        "# 7. COMPETITION FEATURES (4)",
        "# =============================================================================",
        "def create_competition_features(df):",
        "    \"\"\"",
        "    KEPT: drought_seasonal_zscore, heat_seasonal_zscore, ",
        "          us_china_combined, us_brazil_combined",
        "    \"\"\"",
        "    print(\"Creating SLIM Competition Features (4)...\")",
        "    ",
        "    new_features = []",
        "    ",
        "    df['month'] = df['date_on'].dt.month",
        "    ",
        "    # 1. Drought Seasonal Z-Score",
        "    monthly_stats = df.groupby('month')['climate_risk_cnt_locations_drought_risk_high'].agg(['mean', 'std']).reset_index()",
        "    monthly_stats.columns = ['month', 'drought_month_mean', 'drought_month_std']",
        "    df = df.merge(monthly_stats, on='month', how='left')",
        "    df['climate_risk_drought_seasonal_zscore'] = (",
        "        (df['climate_risk_cnt_locations_drought_risk_high'] - df['drought_month_mean']) / ",
        "        df['drought_month_std'].replace(0, 1)",
        "    ).fillna(0)",
        "    new_features.append('climate_risk_drought_seasonal_zscore')",
        "    ",
        "    # 2. Heat Seasonal Z-Score",
        "    heat_stats = df.groupby('month')['climate_risk_cnt_locations_heat_stress_risk_high'].agg(['mean', 'std']).reset_index()",
        "    heat_stats.columns = ['month', 'heat_month_mean', 'heat_month_std']",
        "    df = df.merge(heat_stats, on='month', how='left')",
        "    df['climate_risk_heat_seasonal_zscore'] = (",
        "        (df['climate_risk_cnt_locations_heat_stress_risk_high'] - df['heat_month_mean']) / ",
        "        df['heat_month_std'].replace(0, 1)",
        "    ).fillna(0)",
        "    new_features.append('climate_risk_heat_seasonal_zscore')",
        "    ",
        "    # 3. US-China Combined",
        "    us_drought = df[df['country_name'] == 'United States'].groupby('date_on')['climate_risk_cnt_locations_drought_risk_high'].mean()",
        "    china_drought = df[df['country_name'] == 'China'].groupby('date_on')['climate_risk_cnt_locations_drought_risk_high'].mean()",
        "    combined = pd.DataFrame({'us': us_drought, 'china': china_drought}).reset_index()",
        "    combined['climate_risk_us_china_combined'] = combined['us'].fillna(0) * combined['china'].fillna(0)",
        "    df = df.merge(combined[['date_on', 'climate_risk_us_china_combined']], on='date_on', how='left')",
        "    df['climate_risk_us_china_combined'] = df['climate_risk_us_china_combined'].fillna(0)",
        "    new_features.append('climate_risk_us_china_combined')",
        "    ",
        "    # 4. US-Brazil Combined",
        "    brazil_drought = df[df['country_name'] == 'Brazil'].groupby('date_on')['climate_risk_cnt_locations_drought_risk_high'].mean()",
        "    combined_br = pd.DataFrame({'us': us_drought, 'brazil': brazil_drought}).reset_index()",
        "    combined_br['climate_risk_us_brazil_combined'] = combined_br['us'].fillna(0) * combined_br['brazil'].fillna(0)",
        "    df = df.merge(combined_br[['date_on', 'climate_risk_us_brazil_combined']], on='date_on', how='left')",
        "    df['climate_risk_us_brazil_combined'] = df['climate_risk_us_brazil_combined'].fillna(0)",
        "    new_features.append('climate_risk_us_brazil_combined')",
        "    ",
        "    # Cleanup",
        "    df.drop(columns=['drought_month_mean', 'drought_month_std', 'heat_month_mean', 'heat_month_std'], ",
        "            inplace=True, errors='ignore')",
        "    ",
        "    print(f\"  Created {len(new_features)} competition features.\")",
        "    return df, new_features",
        "",
        "# =============================================================================",
        "# 8. EXTERNAL WEATHER FEATURES (3)",
        "# =============================================================================",
        "def create_external_weather_features(df):",
        "    \"\"\"",
        "    KEPT: vpd anomaly, water_deficit anomaly, heat_excess anomaly",
        "    \"\"\"",
        "    print(\"Creating SLIM External Weather Features (3)...\")",
        "    ",
        "    ext_file = 'external_extended_weather_data.csv'",
        "    if not os.path.exists(ext_file):",
        "        print(f\"  Warning: {ext_file} not found. Returning empty.\")",
        "        return df, []",
        "    ",
        "    weather_df = pd.read_csv(ext_file)",
        "    weather_df['date'] = pd.to_datetime(weather_df['date'])",
        "    weather_df = weather_df.sort_values(['country_name', 'region_name', 'date'])",
        "    weather_df = weather_df.drop_duplicates(subset=['country_name', 'region_name', 'date'])",
        "    ",
        "    # Derived features",
        "    weather_df['water_deficit'] = weather_df['precipitation_sum'] - weather_df['et0_fao_evapotranspiration']",
        "    weather_df['heat_excess'] = (weather_df['temperature_2m_max'] - 30).clip(lower=0)",
        "    ",
        "    new_features = []",
        "    ",
        "    # Calculate anomalies for 3 variables",
        "    for var_col, var_name in [",
        "        ('vapour_pressure_deficit_max', 'vpd'),",
        "        ('water_deficit', 'water_deficit'),",
        "        ('heat_excess', 'heat_excess')",
        "    ]:",
        "        roll = weather_df.groupby(['country_name', 'region_name'])[var_col].rolling(30, min_periods=7)",
        "        weather_df[f'{var_name}_mean30d'] = roll.mean().reset_index(level=[0,1], drop=True)",
        "        weather_df[f'{var_name}_std30d'] = roll.std().reset_index(level=[0,1], drop=True)",
        "        ",
        "        weather_df[f'external_anomaly_{var_name}'] = (",
        "            (weather_df[var_col] - weather_df[f'{var_name}_mean30d']) / ",
        "            weather_df[f'{var_name}_std30d'].replace(0, 1)",
        "        ).fillna(0)",
        "    ",
        "    # Merge to main df",
        "    weather_df['date_on'] = weather_df['date']",
        "    merge_cols = ['date_on', 'country_name', 'region_name']",
        "    anomaly_cols = ['external_anomaly_vpd', 'external_anomaly_water_deficit', 'external_anomaly_heat_excess']",
        "    ",
        "    if 'region_name' in df.columns:",
        "        df = df.merge(weather_df[merge_cols + anomaly_cols], on=merge_cols, how='left')",
        "    else:",
        "        print(\"  Warning: region_name not in df. Aggregating at country level.\")",
        "        country_agg = weather_df.groupby(['date_on', 'country_name'])[anomaly_cols].mean().reset_index()",
        "        df = df.merge(country_agg, on=['date_on', 'country_name'], how='left')",
        "    ",
        "    for c in anomaly_cols:",
        "        if c in df.columns:",
        "            df[c] = df[c].fillna(0)",
        "            # Aggregate to country level",
        "            feat_name = f'{c}_country_weighted'",
        "            if 'production_weight' in df.columns:",
        "                df[f'{c}_w'] = df[c] * df['production_weight']",
        "                country_sum = df.groupby(['date_on', 'country_name'])[f'{c}_w'].transform('sum')",
        "                weights_sum = df.groupby(['date_on', 'country_name'])['production_weight'].transform('sum')",
        "                df[feat_name] = country_sum / weights_sum.replace(0, 1)",
        "                df.drop(columns=[f'{c}_w'], inplace=True)",
        "            else:",
        "                df[feat_name] = df.groupby(['date_on', 'country_name'])[c].transform('mean')",
        "            new_features.append(feat_name)",
        "    ",
        "    print(f\"  Created {len(new_features)} external weather features.\")",
        "    return df, new_features",
        "",
        "# =============================================================================",
        "# 9. SOIL MOISTURE (1)",
        "# =============================================================================",
        "def create_soil_moisture_features(df):",
        "    \"\"\"",
        "    KEPT: country_soil_moisture_zscore (root-weighted, 30d z-score, clipped)",
        "    \"\"\"",
        "    print(\"Creating SLIM Soil Moisture Features (1)...\")",
        "    ",
        "    ext_file = 'external_soil_data.csv'",
        "    if not os.path.exists(ext_file):",
        "        print(f\"  Warning: {ext_file} not found. Returning empty.\")",
        "        return df, []",
        "    ",
        "    soil_df = pd.read_csv(ext_file)",
        "    soil_df['date'] = pd.to_datetime(soil_df['date'])",
        "    soil_df = soil_df.sort_values(['country_name', 'region_name', 'date'])",
        "    ",
        "    # Root-weighted (70% root, 30% surface)",
        "    soil_df['soil_moisture_weighted'] = 0.7 * soil_df['soil_moisture_root'] + 0.3 * soil_df['soil_moisture_surf']",
        "    ",
        "    # 30d rolling z-score",
        "    roll = soil_df.groupby(['country_name', 'region_name'])['soil_moisture_weighted'].rolling(30, min_periods=7)",
        "    soil_df['sm_mean30d'] = roll.mean().reset_index(level=[0,1], drop=True)",
        "    soil_df['sm_std30d'] = roll.std().reset_index(level=[0,1], drop=True)",
        "    ",
        "    soil_df['soil_moisture_zscore'] = (",
        "        (soil_df['soil_moisture_weighted'] - soil_df['sm_mean30d']) / ",
        "        soil_df['sm_std30d'].replace(0, 1)",
        "    ).clip(-3, 3).fillna(0)",
        "    ",
        "    # Merge",
        "    soil_df['date_on'] = soil_df['date']",
        "    merge_cols = ['date_on', 'country_name', 'region_name']",
        "    ",
        "    if 'region_name' in df.columns:",
        "        df = df.merge(soil_df[merge_cols + ['soil_moisture_zscore']], on=merge_cols, how='left')",
        "    else:",
        "        country_agg = soil_df.groupby(['date_on', 'country_name'])['soil_moisture_zscore'].mean().reset_index()",
        "        df = df.merge(country_agg, on=['date_on', 'country_name'], how='left')",
        "    ",
        "    df['soil_moisture_zscore'] = df['soil_moisture_zscore'].fillna(0)",
        "    ",
        "    # Aggregate to country",
        "    if 'production_weight' in df.columns:",
        "        df['sm_w'] = df['soil_moisture_zscore'] * df['production_weight']",
        "        country_sum = df.groupby(['date_on', 'country_name'])['sm_w'].transform('sum')",
        "        weights_sum = df.groupby(['date_on', 'country_name'])['production_weight'].transform('sum')",
        "        df['country_soil_moisture_zscore'] = country_sum / weights_sum.replace(0, 1)",
        "        df.drop(columns=['sm_w'], inplace=True)",
        "    else:",
        "        df['country_soil_moisture_zscore'] = df.groupby(['date_on', 'country_name'])['soil_moisture_zscore'].transform('mean')",
        "    ",
        "    print(\"  Created 1 soil moisture feature.\")",
        "    return df, ['country_soil_moisture_zscore']",
        "",
        "# =============================================================================",
        "# AGGREGATION",
        "# =============================================================================",
        "def aggregate_to_monthly(df, feature_cols):",
        "    \"\"\"Aggregate daily features to monthly mean (broadcast back)\"\"\"",
        "    print(\"Aggregating features to monthly mean...\")",
        "    ",
        "    df['year_month'] = df['date_on'].dt.to_period('M')",
        "    ",
        "    for col in feature_cols:",
        "        if col in df.columns:",
        "            monthly_mean = df.groupby(['country_name', 'year_month'])[col].transform('mean')",
        "            df[col] = monthly_mean",
        "    ",
        "    return df",
        "",
        "# =============================================================================",
        "# MAIN",
        "# =============================================================================",
        "def main():",
        "    df = load_data()",
        "    ",
        "    all_features = []",
        "    ",
        "    # 1. Baseline (8)",
        "    df, baseline_cols = create_baseline_features(df)",
        "    all_features.extend(baseline_cols)",
        "    ",
        "    # 2. Phenology (8)",
        "    df, pheno_cols = create_phenology_features(df)",
        "    all_features.extend(pheno_cols)",
        "    ",
        "    # 3. Market Interaction (5)",
        "    df, market_cols = create_market_interaction_features(df)",
        "    all_features.extend(market_cols)",
        "    ",
        "    # 4. Complex Signals (4)",
        "    df, complex_cols = create_complex_signals(df)",
        "    all_features.extend(complex_cols)",
        "    ",
        "    # 5. Advanced Signals (5)",
        "    df, advanced_cols = create_advanced_signals(df)",
        "    all_features.extend(advanced_cols)",
        "    ",
        "    # 6. Rare Signals (2)",
        "    df, rare_cols = create_rare_signals(df)",
        "    all_features.extend(rare_cols)",
        "    ",
        "    # 7. Competition Features (4)",
        "    df, competition_cols = create_competition_features(df)",
        "    all_features.extend(competition_cols)",
        "    ",
        "    # 8. External Weather (3)",
        "    df, weather_cols = create_external_weather_features(df)",
        "    all_features.extend(weather_cols)",
        "    ",
        "    # 9. Soil Moisture (1)",
        "    df, soil_cols = create_soil_moisture_features(df)",
        "    all_features.extend(soil_cols)",
        "    ",
        "    # Aggregate to monthly",
        "    df = aggregate_to_monthly(df, all_features)",
        "    ",
        "    # Summary",
        "    print(f\"\\n{'='*50}\")",
        "    print(\"SLIM FEATURE ENGINEERING SUMMARY\")",
        "    print(f\"{'='*50}\")",
        "    print(f\"Baseline Features:     {len(baseline_cols)}\")",
        "    print(f\"Phenology Features:    {len(pheno_cols)}\")",
        "    print(f\"Market Interaction:    {len(market_cols)}\")",
        "    print(f\"Complex Signals:       {len(complex_cols)}\")",
        "    print(f\"Advanced Signals:      {len(advanced_cols)}\")",
        "    print(f\"Rare Signals:          {len(rare_cols)}\")",
        "    print(f\"Competition Features:  {len(competition_cols)}\")",
        "    print(f\"External Weather:      {len(weather_cols)}\")",
        "    print(f\"Soil Moisture:         {len(soil_cols)}\")",
        "    print(f\"{'='*50}\")",
        "    print(f\"TOTAL FEATURES:        {len(all_features)}\")",
        "    print(f\"{'='*50}\")",
        "    ",
        "    # Save",
        "    meta = ['date_on', 'country_name', 'region_id', 'crop_name']",
        "    existing_meta = [c for c in meta if c in df.columns]",
        "    final_cols = list(set(existing_meta + all_features))",
        "    ",
        "    output_file = 'engineered_features_slim.csv'",
        "    print(f\"\\nSaving to '{output_file}'...\")",
        "    df[final_cols].to_csv(output_file, index=False)",
        "    ",
        "    print(\"\\nFeature list:\")",
        "    for i, feat in enumerate(all_features, 1):",
        "        print(f\"  {i:2d}. {feat}\")",
        "",
        "if __name__ == \"__main__\":",
        "    main()",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"",
        "Produce Submission (SLIM)",
        "Uses the 40-feature pruned dataset.",
        "\"\"\"",
        "",
        "import pandas as pd",
        "",
        "def main():",
        "    print(\"Loading Template (Sample Submission)...\")",
        "    template = pd.read_csv('sample submission.csv')",
        "    print(f\"Template Rows: {len(template)}\")",
        "    ",
        "    print(\"Loading SLIM Engineered Features...\")",
        "    features = pd.read_csv('engineered_features_slim.csv')",
        "    features['date_on'] = pd.to_datetime(features['date_on'])",
        "    ",
        "    # Identify feature columns (excluding metadata)",
        "    meta_cols = ['date_on', 'country_name', 'region_id', 'crop_name']",
        "    feature_cols = [c for c in features.columns if c not in meta_cols]",
        "    ",
        "    print(f\"Feature columns: {len(feature_cols)}\")",
        "    ",
        "    # CRITICAL: Deduplicate features to country-level (one row per date-country)",
        "    # This prevents row explosion during merge",
        "    print(\"Deduplicating features to country-level...\")",
        "    features_agg = features.groupby(['date_on', 'country_name'])[feature_cols].mean().reset_index()",
        "    print(f\"Deduplicated feature rows: {len(features_agg)}\")",
        "    ",
        "    print(\"Merging Features...\")",
        "    template['date_on'] = pd.to_datetime(template['date_on'])",
        "    ",
        "    # Merge on [date_on, country_name] - now 1:1 mapping",
        "    submission = template.merge(",
        "        features_agg,",
        "        on=['date_on', 'country_name'],",
        "        how='left',",
        "        suffixes=('', '_feat')",
        "    )",
        "    ",
        "    # Drop duplicate columns",
        "    feat_cols = [c for c in submission.columns if c.endswith('_feat')]",
        "    submission.drop(columns=feat_cols, inplace=True)",
        "    ",
        "    # Fill NAs in feature columns with 0",
        "    for col in feature_cols:",
        "        if col in submission.columns:",
        "            submission[col] = submission[col].fillna(0)",
        "    ",
        "    print(f\"Submission Rows: {len(submission)}\")",
        "    ",
        "    # Verify row count matches template",
        "    if len(submission) != len(template):",
        "        print(f\"WARNING: Row count mismatch! Template: {len(template)}, Submission: {len(submission)}\")",
        "    else:",
        "        print(\"Row count verified OK.\")",
        "    ",
        "    # Save",
        "    output_file = 'submission_slim.csv'",
        "    submission.to_csv(output_file, index=False)",
        "    print(f\"Saved to '{output_file}'\")",
        "    print(f\"Columns: {len(submission.columns)}\")",
        "",
        "if __name__ == \"__main__\":",
        "    main()",
        "",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

